# Nøgleteknologier, der nævnes, inkluderer

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - en lavniveau-API til hardwareaccelereret maskinlæring bygget oven på DirectX 12.
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - en platform til parallel computing og en API-model udviklet af Nvidia, der muliggør generel databehandling på grafikkort (GPU'er).
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - et åbent format designet til at repræsentere maskinlæringsmodeller, som giver interoperabilitet mellem forskellige ML-rammeværk.
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - et format, der bruges til at repræsentere og opdatere maskinlæringsmodeller, særligt nyttigt for mindre sprogmodeller, der kan køre effektivt på CPU'er med 4-8bit kvantisering.

## DirectML

DirectML er en lavniveau-API, der muliggør hardwareaccelereret maskinlæring. Den er bygget oven på DirectX 12 for at udnytte GPU-acceleration og er leverandørneutral, hvilket betyder, at der ikke kræves kodeændringer for at fungere på tværs af forskellige GPU-leverandører. Den bruges primært til modeltræning og inferensarbejdsbelastninger på GPU'er.

Med hensyn til hardwareunderstøttelse er DirectML designet til at fungere med en bred vifte af GPU'er, herunder AMD's integrerede og diskrete GPU'er, Intels integrerede GPU'er og NVIDIAs diskrete GPU'er. Det er en del af Windows AI Platform og understøttes på Windows 10 og 11, hvilket gør det muligt at træne og køre modeller på enhver Windows-enhed.

Der har været opdateringer og muligheder relateret til DirectML, såsom understøttelse af op til 150 ONNX-operatører og brug i både ONNX runtime og WinML. Det understøttes af store hardwareleverandører (IHVs), som hver især implementerer forskellige metakommandosæt.

## CUDA

CUDA, som står for Compute Unified Device Architecture, er en platform til parallel computing og en API-model skabt af Nvidia. Den gør det muligt for softwareudviklere at bruge en CUDA-kompatibel GPU til generel databehandling – en tilgang kendt som GPGPU (General-Purpose computing on Graphics Processing Units). CUDA er en nøglefaktor i Nvidias GPU-acceleration og bruges bredt inden for områder som maskinlæring, videnskabelig databehandling og videobehandling.

Hardwareunderstøttelsen for CUDA er specifik for Nvidias GPU'er, da det er en proprietær teknologi udviklet af Nvidia. Hver arkitektur understøtter specifikke versioner af CUDA-værktøjssættet, som giver de nødvendige biblioteker og værktøjer til udviklere til at bygge og køre CUDA-applikationer.

## ONNX

ONNX (Open Neural Network Exchange) er et åbent format designet til at repræsentere maskinlæringsmodeller. Det giver en definition af en udvidelig beregningsgrafmodel samt definitioner af indbyggede operatorer og standarddatatyper. ONNX gør det muligt for udviklere at flytte modeller mellem forskellige ML-rammeværk, hvilket muliggør interoperabilitet og gør det lettere at skabe og implementere AI-applikationer.

Phi3 mini kan køre med ONNX Runtime på CPU og GPU på tværs af enheder, herunder serverplatforme, Windows, Linux og Mac-desktops samt mobile CPU'er. De optimerede konfigurationer, vi har tilføjet, er:

- ONNX-modeller til int4 DML: Kvantiseret til int4 via AWQ
- ONNX-model til fp16 CUDA
- ONNX-model til int4 CUDA: Kvantiseret til int4 via RTN
- ONNX-model til int4 CPU og mobil: Kvantiseret til int4 via RTN

## Llama.cpp

Llama.cpp er et open source-softwarebibliotek skrevet i C++. Det udfører inferens på forskellige store sprogmodeller (LLMs), herunder Llama. Udviklet sammen med ggml-biblioteket (et generelt tensorbibliotek) sigter llama.cpp mod at levere hurtigere inferens og lavere hukommelsesforbrug sammenlignet med den oprindelige Python-implementering. Det understøtter hardwareoptimering, kvantisering og tilbyder en simpel API samt eksempler. Hvis du er interesseret i effektiv LLM-inferens, er llama.cpp værd at udforske, da Phi3 kan køre Llama.cpp.

## GGUF

GGUF (Generic Graph Update Format) er et format, der bruges til at repræsentere og opdatere maskinlæringsmodeller. Det er særligt nyttigt for mindre sprogmodeller (SLMs), der kan køre effektivt på CPU'er med 4-8bit kvantisering. GGUF er gavnligt til hurtig prototyping og kørsel af modeller på edge-enheder eller i batchjobs som CI/CD-pipelines.

**Ansvarsfraskrivelse**:  
Dette dokument er blevet oversat ved hjælp af maskinbaserede AI-oversættelsestjenester. Selvom vi bestræber os på nøjagtighed, skal det bemærkes, at automatiserede oversættelser kan indeholde fejl eller unøjagtigheder. Det originale dokument på dets oprindelige sprog bør betragtes som den autoritative kilde. For kritisk information anbefales professionel menneskelig oversættelse. Vi påtager os ikke ansvaret for misforståelser eller fejltolkninger, der måtte opstå som følge af brugen af denne oversættelse.