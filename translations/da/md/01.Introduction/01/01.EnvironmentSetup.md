# Kom godt i gang med Phi-3 lokalt

Denne guide hjælper dig med at sætte dit lokale miljø op til at køre Phi-3-modellen ved hjælp af Ollama. Du kan køre modellen på flere forskellige måder, herunder ved brug af GitHub Codespaces, VS Code Dev Containers eller dit lokale miljø.

## Opsætning af miljø

### GitHub Codespaces

Du kan køre denne skabelon virtuelt ved hjælp af GitHub Codespaces. Knappen åbner en webbaseret VS Code-instans i din browser:

1. Åbn skabelonen (det kan tage flere minutter):

    [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/microsoft/phi-3cookbook)

2. Åbn et terminalvindue

### VS Code Dev Containers

⚠️ Denne mulighed fungerer kun, hvis din Docker Desktop er tildelt mindst 16 GB RAM. Hvis du har mindre end 16 GB RAM, kan du prøve [GitHub Codespaces-muligheden](../../../../../md/01.Introduction/01) eller [opsætte det lokalt](../../../../../md/01.Introduction/01).

En relateret mulighed er VS Code Dev Containers, som åbner projektet i din lokale VS Code ved hjælp af [Dev Containers-udvidelsen](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers):

1. Start Docker Desktop (installer det, hvis det ikke allerede er installeret)
2. Åbn projektet:

    [![Open in Dev Containers](https://img.shields.io/static/v1?style=for-the-badge&label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/microsoft/phi-3cookbook)

3. I det VS Code-vindue, der åbnes, når projektfilerne dukker op (det kan tage flere minutter), skal du åbne et terminalvindue.
4. Fortsæt med [deployments-trinene](../../../../../md/01.Introduction/01)

### Lokalt miljø

1. Sørg for, at følgende værktøjer er installeret:

    * [Ollama](https://ollama.com/)
    * [Python 3.10+](https://www.python.org/downloads/)
    * [OpenAI Python SDK](https://pypi.org/project/openai/)

## Test modellen

1. Bed Ollama om at downloade og køre phi3:mini-modellen:

    ```shell
    ollama run phi3:mini
    ```

    Det vil tage et par minutter at downloade modellen.

2. Når du ser "success" i outputtet, kan du sende en besked til modellen fra prompten.

    ```shell
    >>> Write a haiku about hungry hippos
    ```

3. Efter nogle sekunder bør du se en respons strømme ind fra modellen.

4. For at lære om forskellige teknikker, der bruges med sprogmodeller, kan du åbne Python-notebogen [ollama.ipynb](../../../../../code/01.Introduce/ollama.ipynb) og køre hver celle. Hvis du brugte en anden model end 'phi3:mini', skal du ændre `MODEL_NAME` in the first cell.

5. To have a conversation with the phi3:mini model from Python, open the Python file [chat.py](../../../../../code/01.Introduce/chat.py) and run it. You can change the `MODEL_NAME` øverst i filen efter behov, og du kan også ændre systembeskeden eller tilføje få-skud-eksempler, hvis det ønskes.

**Ansvarsfraskrivelse**:  
Dette dokument er blevet oversat ved hjælp af maskinbaserede AI-oversættelsestjenester. Selvom vi bestræber os på at opnå nøjagtighed, bedes du være opmærksom på, at automatiserede oversættelser kan indeholde fejl eller unøjagtigheder. Det originale dokument på dets oprindelige sprog bør betragtes som den autoritative kilde. For kritisk information anbefales professionel menneskelig oversættelse. Vi påtager os intet ansvar for misforståelser eller fejltolkninger, der måtte opstå som følge af brugen af denne oversættelse.