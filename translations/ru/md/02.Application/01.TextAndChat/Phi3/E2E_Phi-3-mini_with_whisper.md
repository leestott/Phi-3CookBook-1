# Интерактивный чат-бот Phi 3 Mini 4K Instruct с Whisper

## Обзор

Интерактивный чат-бот Phi 3 Mini 4K Instruct — это инструмент, который позволяет пользователям взаимодействовать с демоверсией Microsoft Phi 3 Mini 4K Instruct с помощью текстового или голосового ввода. Чат-бот может использоваться для выполнения различных задач, таких как перевод, обновления погоды и получение общей информации.

### Начало работы

Чтобы воспользоваться этим чат-ботом, выполните следующие шаги:

1. Откройте новый [E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb](https://github.com/microsoft/Phi-3CookBook/blob/main/code/06.E2E/E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb).
2. В главном окне блокнота вы увидите интерфейс чата с текстовым полем для ввода и кнопкой "Send".
3. Чтобы использовать текстовый чат-бот, просто введите сообщение в текстовое поле и нажмите кнопку "Send". Чат-бот ответит аудиофайлом, который можно воспроизвести прямо в блокноте.

**Примечание**: Этот инструмент требует наличия GPU и доступа к моделям Microsoft Phi-3 и OpenAI Whisper, которые используются для распознавания речи и перевода.

### Требования к GPU

Для запуска этой демонстрации требуется 12 ГБ памяти GPU.

Требования к памяти GPU для запуска демоверсии **Microsoft-Phi-3-Mini-4K Instruct** зависят от нескольких факторов, таких как размер входных данных (аудио или текст), язык перевода, скорость работы модели и доступная память GPU.

В целом, модель Whisper разработана для работы на GPU. Рекомендуемый минимальный объем памяти GPU для работы с моделью Whisper составляет 8 ГБ, однако она может использовать больше памяти при необходимости.

Важно отметить, что обработка большого объема данных или большого количества запросов может потребовать больше памяти GPU и/или вызвать проблемы с производительностью. Рекомендуется протестировать ваш сценарий использования с различными конфигурациями и следить за использованием памяти, чтобы определить оптимальные настройки для ваших нужд.

## Пример E2E для интерактивного чат-бота Phi 3 Mini 4K Instruct с Whisper

Блокнот Jupyter под названием [Interactive Phi 3 Mini 4K Instruct Chatbot with Whisper](https://github.com/microsoft/Phi-3CookBook/blob/main/code/06.E2E/E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb) демонстрирует, как использовать демоверсию Microsoft Phi 3 Mini 4K Instruct для генерации текста из аудио или текстового ввода. В блокноте определены несколько функций:

1. `tts_file_name(text)`: Эта функция создает имя файла на основе входного текста для сохранения сгенерированного аудиофайла.
2. `edge_free_tts(chunks_list,speed,voice_name,save_path)`: Эта функция использует Edge TTS API для генерации аудиофайла из списка фрагментов входного текста. Входные параметры включают список фрагментов, скорость речи, имя голоса и путь для сохранения сгенерированного аудиофайла.
3. `talk(input_text)`: Эта функция генерирует аудиофайл, используя Edge TTS API, и сохраняет его под случайным именем в директории /content/audio. Входной параметр — текст, который нужно преобразовать в речь.
4. `run_text_prompt(message, chat_history)`: Эта функция использует демоверсию Microsoft Phi 3 Mini 4K Instruct для генерации аудиофайла из входного сообщения и добавляет его в историю чата.
5. `run_audio_prompt(audio, chat_history)`: Эта функция преобразует аудиофайл в текст с помощью API модели Whisper и передает его в функцию `run_text_prompt()`.
6. Код запускает приложение Gradio, которое позволяет пользователям взаимодействовать с демоверсией Phi 3 Mini 4K Instruct, вводя сообщения или загружая аудиофайлы. Вывод отображается в виде текстового сообщения в приложении.

## Устранение неполадок

Установка драйверов Cuda GPU

1. Убедитесь, что ваши приложения Linux обновлены:

    ```bash
    sudo apt update
    ```

2. Установите драйверы Cuda:

    ```bash
    sudo apt install nvidia-cuda-toolkit
    ```

3. Зарегистрируйте расположение драйвера Cuda:

    ```bash
    echo /usr/lib64-nvidia/ >/etc/ld.so.conf.d/libcuda.conf; ldconfig
    ```

4. Проверьте объем памяти Nvidia GPU (требуется 12 ГБ памяти GPU):

    ```bash
    nvidia-smi
    ```

5. Очистите кэш: Если вы используете PyTorch, вы можете вызвать torch.cuda.empty_cache(), чтобы освободить всю неиспользуемую память в кэше для использования другими приложениями GPU.

    ```python
    torch.cuda.empty_cache() 
    ```

6. Проверьте Nvidia Cuda:

    ```bash
    nvcc --version
    ```

7. Выполните следующие действия, чтобы создать токен Hugging Face:

    - Перейдите на страницу [Hugging Face Token Settings](https://huggingface.co/settings/tokens?WT.mc_id=aiml-137032-kinfeylo).
    - Выберите **New token**.
    - Введите **Name** проекта, который вы хотите использовать.
    - Установите **Type** в значение **Write**.

> **Примечание**
>
> Если вы столкнулись с ошибкой:
>
> ```bash
> /sbin/ldconfig.real: Can't create temporary cache file /etc/ld.so.cache~: Permission denied 
> ```
>
> Чтобы устранить её, введите следующую команду в вашем терминале:
>
> ```bash
> sudo ldconfig
> ```

**Отказ от ответственности**:  
Этот документ был переведен с использованием автоматических сервисов машинного перевода на основе искусственного интеллекта. Несмотря на то, что мы стремимся к точности, обратите внимание, что автоматические переводы могут содержать ошибки или неточности. Оригинальный документ на его родном языке следует считать авторитетным источником. Для получения критически важной информации рекомендуется обратиться к профессиональному переводу, выполненному человеком. Мы не несем ответственности за любые недоразумения или неправильные толкования, возникшие в результате использования данного перевода.