# Использование Windows GPU для создания решения Prompt Flow с Phi-3.5-Instruct ONNX

Этот документ представляет собой пример использования PromptFlow с ONNX (Open Neural Network Exchange) для разработки AI-приложений на основе моделей Phi-3.

PromptFlow — это набор инструментов для разработки, созданный для упрощения полного цикла разработки AI-приложений на основе больших языковых моделей (LLM), начиная с идей и прототипирования и заканчивая тестированием и оценкой.

Интеграция PromptFlow с ONNX позволяет разработчикам:

- Оптимизировать производительность модели: Используйте ONNX для эффективного вывода и развертывания моделей.
- Упростить разработку: Используйте PromptFlow для управления рабочим процессом и автоматизации повторяющихся задач.
- Улучшить сотрудничество: Обеспечьте лучшую командную работу благодаря единой среде разработки.

**Prompt flow** — это набор инструментов для разработки, созданный для упрощения полного цикла разработки AI-приложений на основе LLM: от идей и прототипирования до тестирования, оценки, развертывания и мониторинга в производственной среде. Он значительно упрощает процесс проектирования подсказок и позволяет создавать LLM-приложения высокого качества.

Prompt flow может подключаться к OpenAI, Azure OpenAI Service и настраиваемым моделям (Huggingface, локальные LLM/SLM). Мы стремимся развернуть квантованную ONNX-модель Phi-3.5 в локальных приложениях. Prompt flow поможет нам лучше спланировать наш бизнес и реализовать локальные решения на основе Phi-3.5. В этом примере мы объединим ONNX Runtime GenAI Library, чтобы завершить создание решения Prompt flow на основе Windows GPU.

## **Установка**

### **ONNX Runtime GenAI для Windows GPU**

Ознакомьтесь с этим руководством по настройке ONNX Runtime GenAI для Windows GPU [нажмите здесь](./ORTWindowGPUGuideline.md).

### **Настройка Prompt flow в VSCode**

1. Установите расширение Prompt flow для VS Code.

![pfvscode](../../../../../../translated_images/pfvscode.79f42ae5dd93ed35c19d6d978ae75831fef40e0b8440ee48b893b5a0597d2260.ru.png)

2. После установки расширения Prompt flow для VS Code, нажмите на него и выберите **Installation dependencies**. Следуйте этому руководству, чтобы установить SDK Prompt flow в вашей среде.

![pfsetup](../../../../../../translated_images/pfsetup.0c82d99c7760aac29833b37faf4329e67e22279b1c5f37a73724dfa9ebaa32ee.ru.png)

3. Загрузите [Пример кода](../../../../../../code/09.UpdateSamples/Aug/pf/onnx_inference_pf) и откройте этот пример в VS Code.

![pfsample](../../../../../../translated_images/pfsample.7bf40b133a558d86356dd6bc0e480bad2659d9c5364823dae9b3e6784e6f2d25.ru.png)

4. Откройте **flow.dag.yaml**, чтобы выбрать вашу Python-среду.

![pfdag](../../../../../../translated_images/pfdag.c5eb356fa3a96178cd594de9a5da921c4bbe646a9946f32aa20d344ccbeb51a0.ru.png)

   Откройте **chat_phi3_ort.py**, чтобы указать расположение вашей ONNX-модели Phi-3.5-instruct.

![pfphi](../../../../../../translated_images/pfphi.fff4b0afea47c92c8481174dbf3092823906fca5b717fc642f78947c3e5bbb39.ru.png)

5. Запустите ваш Prompt flow для тестирования.

Откройте **flow.dag.yaml** и нажмите на визуальный редактор.

![pfv](../../../../../../translated_images/pfv.7af6ecd65784a98558b344ba69b5ba6233876823fb435f163e916a632394fc1e.ru.png)

После этого запустите его для тестирования.

![pfflow](../../../../../../translated_images/pfflow.9697e0fda67794bb0cf4b78d52e6f5a42002eec935bc2519933064afbbdd34f0.ru.png)

1. Вы можете запустить пакетную обработку в терминале, чтобы проверить больше результатов.

```bash

pf run create --file batch_run.yaml --stream --name 'Your eval qa name'    

```

Вы можете просмотреть результаты в вашем браузере по умолчанию.

![pfresult](../../../../../../translated_images/pfresult.972eb57dd5bec646e1aa01148991ba8959897efea396e42cf9d7df259444878d.ru.png)

**Отказ от ответственности**:  
Этот документ был переведен с использованием автоматических услуг машинного перевода на основе ИИ. Хотя мы стремимся к точности, имейте в виду, что автоматические переводы могут содержать ошибки или неточности. Оригинальный документ на его исходном языке следует считать авторитетным источником. Для получения критически важной информации рекомендуется профессиональный перевод человеком. Мы не несем ответственности за любые недоразумения или неверные толкования, возникающие в результате использования данного перевода.