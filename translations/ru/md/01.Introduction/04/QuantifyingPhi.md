# **Квантование семейства Phi**

Квантование модели — это процесс преобразования параметров (таких как веса и значения активации) в модели нейронной сети из большого диапазона значений (обычно непрерывного) в меньший конечный диапазон значений. Эта технология позволяет уменьшить размер и вычислительную сложность модели, а также повысить её эффективность в условиях ограниченных ресурсов, таких как мобильные устройства или встроенные системы. Квантование модели достигает компрессии за счёт снижения точности параметров, но при этом вносит определённые потери точности. Поэтому в процессе квантования необходимо найти баланс между размером модели, вычислительной сложностью и точностью. Среди распространённых методов квантования можно выделить квантование с фиксированной точкой, квантование с плавающей точкой и другие. Выбор подходящей стратегии квантования зависит от конкретного сценария и требований.

Мы стремимся развернуть модели GenAI на устройствах периферии и позволить большему числу устройств участвовать в сценариях GenAI, таких как мобильные устройства, AI PC/Copilot+PC и традиционные IoT-устройства. Благодаря квантованным моделям мы можем адаптировать их для различных устройств периферии. В сочетании с фреймворками ускорения моделей и квантованными моделями, предоставляемыми производителями оборудования, мы можем создавать более эффективные сценарии применения SLM.

В контексте квантования мы имеем различные уровни точности (INT4, INT8, FP16, FP32). Ниже приведено описание наиболее часто используемых уровней точности.

### **INT4**

Квантование INT4 — это радикальный метод, который преобразует веса и значения активации модели в 4-битные целые числа. INT4-квантование обычно приводит к более значительным потерям точности из-за меньшего диапазона представления и более низкой точности. Однако по сравнению с INT8-квантованием этот метод позволяет ещё больше снизить требования к памяти и вычислительной сложности модели. Следует отметить, что INT4-квантование редко используется на практике, так как слишком низкая точность может существенно ухудшить производительность модели. Кроме того, не всё оборудование поддерживает операции INT4, поэтому при выборе метода квантования необходимо учитывать совместимость с оборудованием.

### **INT8**

Квантование INT8 — это процесс преобразования весов и активаций модели из чисел с плавающей точкой в 8-битные целые числа. Хотя диапазон значений, представляемых INT8, меньше, а точность ниже, этот метод значительно снижает требования к памяти и вычислениям. При квантовании INT8 веса и значения активации модели проходят процесс масштабирования и смещения, чтобы максимально сохранить информацию из исходных чисел с плавающей точкой. Во время вывода эти квантованные значения деквантуются обратно в числа с плавающей точкой для вычислений, а затем снова квантуются в INT8 для следующего шага. Этот метод обеспечивает достаточную точность в большинстве приложений при сохранении высокой вычислительной эффективности.

### **FP16**

Формат FP16, то есть 16-битные числа с плавающей точкой (float16), позволяет сократить объём памяти вдвое по сравнению с 32-битными числами с плавающей точкой (float32), что даёт значительные преимущества в масштабных приложениях глубокого обучения. Формат FP16 позволяет загружать более крупные модели или обрабатывать больше данных в пределах тех же ограничений памяти GPU. По мере того как современное оборудование GPU всё больше поддерживает операции FP16, использование этого формата также может привести к увеличению скорости вычислений. Однако FP16 имеет свои недостатки, такие как меньшая точность, что в некоторых случаях может приводить к численной нестабильности или потерям точности.

### **FP32**

Формат FP32 обеспечивает более высокую точность и может точно представлять широкий диапазон значений. В сценариях, где выполняются сложные математические операции или требуются высокоточные результаты, предпочтителен формат FP32. Однако высокая точность также означает больший объём памяти и большее время вычислений. Для крупномасштабных моделей глубокого обучения, особенно при большом числе параметров модели и огромном объёме данных, использование FP32 может привести к недостатку памяти GPU или снижению скорости вывода.

На мобильных устройствах или IoT-устройствах мы можем преобразовать модели Phi-3.x в INT4, в то время как AI PC / Copilot PC могут использовать более высокую точность, такую как INT8, FP16 или FP32.

В настоящее время разные производители оборудования предоставляют различные фреймворки для поддержки генеративных моделей, такие как OpenVINO от Intel, QNN от Qualcomm, MLX от Apple и CUDA от Nvidia. В сочетании с квантованием моделей они позволяют осуществлять локальное развертывание.

С технической точки зрения, после квантования мы имеем поддержку различных форматов, таких как PyTorch / Tensorflow, GGUF и ONNX. Я провёл сравнение форматов GGUF и ONNX и их применяемости в различных сценариях. Здесь я рекомендую формат ONNX для квантования, поскольку он хорошо поддерживается как фреймворками моделей, так и оборудованием. В этой главе мы сосредоточимся на ONNX Runtime для GenAI, OpenVINO и Apple MLX для выполнения квантования моделей (если у вас есть лучший способ, вы можете предложить его, отправив PR).

**Эта глава включает**

1. [Квантование Phi-3.5 / 4 с использованием llama.cpp](./UsingLlamacppQuantifyingPhi.md)

2. [Квантование Phi-3.5 / 4 с использованием расширений Generative AI для onnxruntime](./UsingORTGenAIQuantifyingPhi.md)

3. [Квантование Phi-3.5 / 4 с использованием Intel OpenVINO](./UsingIntelOpenVINOQuantifyingPhi.md)

4. [Квантование Phi-3.5 / 4 с использованием фреймворка Apple MLX](./UsingAppleMLXQuantifyingPhi.md)

**Отказ от ответственности**:  
Этот документ был переведен с использованием автоматических сервисов перевода на базе искусственного интеллекта. Несмотря на наши усилия обеспечить точность, автоматические переводы могут содержать ошибки или неточности. Оригинальный документ на его исходном языке следует считать авторитетным источником. Для критически важной информации рекомендуется обращаться к профессиональному переводу, выполненному человеком. Мы не несем ответственности за любые недоразумения или неправильные интерпретации, возникающие в результате использования данного перевода.