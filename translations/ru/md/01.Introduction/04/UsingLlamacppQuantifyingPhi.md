# **Квантование семейства Phi с использованием llama.cpp**

## **Что такое llama.cpp**

llama.cpp — это библиотека с открытым исходным кодом, написанная преимущественно на C++, которая выполняет вывод на различных больших языковых моделях (LLMs), таких как Llama. Основная цель — обеспечить передовую производительность для вывода LLM на широком спектре оборудования с минимальной настройкой. Кроме того, для этой библиотеки доступны привязки на Python, которые предоставляют высокоуровневый API для завершения текста и веб-сервер, совместимый с OpenAI.

Основная цель llama.cpp — обеспечить вывод LLM с минимальной настройкой и передовой производительностью на разнообразном оборудовании — как локально, так и в облаке.

- Реализация на чистом C/C++ без зависимостей
- Apple Silicon — приоритетный пользователь — оптимизация через ARM NEON, Accelerate и Metal
- Поддержка AVX, AVX2 и AVX512 для архитектур x86
- Квантование с целыми числами на 1.5, 2, 3, 4, 5, 6 и 8 бит для ускорения вывода и уменьшения использования памяти
- Пользовательские CUDA-ядра для работы с LLM на NVIDIA GPU (поддержка AMD GPU через HIP)
- Поддержка бэкендов Vulkan и SYCL
- Гибридный вывод CPU+GPU для частичного ускорения моделей, которые превышают общий объем VRAM

## **Квантование Phi-3.5 с использованием llama.cpp**

Модель Phi-3.5-Instruct может быть квантована с использованием llama.cpp, но Phi-3.5-Vision и Phi-3.5-MoE пока не поддерживаются. Формат, преобразуемый llama.cpp, — это GGUF, который также является наиболее широко используемым форматом квантования.

На Hugging Face представлено большое количество моделей в квантованном формате GGUF. AI Foundry, Ollama и LlamaEdge используют llama.cpp, поэтому модели GGUF также часто применяются.

### **Что такое GGUF**

GGUF — это бинарный формат, оптимизированный для быстрого загрузки и сохранения моделей, что делает его высокоэффективным для целей вывода. GGUF разработан для использования с GGML и другими исполнительными средами. GGUF был создан @ggerganov, который также является разработчиком llama.cpp, популярного фреймворка вывода LLM на C/C++. Модели, изначально разработанные в таких фреймворках, как PyTorch, могут быть преобразованы в формат GGUF для использования с этими движками.

### **ONNX vs GGUF**

ONNX — это традиционный формат машинного обучения/глубокого обучения, который хорошо поддерживается в различных AI-фреймворках и имеет широкие сценарии использования на периферийных устройствах. Что касается GGUF, он основан на llama.cpp и может считаться продуктом эпохи GenAI. Эти два формата имеют схожие области применения. Если вам нужна лучшая производительность на встроенном оборудовании и в прикладных слоях, ONNX может быть вашим выбором. Если вы используете производные фреймворки и технологии llama.cpp, тогда GGUF может подойти лучше.

### **Квантование Phi-3.5-Instruct с использованием llama.cpp**

**1. Настройка окружения**


```bash

git clone https://github.com/ggerganov/llama.cpp.git

cd llama.cpp

make -j8

```


**2. Квантование**

С помощью llama.cpp преобразуйте Phi-3.5-Instruct в FP16 GGUF


```bash

./convert_hf_to_gguf.py <Your Phi-3.5-Instruct Location> --outfile phi-3.5-128k-mini_fp16.gguf

```

Квантование Phi-3.5 до INT4


```bash

./llama.cpp/llama-quantize <Your phi-3.5-128k-mini_fp16.gguf location> ./gguf/phi-3.5-128k-mini_Q4_K_M.gguf Q4_K_M

```


**3. Тестирование**

Установите llama-cpp-python


```bash

pip install llama-cpp-python -U

```

***Примечание*** 

Если вы используете Apple Silicon, установите llama-cpp-python следующим образом


```bash

CMAKE_ARGS="-DLLAMA_METAL=on" pip install llama-cpp-python -U

```

Тестирование


```bash

llama.cpp/llama-cli --model <Your phi-3.5-128k-mini_Q4_K_M.gguf location> --prompt "<|user|>\nCan you introduce .NET<|end|>\n<|assistant|>\n"  --gpu-layers 10

```



## **Ресурсы**

1. Узнайте больше о llama.cpp [https://onnxruntime.ai/docs/genai/](https://onnxruntime.ai/docs/genai/)

2. Узнайте больше о GGUF [https://huggingface.co/docs/hub/en/gguf](https://huggingface.co/docs/hub/en/gguf)

**Отказ от ответственности**:  
Этот документ был переведен с использованием автоматических сервисов перевода на основе ИИ. Хотя мы стремимся к точности, пожалуйста, имейте в виду, что автоматические переводы могут содержать ошибки или неточности. Оригинальный документ на его родном языке следует считать авторитетным источником. Для получения критически важной информации рекомендуется профессиональный перевод человеком. Мы не несем ответственности за любые недоразумения или неправильные интерпретации, возникающие в результате использования данного перевода.