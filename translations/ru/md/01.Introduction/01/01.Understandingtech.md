# Основные упомянутые технологии

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) — низкоуровневый API для аппаратного ускорения машинного обучения, построенный на базе DirectX 12.
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) — платформа параллельных вычислений и модель программного интерфейса приложений (API), разработанная Nvidia, которая позволяет использовать графические процессоры (GPU) для универсальных вычислений.
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) — открытый формат, предназначенный для представления моделей машинного обучения, обеспечивающий совместимость между различными ML-фреймворками.
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) — формат, используемый для представления и обновления моделей машинного обучения, особенно полезный для небольших языковых моделей, которые эффективно работают на CPU с квантованием 4-8 бит.

## DirectML

DirectML — это низкоуровневый API, который позволяет использовать аппаратное ускорение для машинного обучения. Он построен на базе DirectX 12, чтобы задействовать ускорение GPU, и является независимым от вендоров, то есть не требует изменений в коде для работы с различными производителями GPU. Основное применение — обучение моделей и выполнение выводов на GPU.

Что касается аппаратной поддержки, DirectML разработан для работы с широким спектром GPU, включая интегрированные и дискретные GPU от AMD, интегрированные GPU от Intel и дискретные GPU от NVIDIA. Он является частью платформы Windows AI и поддерживается в Windows 10 и 11, что позволяет выполнять обучение и вывод моделей на любом устройстве с Windows.

Были внесены обновления и представлены новые возможности для DirectML, такие как поддержка до 150 операторов ONNX и использование как в ONNX Runtime, так и в WinML. API поддерживается основными производителями аппаратного обеспечения (IHV), каждый из которых реализует различные метакоманды.

## CUDA

CUDA (Compute Unified Device Architecture) — это платформа параллельных вычислений и модель программного интерфейса приложений (API), созданная Nvidia. Она позволяет разработчикам использовать графический процессор (GPU) с поддержкой CUDA для универсальных вычислений — подход, известный как GPGPU (универсальные вычисления на графических процессорах). CUDA является ключевым элементом ускорения GPU от Nvidia и широко используется в таких областях, как машинное обучение, научные вычисления и обработка видео.

Аппаратная поддержка CUDA ограничивается GPU от Nvidia, так как это проприетарная технология, разработанная Nvidia. Каждая архитектура поддерживает определённые версии инструментария CUDA, который предоставляет необходимые библиотеки и инструменты для разработки и запуска приложений CUDA.

## ONNX

ONNX (Open Neural Network Exchange) — это открытый формат, предназначенный для представления моделей машинного обучения. Он определяет расширяемую модель вычислительного графа, а также включает определения встроенных операторов и стандартных типов данных. ONNX позволяет разработчикам переносить модели между различными ML-фреймворками, обеспечивая их совместимость и упрощая создание и развертывание AI-приложений.

Phi3 mini может работать с ONNX Runtime на CPU и GPU на различных устройствах, включая серверные платформы, настольные компьютеры с Windows, Linux и Mac, а также мобильные процессоры. Оптимизированные конфигурации, которые мы добавили:

- ONNX-модели для int4 DML: квантованы в int4 через AWQ
- ONNX-модель для fp16 CUDA
- ONNX-модель для int4 CUDA: квантована в int4 через RTN
- ONNX-модель для int4 CPU и мобильных устройств: квантована в int4 через RTN

## Llama.cpp

Llama.cpp — это библиотека с открытым исходным кодом, написанная на C++. Она выполняет вывод для различных больших языковых моделей (LLM), включая Llama. Разработанная вместе с библиотекой ggml (универсальная библиотека тензоров), llama.cpp направлена на обеспечение более быстрого вывода и меньшего использования памяти по сравнению с оригинальной реализацией на Python. Она поддерживает оптимизацию под оборудование, квантование и предоставляет простой API и примеры. Если вас интересует эффективный вывод LLM, llama.cpp заслуживает внимания, так как Phi3 может работать с Llama.cpp.

## GGUF

GGUF (Generic Graph Update Format) — это формат, используемый для представления и обновления моделей машинного обучения. Он особенно полезен для небольших языковых моделей (SLM), которые эффективно работают на CPU с квантованием 4-8 бит. GGUF полезен для быстрого прототипирования и запуска моделей на периферийных устройствах или в пакетных заданиях, таких как CI/CD-пайплайны.

**Отказ от ответственности**:  
Данный документ был переведен с использованием автоматических AI-сервисов перевода. Несмотря на наши усилия обеспечить точность, имейте в виду, что автоматические переводы могут содержать ошибки или неточности. Оригинальный документ на его исходном языке следует считать авторитетным источником. Для получения критически важной информации рекомендуется профессиональный перевод человеком. Мы не несем ответственности за любые недоразумения или неправильные интерпретации, возникшие в результате использования данного перевода.