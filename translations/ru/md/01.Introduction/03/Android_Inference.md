# **Inference Phi-3 на Android**

Давайте рассмотрим, как выполнять вывод с использованием Phi-3-mini на устройствах Android. Phi-3-mini — это новая серия моделей от Microsoft, которая позволяет запускать большие языковые модели (LLMs) на пограничных устройствах и устройствах IoT.

## Semantic Kernel и вывод

[Semantic Kernel](https://github.com/microsoft/semantic-kernel) — это фреймворк для создания приложений, совместимых с Azure OpenAI Service, моделями OpenAI и локальными моделями. Если вы новичок в Semantic Kernel, рекомендуем ознакомиться с [Semantic Kernel Cookbook](https://github.com/microsoft/SemanticKernelCookBook?WT.mc_id=aiml-138114-kinfeylo).

### Доступ к Phi-3-mini с помощью Semantic Kernel

Вы можете использовать его вместе с коннектором Hugging Face в Semantic Kernel. Ознакомьтесь с этим [примером кода](https://github.com/Azure-Samples/Phi-3MiniSamples/tree/main/semantickernel?WT.mc_id=aiml-138114-kinfeylo).

По умолчанию он соответствует идентификатору модели на Hugging Face. Однако вы также можете подключиться к локальному серверу модели Phi-3-mini.

### Вызов квантованных моделей с Ollama или LlamaEdge

Многие пользователи предпочитают использовать квантованные модели для локального запуска. [Ollama](https://ollama.com/) и [LlamaEdge](https://llamaedge.com) позволяют вызывать различные квантованные модели:

#### Ollama

Вы можете запускать `ollama run Phi-3` напрямую или настроить его оффлайн, создав `Modelfile` с указанием пути к вашему файлу `.gguf`.

```gguf
FROM {Add your gguf file path}
TEMPLATE \"\"\"<|user|> .Prompt<|end|> <|assistant|>\"\"\"
PARAMETER stop <|end|>
PARAMETER num_ctx 4096
```

[Пример кода](https://github.com/Azure-Samples/Phi-3MiniSamples/tree/main/ollama?WT.mc_id=aiml-138114-kinfeylo)

#### LlamaEdge

Если вам нужно использовать файлы `.gguf` как в облаке, так и на пограничных устройствах, LlamaEdge — отличный выбор. Вы можете ознакомиться с этим [примером кода](https://github.com/Azure-Samples/Phi-3MiniSamples/tree/main/wasm?WT.mc_id=aiml-138114-kinfeylo), чтобы начать работу.

### Установка и запуск на Android

1. **Скачайте приложение MLC Chat** (бесплатно) для Android.
2. Загрузите APK-файл (148MB) и установите его на ваше устройство.
3. Запустите приложение MLC Chat. Вы увидите список AI-моделей, включая Phi-3-mini.

В итоге, Phi-3-mini открывает захватывающие возможности для генеративного ИИ на пограничных устройствах, и вы уже можете начать изучать его возможности на Android.

**Отказ от ответственности**:  
Этот документ был переведен с использованием автоматических сервисов перевода на основе искусственного интеллекта. Хотя мы стремимся к точности, пожалуйста, имейте в виду, что автоматический перевод может содержать ошибки или неточности. Оригинальный документ на его исходном языке следует считать авторитетным источником. Для получения критически важной информации рекомендуется профессиональный перевод человеком. Мы не несем ответственности за любые недоразумения или неправильные интерпретации, возникающие в результате использования данного перевода.