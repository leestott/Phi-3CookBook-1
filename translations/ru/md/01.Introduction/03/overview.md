В контексте Phi-3-mini вывод (inference) относится к процессу использования модели для создания прогнозов или генерации результатов на основе входных данных. Позвольте мне рассказать подробнее о Phi-3-mini и его возможностях вывода.

Phi-3-mini является частью серии моделей Phi-3, выпущенных Microsoft. Эти модели созданы для того, чтобы переосмыслить возможности Малых Языковых Моделей (SLMs).

Вот основные моменты о Phi-3-mini и его возможностях вывода:

## **Обзор Phi-3-mini:**
- Phi-3-mini имеет размер параметров 3,8 миллиарда.
- Он может работать не только на традиционных вычислительных устройствах, но и на периферийных устройствах, таких как мобильные устройства и устройства Интернета вещей (IoT).
- Выпуск Phi-3-mini позволяет как частным пользователям, так и предприятиям разворачивать SLM на различных аппаратных устройствах, особенно в условиях ограниченных ресурсов.
- Поддерживаются различные форматы моделей, включая традиционный формат PyTorch, квантованную версию в формате gguf и квантованную версию на основе ONNX.

## **Доступ к Phi-3-mini:**
Чтобы получить доступ к Phi-3-mini, вы можете использовать [Semantic Kernel](https://github.com/microsoft/SemanticKernelCookBook?WT.mc_id=aiml-138114-kinfeylo) в приложении Copilot. Semantic Kernel совместим с Azure OpenAI Service, моделями с открытым исходным кодом на Hugging Face, а также локальными моделями.
Вы также можете использовать [Ollama](https://ollama.com) или [LlamaEdge](https://llamaedge.com) для вызова квантованных моделей. Ollama позволяет отдельным пользователям вызывать различные квантованные модели, а LlamaEdge обеспечивает кроссплатформенную доступность для моделей в формате GGUF.

## **Квантованные модели:**
Многие пользователи предпочитают использовать квантованные модели для локального вывода. Например, вы можете напрямую запустить Ollama run Phi-3 или настроить его оффлайн, используя Modelfile. Modelfile указывает путь к файлу GGUF и формат подсказки.

## **Возможности генеративного ИИ:**
Комбинирование SLM, таких как Phi-3-mini, открывает новые возможности для генеративного ИИ. Вывод — это только первый шаг; эти модели могут использоваться для выполнения различных задач в условиях ограниченных ресурсов, низкой задержки и ограниченного бюджета.

## **Открываем генеративный ИИ с Phi-3-mini: Руководство по выводу и развертыванию**  
Узнайте, как использовать Semantic Kernel, Ollama/LlamaEdge и ONNX Runtime для доступа к моделям Phi-3-mini и вывода, а также исследуйте возможности генеративного ИИ в различных сценариях применения.

**Особенности**
Вывод модели phi3-mini в:

- [Semantic Kernel](https://github.com/Azure-Samples/Phi-3MiniSamples/tree/main/semantickernel?WT.mc_id=aiml-138114-kinfeylo)
- [Ollama](https://github.com/Azure-Samples/Phi-3MiniSamples/tree/main/ollama?WT.mc_id=aiml-138114-kinfeylo)
- [LlamaEdge WASM](https://github.com/Azure-Samples/Phi-3MiniSamples/tree/main/wasm?WT.mc_id=aiml-138114-kinfeylo)
- [ONNX Runtime](https://github.com/Azure-Samples/Phi-3MiniSamples/tree/main/onnx?WT.mc_id=aiml-138114-kinfeylo)
- [iOS](https://github.com/Azure-Samples/Phi-3MiniSamples/tree/main/ios?WT.mc_id=aiml-138114-kinfeylo)

В итоге, Phi-3-mini предоставляет разработчикам возможность изучать различные форматы моделей и использовать генеративный ИИ в самых разных сценариях применения.

**Отказ от ответственности**:  
Этот документ был переведен с использованием автоматизированных сервисов машинного перевода на основе ИИ. Несмотря на наши усилия обеспечить точность, обратите внимание, что автоматические переводы могут содержать ошибки или неточности. Оригинальный документ на его исходном языке следует считать авторитетным источником. Для получения критически важной информации рекомендуется обращаться к профессиональному переводу, выполненному человеком. Мы не несем ответственности за любые недоразумения или неправильные толкования, возникшие в результате использования данного перевода.