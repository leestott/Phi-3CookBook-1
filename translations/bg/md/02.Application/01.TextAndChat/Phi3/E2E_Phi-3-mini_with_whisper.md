# Интерактивен Phi 3 Mini 4K Instruct Чатбот с Whisper

## Обща информация

Интерактивният Phi 3 Mini 4K Instruct Чатбот е инструмент, който позволява на потребителите да взаимодействат с Microsoft Phi 3 Mini 4K instruct демонстрация чрез текстов или аудио вход. Чатботът може да се използва за различни задачи, като преводи, прогнози за времето и събиране на обща информация.

### Как да започнете

За да използвате този чатбот, следвайте тези инструкции:

1. Отворете нов [E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb](https://github.com/microsoft/Phi-3CookBook/blob/main/code/06.E2E/E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb)
2. В основния прозорец на тетрадката ще видите интерфейс на чат с текстово поле за въвеждане и бутон „Изпрати“.
3. За да използвате текстовия чатбот, просто въведете съобщението си в текстовото поле и натиснете бутона „Изпрати“. Чатботът ще отговори с аудио файл, който може да се възпроизвежда директно от тетрадката.

**Забележка**: Този инструмент изисква GPU и достъп до Microsoft Phi-3 и OpenAI Whisper модели, които се използват за разпознаване на реч и преводи.

### Изисквания за GPU

За да стартирате тази демонстрация, ви трябват 12 GB GPU памет.

Изискванията за памет за изпълнение на демонстрацията **Microsoft-Phi-3-Mini-4K instruct** на GPU зависят от няколко фактора, като размер на входните данни (аудио или текст), език за превод, скорост на модела и налична памет на GPU.

Като цяло моделът Whisper е проектиран да работи на GPU. Препоръчителният минимален размер на GPU паметта за работа с модела Whisper е 8 GB, но той може да обработва и по-големи количества памет, ако е необходимо.

Важно е да се отбележи, че обработването на голям обем данни или висока честота на заявки към модела може да изисква повече GPU памет и/или да доведе до проблеми с производителността. Препоръчва се да тествате вашия случай на употреба с различни конфигурации и да наблюдавате използването на паметта, за да определите оптималните настройки за вашите специфични нужди.

## Пример от край до край за Интерактивен Phi 3 Mini 4K Instruct Чатбот с Whisper

Тетрадката с име [Interactive Phi 3 Mini 4K Instruct Chatbot with Whisper](https://github.com/microsoft/Phi-3CookBook/blob/main/code/06.E2E/E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb) демонстрира как да използвате Microsoft Phi 3 Mini 4K instruct Демонстрацията за генериране на текст от аудио или текстов вход. Тетрадката дефинира няколко функции:

1. `tts_file_name(text)`: Тази функция генерира име на файл на базата на входния текст за запазване на генерирания аудио файл.
2. `edge_free_tts(chunks_list,speed,voice_name,save_path)`: Тази функция използва Edge TTS API за генериране на аудио файл от списък с части от входния текст. Входните параметри са списъкът с части, скоростта на речта, името на гласа и пътят за запазване на генерирания аудио файл.
3. `talk(input_text)`: Тази функция генерира аудио файл, използвайки Edge TTS API, и го запазва с произволно име във /content/audio директорията. Входният параметър е текстът, който ще бъде конвертиран в реч.
4. `run_text_prompt(message, chat_history)`: Тази функция използва Microsoft Phi 3 Mini 4K instruct демонстрацията за генериране на аудио файл от входно съобщение и го добавя към историята на чата.
5. `run_audio_prompt(audio, chat_history)`: Тази функция конвертира аудио файл в текст, използвайки Whisper модел API, и го подава на `run_text_prompt()` функцията.
6. Кодът стартира Gradio приложение, което позволява на потребителите да взаимодействат с Phi 3 Mini 4K instruct демонстрацията, като въвеждат съобщения или качват аудио файлове. Изходът се показва като текстово съобщение в приложението.

## Отстраняване на проблеми

Инсталиране на Cuda GPU драйвери

1. Уверете се, че вашите Linux приложения са актуални

    ```bash
    sudo apt update
    ```

2. Инсталирайте Cuda драйвери

    ```bash
    sudo apt install nvidia-cuda-toolkit
    ```

3. Регистрирайте местоположението на Cuda драйвера

    ```bash
    echo /usr/lib64-nvidia/ >/etc/ld.so.conf.d/libcuda.conf; ldconfig
    ```

4. Проверка на размера на Nvidia GPU паметта (Изисква се 12 GB GPU памет)

    ```bash
    nvidia-smi
    ```

5. Изчистване на кеша: Ако използвате PyTorch, можете да извикате torch.cuda.empty_cache(), за да освободите цялата неизползвана кеширана памет, така че да може да се използва от други GPU приложения

    ```python
    torch.cuda.empty_cache() 
    ```

6. Проверка на Nvidia Cuda

    ```bash
    nvcc --version
    ```

7. Изпълнете следните стъпки, за да създадете Hugging Face токен.

    - Отидете на [Hugging Face Token Settings page](https://huggingface.co/settings/tokens?WT.mc_id=aiml-137032-kinfeylo).
    - Изберете **Нов токен**.
    - Въведете **Име** на проекта, който искате да използвате.
    - Изберете **Тип** на **Запис**.

> **Забележка**
>
> Ако срещнете следната грешка:
>
> ```bash
> /sbin/ldconfig.real: Can't create temporary cache file /etc/ld.so.cache~: Permission denied 
> ```
>
> За да я разрешите, въведете следната команда в терминала си.
>
> ```bash
> sudo ldconfig
> ```

**Отказ от отговорност**:  
Този документ е преведен с помощта на услуги за машинен превод, базирани на изкуствен интелект. Въпреки че се стремим към точност, имайте предвид, че автоматизираните преводи може да съдържат грешки или неточности. Оригиналният документ на неговия изходен език следва да се счита за авторитетен източник. За критична информация се препоръчва професионален превод от човек. Ние не носим отговорност за никакви недоразумения или погрешни тълкувания, произтичащи от използването на този превод.