# **Квантифициране на Phi Family**

Квантификацията на модел означава процеса на преобразуване на параметрите (като тегла и стойности на активация) в модел на невронна мрежа от широк диапазон от стойности (обикновено непрекъснат диапазон) към по-малък краен диапазон от стойности. Тази технология може да намали размера и изчислителната сложност на модела и да подобри ефективността му в среди с ограничени ресурси, като мобилни устройства или вградени системи. Квантификацията на модела постига компресия чрез намаляване на прецизността на параметрите, но също така въвежда известна загуба на точност. Затова при процеса на квантификация е необходимо да се балансират размерът на модела, изчислителната сложност и точността. Често използваните методи за квантификация включват фиксирана точка, плаваща точка и др. Можете да изберете подходяща стратегия за квантификация според конкретния сценарий и нужди.

Ние се стремим да внедрим GenAI модел в edge устройства и да позволим на повече устройства да навлязат в GenAI сценарии, като мобилни устройства, AI PC/Copilot+PC и традиционни IoT устройства. Чрез квантифицирания модел можем да го внедрим в различни edge устройства въз основа на техните характеристики. Комбинирайки го с рамките за ускорение на модела и квантифицираните модели, предоставени от хардуерните производители, можем да изградим по-добри SLM приложения.

В контекста на квантификацията разполагаме с различни нива на прецизност (INT4, INT8, FP16, FP32). По-долу е обяснение на най-често използваните нива на квантификация.

### **INT4**

INT4 квантификацията е радикален метод, който преобразува теглата и стойностите на активация на модела в 4-битови цели числа. INT4 квантификацията обикновено води до по-голяма загуба на точност поради по-малкия диапазон на представяне и по-ниската прецизност. Въпреки това, в сравнение с INT8, INT4 квантификацията може допълнително да намали изискванията за съхранение и изчислителната сложност на модела. Трябва да се отбележи, че INT4 квантификацията е сравнително рядка в практическите приложения, защото твърде ниската точност може да доведе до значително влошаване на производителността на модела. Освен това не всички хардуери поддържат операции с INT4, така че е необходимо да се вземе предвид хардуерната съвместимост при избора на метод за квантификация.

### **INT8**

INT8 квантификацията представлява процеса на преобразуване на теглата и активациите на модела от числа с плаваща точка към 8-битови цели числа. Въпреки че числовият диапазон, представен от INT8, е по-малък и по-малко прецизен, той може значително да намали изискванията за съхранение и изчисления. При INT8 квантификация теглата и стойностите на активация на модела преминават през процес на квантификация, включително скалиране и отместване, за да се запази възможно най-много оригиналната информация от числата с плаваща точка. По време на инференцията тези квантифицирани стойности ще бъдат деквантифицирани обратно към числа с плаваща точка за изчисление и след това отново квантифицирани към INT8 за следващата стъпка. Този метод може да осигури достатъчна точност в повечето приложения, като същевременно поддържа висока изчислителна ефективност.

### **FP16**

Форматът FP16, тоест 16-битови числа с плаваща точка (float16), намалява използването на памет наполовина в сравнение с 32-битовите числа с плаваща точка (float32), което има значителни предимства в приложенията за мащабно дълбоко обучение. Форматът FP16 позволява зареждането на по-големи модели или обработката на повече данни в рамките на същите ограничения на GPU паметта. С напредването на съвременния GPU хардуер, който поддържа FP16 операции, използването на FP16 формат може също да доведе до подобрения в скоростта на изчисление. Въпреки това, форматът FP16 има и своите присъщи недостатъци, а именно по-ниска прецизност, което може да доведе до числова нестабилност или загуба на точност в някои случаи.

### **FP32**

Форматът FP32 предоставя по-висока прецизност и може точно да представя широк диапазон от стойности. В сценарии, където се извършват сложни математически операции или се изискват резултати с висока прецизност, FP32 форматът е предпочитан. Въпреки това, високата точност също означава по-голямо използване на памет и по-дълго време за изчисление. За мащабни модели за дълбоко обучение, особено когато има много параметри и огромно количество данни, форматът FP32 може да доведе до недостатъчна GPU памет или намаляване на скоростта на инференция.

На мобилни устройства или IoT устройства можем да преобразуваме Phi-3.x моделите към INT4, докато AI PC / Copilot PC могат да използват по-висока прецизност, като INT8, FP16, FP32.

Понастоящем различни производители на хардуер предлагат различни рамки за поддръжка на генеративни модели, като OpenVINO на Intel, QNN на Qualcomm, MLX на Apple и CUDA на Nvidia, които в комбинация с квантификация на модела позволяват локално внедряване.

От технологична гледна точка разполагаме с различни формати за поддръжка след квантификация, като PyTorch / Tensorflow формат, GGUF и ONNX. Направих сравнение на форматите и приложенията между GGUF и ONNX. Тук препоръчвам ONNX квантификационен формат, който има добра поддръжка от рамката на модела до хардуера. В тази глава ще се фокусираме върху ONNX Runtime за GenAI, OpenVINO и Apple MLX за изпълнение на квантификация на модел (ако имате по-добър метод, можете да ни го предложите чрез подаване на PR).

**Тази глава включва**

1. [Квантификация на Phi-3.5 / 4 с llama.cpp](./UsingLlamacppQuantifyingPhi.md)

2. [Квантификация на Phi-3.5 / 4 с разширения за Generative AI за onnxruntime](./UsingORTGenAIQuantifyingPhi.md)

3. [Квантификация на Phi-3.5 / 4 с Intel OpenVINO](./UsingIntelOpenVINOQuantifyingPhi.md)

4. [Квантификация на Phi-3.5 / 4 с рамката Apple MLX](./UsingAppleMLXQuantifyingPhi.md)

**Отказ от отговорност**:  
Този документ е преведен с помощта на услуги за машинен превод, базирани на изкуствен интелект. Въпреки че се стремим към точност, имайте предвид, че автоматизираните преводи може да съдържат грешки или неточности. Оригиналният документ на неговия изходен език трябва да се счита за авторитетен източник. За критична информация се препоръчва професионален превод от човек. Не носим отговорност за каквито и да било недоразумения или погрешни тълкувания, възникнали от използването на този превод.