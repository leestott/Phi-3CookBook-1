# **Квантизация на Phi Family с помощта на llama.cpp**

## **Какво е llama.cpp**

llama.cpp е софтуерна библиотека с отворен код, написана основно на C++, която извършва инференция върху различни Големи Езикови Модели (LLMs), като Llama. Основната ѝ цел е да предостави водеща в индустрията производителност за инференция на LLM върху широк спектър от хардуер с минимална настройка. Освен това библиотеката предлага Python обвивки, които осигуряват високо ниво API за текстово допълване и сървър, съвместим с OpenAI.

Основната цел на llama.cpp е да позволи инференция на LLM с минимална настройка и водеща производителност на разнообразен хардуер - както локално, така и в облака.

- Изчистена C/C++ имплементация без зависимости
- Apple silicon е приоритет - оптимизирана чрез ARM NEON, Accelerate и Metal frameworks
- Поддръжка на AVX, AVX2 и AVX512 за x86 архитектури
- Квантизация на 1.5-бита, 2-бита, 3-бита, 4-бита, 5-бита, 6-бита и 8-бита за по-бърза инференция и намалена употреба на памет
- Специализирани CUDA ядра за работа с LLM върху NVIDIA GPU-та (поддръжка за AMD GPU-та чрез HIP)
- Поддръжка на Vulkan и SYCL бекенд
- Хибридна инференция CPU+GPU за частично ускорение на модели, по-големи от общия капацитет на VRAM

## **Квантизация на Phi-3.5 с llama.cpp**

Моделът Phi-3.5-Instruct може да бъде квантизиран с помощта на llama.cpp, но Phi-3.5-Vision и Phi-3.5-MoE все още не се поддържат. Форматът, конвертиран от llama.cpp, е gguf, който също е най-широко използваният формат за квантизация.

В Hugging Face има голям брой модели в квантизиран GGUF формат. AI Foundry, Ollama и LlamaEdge разчитат на llama.cpp, така че GGUF моделите често се използват.

### **Какво е GGUF**

GGUF е бинарен формат, оптимизиран за бързо зареждане и записване на модели, което го прави изключително ефективен за инференция. GGUF е проектиран за използване с GGML и други изпълнители. GGUF е разработен от @ggerganov, който също е разработчикът на llama.cpp, популярна рамка за инференция на LLM, написана на C/C++. Модели, първоначално създадени в рамки като PyTorch, могат да бъдат конвертирани в GGUF формат за използване с тези енджини.

### **ONNX срещу GGUF**

ONNX е традиционен формат за машинно обучение/дълбоко обучение, който е добре поддържан в различни AI рамки и има добри приложения в edge устройства. Що се отнася до GGUF, той е базиран на llama.cpp и може да се каже, че е създаден в ерата на GenAI. Двата формата имат сходни приложения. Ако искате по-добра производителност в embedded хардуер и приложни слоеве, ONNX може да бъде вашият избор. Ако използвате производни рамки и технологии на llama.cpp, тогава GGUF може да е по-добрият избор.

### **Квантизация на Phi-3.5-Instruct с помощта на llama.cpp**

**1. Конфигурация на средата**


```bash

git clone https://github.com/ggerganov/llama.cpp.git

cd llama.cpp

make -j8

```


**2. Квантизация**

С помощта на llama.cpp конвертирайте Phi-3.5-Instruct в FP16 GGUF


```bash

./convert_hf_to_gguf.py <Your Phi-3.5-Instruct Location> --outfile phi-3.5-128k-mini_fp16.gguf

```

Квантизация на Phi-3.5 до INT4


```bash

./llama.cpp/llama-quantize <Your phi-3.5-128k-mini_fp16.gguf location> ./gguf/phi-3.5-128k-mini_Q4_K_M.gguf Q4_K_M

```


**3. Тестване**

Инсталирайте llama-cpp-python


```bash

pip install llama-cpp-python -U

```

***Забележка*** 

Ако използвате Apple Silicon, инсталирайте llama-cpp-python по този начин


```bash

CMAKE_ARGS="-DLLAMA_METAL=on" pip install llama-cpp-python -U

```

Тестване 


```bash

llama.cpp/llama-cli --model <Your phi-3.5-128k-mini_Q4_K_M.gguf location> --prompt "<|user|>\nCan you introduce .NET<|end|>\n<|assistant|>\n"  --gpu-layers 10

```



## **Ресурси**

1. Научете повече за llama.cpp [https://onnxruntime.ai/docs/genai/](https://onnxruntime.ai/docs/genai/)

2. Научете повече за GGUF [https://huggingface.co/docs/hub/en/gguf](https://huggingface.co/docs/hub/en/gguf)

**Отказ от отговорност**:  
Този документ е преведен с помощта на автоматизирани AI услуги за превод. Въпреки че се стремим към точност, моля, имайте предвид, че автоматизираните преводи могат да съдържат грешки или неточности. Оригиналният документ на неговия оригинален език трябва да се счита за авторитетен източник. За критична информация се препоръчва професионален превод от човек. Не носим отговорност за каквито и да било недоразумения или погрешни интерпретации, възникнали в резултат от използването на този превод.