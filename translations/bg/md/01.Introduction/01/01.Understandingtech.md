# Ключови технологии включват

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - ниско ниво API за хардуерно ускорено машинно обучение, изградено върху DirectX 12.
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - платформа за паралелно изчисление и модел за програмни интерфейси (API), разработена от Nvidia, която позволява общо предназначение за обработка на графични процесори (GPUs).
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - отворен формат, създаден за представяне на модели за машинно обучение, който осигурява съвместимост между различни ML рамки.
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - формат, използван за представяне и обновяване на модели за машинно обучение, особено полезен за по-малки езикови модели, които могат ефективно да работят на CPU с 4-8 битова квантизация.

## DirectML

DirectML е ниско ниво API, което позволява хардуерно ускорено машинно обучение. Построено е върху DirectX 12, за да използва ускорение от GPU, и е независимо от конкретен доставчик, което означава, че не изисква промени в кода, за да работи с различни GPU производители. Основно се използва за обучение на модели и инференция върху GPU.

Що се отнася до хардуерната поддръжка, DirectML е проектиран да работи с широк спектър от GPU, включително интегрирани и дискретни AMD GPU, интегрирани Intel GPU и дискретни NVIDIA GPU. Той е част от Windows AI платформата и се поддържа на Windows 10 и 11, позволявайки обучение на модели и инференция на всяко Windows устройство.

Има актуализации и възможности, свързани с DirectML, като например поддръжка на до 150 ONNX оператора и използване както от ONNX runtime, така и от WinML. Подкрепен е от основни производители на интегриран хардуер (IHVs), всеки от които имплементира различни мета-команди.

## CUDA

CUDA, което означава Compute Unified Device Architecture, е платформа за паралелно изчисление и модел за програмни интерфейси (API), създадена от Nvidia. Тя позволява на софтуерните разработчици да използват графичен процесор (GPU), съвместим с CUDA, за общо предназначение – подход, известен като GPGPU (General-Purpose computing on Graphics Processing Units). CUDA е ключов фактор за GPU ускорението на Nvidia и се използва широко в различни области, включително машинно обучение, научни изчисления и видеообработка.

Хардуерната поддръжка за CUDA е специфична за GPU на Nvidia, тъй като това е патентована технология, разработена от Nvidia. Всяка архитектура поддържа специфични версии на CUDA инструменти, които предоставят необходимите библиотеки и инструменти за разработчиците да изграждат и изпълняват CUDA приложения.

## ONNX

ONNX (Open Neural Network Exchange) е отворен формат, създаден за представяне на модели за машинно обучение. Той предоставя дефиниция на разширяем модел на изчислителен граф, както и дефиниции на вградени оператори и стандартни типове данни. ONNX позволява на разработчиците да прехвърлят модели между различни ML рамки, осигурявайки съвместимост и улеснявайки създаването и внедряването на AI приложения.

Phi3 mini може да работи с ONNX Runtime на CPU и GPU на различни устройства, включително сървърни платформи, Windows, Linux и Mac десктопи, както и мобилни CPU. Оптимизираните конфигурации, които сме добавили, са:

- ONNX модели за int4 DML: Квантизирани до int4 чрез AWQ
- ONNX модел за fp16 CUDA
- ONNX модел за int4 CUDA: Квантизиран до int4 чрез RTN
- ONNX модел за int4 CPU и мобилни устройства: Квантизиран до int4 чрез RTN

## Llama.cpp

Llama.cpp е софтуерна библиотека с отворен код, написана на C++. Тя извършва инференция на различни Големи Езикови Модели (LLMs), включително Llama. Разработена заедно с библиотеката ggml (универсална библиотека за тензори), llama.cpp има за цел да осигури по-бърза инференция и по-ниска консумация на памет в сравнение с оригиналната Python имплементация. Тя поддържа хардуерна оптимизация, квантизация и предлага опростен API и примери. Ако се интересувате от ефективна инференция на LLM, llama.cpp си струва да бъде разгледан, тъй като Phi3 може да работи с Llama.cpp.

## GGUF

GGUF (Generic Graph Update Format) е формат, използван за представяне и обновяване на модели за машинно обучение. Той е особено полезен за по-малки езикови модели (SLMs), които могат ефективно да работят на CPU с 4-8 битова квантизация. GGUF е полезен за бързо прототипиране и изпълнение на модели на edge устройства или в групови задачи като CI/CD процеси.

**Отказ от отговорност**:  
Този документ е преведен с помощта на услуги за машинен превод, базирани на изкуствен интелект. Въпреки че се стремим към точност, моля, имайте предвид, че автоматичните преводи може да съдържат грешки или неточности. Оригиналният документ на неговия оригинален език трябва да се счита за авторитетен източник. За критична информация се препоръчва професионален човешки превод. Не носим отговорност за каквито и да било недоразумения или погрешни тълкувания, произтичащи от използването на този превод.