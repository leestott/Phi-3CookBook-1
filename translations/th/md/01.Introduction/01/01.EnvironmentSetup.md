# เริ่มต้นใช้งาน Phi-3 ในเครื่องของคุณ

คู่มือนี้จะช่วยคุณตั้งค่าสภาพแวดล้อมในเครื่องเพื่อรันโมเดล Phi-3 ด้วย Ollama คุณสามารถรันโมเดลได้หลายวิธี เช่น การใช้ GitHub Codespaces, VS Code Dev Containers หรือในสภาพแวดล้อมในเครื่องของคุณเอง

## การตั้งค่าสภาพแวดล้อม

### GitHub Codespaces

คุณสามารถรันเทมเพลตนี้ในรูปแบบเสมือนจริงผ่าน GitHub Codespaces ปุ่มนี้จะเปิด VS Code ในเว็บเบราว์เซอร์ของคุณ:

1. เปิดเทมเพลต (อาจใช้เวลาหลายนาที):

    [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/microsoft/phi-3cookbook)

2. เปิดหน้าต่างเทอร์มินัล

### VS Code Dev Containers

⚠️ ตัวเลือกนี้จะทำงานได้ก็ต่อเมื่อ Docker Desktop ของคุณถูกจัดสรร RAM อย่างน้อย 16 GB หากคุณมี RAM น้อยกว่า 16 GB คุณสามารถลองใช้ [GitHub Codespaces](../../../../../md/01.Introduction/01) หรือ [ตั้งค่าในเครื่อง](../../../../../md/01.Introduction/01)

อีกตัวเลือกหนึ่งคือ VS Code Dev Containers ซึ่งจะเปิดโปรเจกต์ใน VS Code บนเครื่องของคุณโดยใช้ [Dev Containers extension](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers):

1. เริ่มต้น Docker Desktop (ติดตั้งถ้ายังไม่ได้ติดตั้ง)
2. เปิดโปรเจกต์:

    [![Open in Dev Containers](https://img.shields.io/static/v1?style=for-the-badge&label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/microsoft/phi-3cookbook)

3. ในหน้าต่าง VS Code ที่เปิดขึ้น เมื่อไฟล์โปรเจกต์แสดงขึ้นมา (อาจใช้เวลาหลายนาที) ให้เปิดหน้าต่างเทอร์มินัล
4. ดำเนินการต่อด้วย [ขั้นตอนการปรับใช้](../../../../../md/01.Introduction/01)

### สภาพแวดล้อมในเครื่อง

1. ตรวจสอบให้แน่ใจว่าคุณติดตั้งเครื่องมือดังต่อไปนี้แล้ว:

    * [Ollama](https://ollama.com/)
    * [Python 3.10+](https://www.python.org/downloads/)
    * [OpenAI Python SDK](https://pypi.org/project/openai/)

## ทดสอบโมเดล

1. สั่งให้ Ollama ดาวน์โหลดและรันโมเดล phi3:mini:

    ```shell
    ollama run phi3:mini
    ```

    การดาวน์โหลดโมเดลจะใช้เวลาสักครู่

2. เมื่อคุณเห็นคำว่า "success" ในผลลัพธ์ คุณสามารถส่งข้อความไปยังโมเดลนั้นจากพรอมต์ได้

    ```shell
    >>> Write a haiku about hungry hippos
    ```

3. หลังจากผ่านไปไม่กี่วินาที คุณจะเห็นการตอบกลับจากโมเดลที่สตรีมออกมา

4. หากต้องการเรียนรู้เกี่ยวกับเทคนิคต่างๆ ที่ใช้กับโมเดลภาษา ให้เปิด Python notebook [ollama.ipynb](../../../../../code/01.Introduce/ollama.ipynb) และรันแต่ละเซลล์ หากคุณใช้โมเดลที่ไม่ใช่ 'phi3:mini' ให้เปลี่ยน `MODEL_NAME` in the first cell.

5. To have a conversation with the phi3:mini model from Python, open the Python file [chat.py](../../../../../code/01.Introduce/chat.py) and run it. You can change the `MODEL_NAME` ที่ด้านบนของไฟล์ตามความเหมาะสม และคุณยังสามารถแก้ไขข้อความระบบหรือเพิ่มตัวอย่างแบบ few-shot ได้หากต้องการ

**คำปฏิเสธความรับผิดชอบ**:  
เอกสารนี้ได้รับการแปลโดยใช้บริการแปลภาษาอัตโนมัติด้วย AI แม้ว่าเราจะพยายามอย่างเต็มที่เพื่อให้ได้ความถูกต้อง แต่โปรดทราบว่าการแปลอัตโนมัติอาจมีข้อผิดพลาดหรือความไม่แม่นยำ เอกสารต้นฉบับในภาษาดั้งเดิมควรถือเป็นแหล่งข้อมูลที่เชื่อถือได้ สำหรับข้อมูลที่มีความสำคัญ ขอแนะนำให้ใช้บริการแปลภาษาจากผู้เชี่ยวชาญ เราไม่รับผิดชอบต่อความเข้าใจผิดหรือการตีความผิดพลาดที่เกิดจากการใช้การแปลนี้