# เทคโนโลยีสำคัญที่กล่าวถึง

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - API ระดับต่ำสำหรับการเร่งความเร็วการเรียนรู้ของเครื่องด้วยฮาร์ดแวร์ พัฒนาบนพื้นฐานของ DirectX 12
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - แพลตฟอร์มการประมวลผลแบบขนานและ API ที่พัฒนาโดย Nvidia เพื่อให้สามารถประมวลผลทั่วไปบนหน่วยประมวลผลกราฟิก (GPU)
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - รูปแบบเปิดที่ออกแบบมาเพื่อเป็นตัวแทนของโมเดลการเรียนรู้ของเครื่อง และช่วยให้สามารถใช้งานร่วมกันได้ระหว่างเฟรมเวิร์ก ML ต่าง ๆ
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - รูปแบบที่ใช้สำหรับการแทนและอัปเดตโมเดลการเรียนรู้ของเครื่อง เหมาะสำหรับโมเดลภาษาขนาดเล็กที่สามารถทำงานได้อย่างมีประสิทธิภาพบน CPU ด้วยการควอนไทซ์ 4-8 บิต

## DirectML

DirectML เป็น API ระดับต่ำที่ช่วยให้การเรียนรู้ของเครื่องสามารถเร่งความเร็วด้วยฮาร์ดแวร์ได้ พัฒนาบนพื้นฐานของ DirectX 12 เพื่อใช้ประโยชน์จากการเร่งความเร็วของ GPU และไม่ขึ้นกับผู้ผลิตฮาร์ดแวร์ ซึ่งหมายความว่าไม่จำเป็นต้องเปลี่ยนโค้ดเพื่อใช้งานกับ GPU จากผู้ผลิตรายต่าง ๆ โดยหลักแล้วใช้สำหรับการฝึกโมเดลและการทำงาน inference บน GPU

ในด้านการรองรับฮาร์ดแวร์ DirectML ถูกออกแบบมาให้ทำงานกับ GPU หลากหลายประเภท เช่น AMD (ทั้งแบบรวมและแยก), Intel (แบบรวม) และ NVIDIA (แบบแยก) โดยเป็นส่วนหนึ่งของ Windows AI Platform และรองรับบน Windows 10 และ 11 ซึ่งช่วยให้สามารถฝึกโมเดลและทำ inference บนอุปกรณ์ Windows ได้ทุกเครื่อง

มีการอัปเดตและโอกาสที่เกี่ยวข้องกับ DirectML เช่น การรองรับ ONNX operators ได้มากถึง 150 ตัว และถูกใช้งานโดยทั้ง ONNX runtime และ WinML นอกจากนี้ยังได้รับการสนับสนุนจากผู้ผลิตฮาร์ดแวร์รายใหญ่ (IHVs) ที่แต่ละรายมีการใช้งาน metacommand ต่าง ๆ

## CUDA

CUDA หรือ Compute Unified Device Architecture เป็นแพลตฟอร์มการประมวลผลแบบขนานและ API ที่พัฒนาโดย Nvidia ซึ่งช่วยให้นักพัฒนาซอฟต์แวร์สามารถใช้ GPU ที่รองรับ CUDA สำหรับการประมวลผลทั่วไป หรือที่เรียกว่า GPGPU (General-Purpose computing on Graphics Processing Units) CUDA เป็นปัจจัยสำคัญที่ช่วยให้ Nvidia สามารถเร่งความเร็ว GPU ได้ และถูกใช้อย่างกว้างขวางในหลายสาขา เช่น การเรียนรู้ของเครื่อง การคำนวณเชิงวิทยาศาสตร์ และการประมวลผลวิดีโอ

การรองรับฮาร์ดแวร์ของ CUDA นั้นจำกัดเฉพาะ GPU ของ Nvidia เนื่องจากเป็นเทคโนโลยีที่เป็นกรรมสิทธิ์ของ Nvidia แต่ละสถาปัตยกรรมของ GPU จะรองรับเวอร์ชันเฉพาะของชุดเครื่องมือ CUDA ซึ่งมีไลบรารีและเครื่องมือที่จำเป็นสำหรับนักพัฒนาในการสร้างและรันแอปพลิเคชัน CUDA

## ONNX

ONNX (Open Neural Network Exchange) เป็นรูปแบบเปิดที่ออกแบบมาเพื่อเป็นตัวแทนของโมเดลการเรียนรู้ของเครื่อง โดยให้คำจำกัดความของโมเดลกราฟการคำนวณที่ขยายได้ รวมถึงคำจำกัดความของตัวดำเนินการในตัวและประเภทข้อมูลมาตรฐาน ONNX ช่วยให้นักพัฒนาสามารถย้ายโมเดลระหว่างเฟรมเวิร์ก ML ต่าง ๆ ได้ง่ายขึ้น ทำให้สามารถใช้งานร่วมกันได้และง่ายต่อการสร้างและปรับใช้แอปพลิเคชัน AI

Phi3 mini สามารถทำงานร่วมกับ ONNX Runtime บน CPU และ GPU ในอุปกรณ์หลากหลายประเภท รวมถึงแพลตฟอร์มเซิร์ฟเวอร์ เดสก์ท็อป Windows, Linux, Mac และ CPU บนอุปกรณ์พกพา การกำหนดค่าที่ปรับแต่งไว้มีดังนี้:

- โมเดล ONNX สำหรับ int4 DML: ควอนไทซ์เป็น int4 ผ่าน AWQ
- โมเดล ONNX สำหรับ fp16 CUDA
- โมเดล ONNX สำหรับ int4 CUDA: ควอนไทซ์เป็น int4 ผ่าน RTN
- โมเดล ONNX สำหรับ int4 CPU และ Mobile: ควอนไทซ์เป็น int4 ผ่าน RTN

## Llama.cpp

Llama.cpp เป็นไลบรารีซอฟต์แวร์โอเพ่นซอร์สที่พัฒนาในภาษา C++ ซึ่งใช้สำหรับ inference บนโมเดลภาษาขนาดใหญ่ (LLMs) หลายตัว รวมถึง Llama ไลบรารีนี้พัฒนาควบคู่ไปกับไลบรารี ggml (ไลบรารี tensor อเนกประสงค์) โดยมีเป้าหมายเพื่อให้การทำ inference เร็วขึ้นและใช้หน่วยความจำน้อยลงเมื่อเทียบกับการใช้งานใน Python เวอร์ชันดั้งเดิม นอกจากนี้ยังรองรับการปรับแต่งฮาร์ดแวร์ การควอนไทซ์ และมี API ที่ใช้งานง่ายพร้อมตัวอย่างการใช้งาน หากคุณสนใจการทำ inference โมเดล LLM ที่มีประสิทธิภาพ Llama.cpp ถือเป็นตัวเลือกที่น่าสนใจ โดย Phi3 สามารถใช้งาน Llama.cpp ได้

## GGUF

GGUF (Generic Graph Update Format) เป็นรูปแบบที่ใช้สำหรับการแทนและอัปเดตโมเดลการเรียนรู้ของเครื่อง โดยเฉพาะอย่างยิ่งโมเดลภาษาขนาดเล็ก (SLMs) ที่สามารถทำงานได้อย่างมีประสิทธิภาพบน CPU ด้วยการควอนไทซ์ 4-8 บิต GGUF เหมาะสำหรับการสร้างต้นแบบอย่างรวดเร็วและการรันโมเดลบนอุปกรณ์ edge หรือในงานแบตช์ เช่น CI/CD pipeline

**ข้อจำกัดความรับผิดชอบ**:  
เอกสารนี้ได้รับการแปลโดยใช้บริการแปลภาษาอัตโนมัติที่ใช้ AI แม้ว่าเราจะพยายามอย่างเต็มที่เพื่อให้การแปลมีความถูกต้อง แต่โปรดทราบว่าการแปลอัตโนมัติอาจมีข้อผิดพลาดหรือความไม่แม่นยำ เอกสารต้นฉบับในภาษาดั้งเดิมควรถือเป็นแหล่งข้อมูลที่เชื่อถือได้ หากเป็นข้อมูลสำคัญ แนะนำให้ใช้บริการแปลภาษาจากผู้เชี่ยวชาญ เราไม่รับผิดชอบต่อความเข้าใจผิดหรือการตีความผิดที่เกิดจากการใช้การแปลนี้