ในบริบทของ Phi-3-mini การอนุมานหมายถึงกระบวนการใช้โมเดลเพื่อทำการคาดการณ์หรือสร้างผลลัพธ์ตามข้อมูลที่ป้อนเข้า ต่อไปนี้คือรายละเอียดเพิ่มเติมเกี่ยวกับ Phi-3-mini และความสามารถด้านการอนุมานของมัน

Phi-3-mini เป็นส่วนหนึ่งของชุดโมเดล Phi-3 ที่เปิดตัวโดย Microsoft โมเดลเหล่านี้ถูกออกแบบมาเพื่อกำหนดนิยามใหม่ให้กับความสามารถของ Small Language Models (SLMs)

นี่คือประเด็นสำคัญเกี่ยวกับ Phi-3-mini และความสามารถด้านการอนุมาน:

## **ภาพรวมของ Phi-3-mini:**
- Phi-3-mini มีขนาดพารามิเตอร์ 3.8 พันล้าน
- สามารถทำงานได้ไม่เพียงแต่บนอุปกรณ์คอมพิวเตอร์ทั่วไป แต่ยังรวมถึงอุปกรณ์ Edge เช่น อุปกรณ์พกพาและอุปกรณ์ IoT
- การเปิดตัว Phi-3-mini ทำให้บุคคลและองค์กรสามารถปรับใช้ SLMs บนอุปกรณ์ฮาร์ดแวร์ที่หลากหลาย โดยเฉพาะในสภาพแวดล้อมที่มีทรัพยากรจำกัด
- รองรับรูปแบบโมเดลที่หลากหลาย เช่น รูปแบบ PyTorch แบบดั้งเดิม, รูปแบบ gguf ที่ถูกบีบอัด, และรูปแบบ ONNX ที่ถูกบีบอัด

## **การเข้าถึง Phi-3-mini:**
เพื่อเข้าถึง Phi-3-mini คุณสามารถใช้ [Semantic Kernel](https://github.com/microsoft/SemanticKernelCookBook?WT.mc_id=aiml-138114-kinfeylo) ในแอปพลิเคชัน Copilot ได้ Semantic Kernel รองรับการใช้งานร่วมกับ Azure OpenAI Service, โมเดลโอเพ่นซอร์สบน Hugging Face, และโมเดลในเครื่อง
คุณยังสามารถใช้ [Ollama](https://ollama.com) หรือ [LlamaEdge](https://llamaedge.com) เพื่อเรียกใช้โมเดลที่ถูกบีบอัด Ollama ช่วยให้ผู้ใช้ทั่วไปสามารถเรียกใช้โมเดลที่ถูกบีบอัดต่างๆ ได้ ในขณะที่ LlamaEdge ช่วยให้สามารถใช้งาน GGUF โมเดลได้ข้ามแพลตฟอร์ม

## **โมเดลที่ถูกบีบอัด:**
ผู้ใช้หลายคนมักเลือกใช้โมเดลที่ถูกบีบอัดสำหรับการอนุมานในเครื่อง ตัวอย่างเช่น คุณสามารถเรียกใช้ Ollama run Phi-3 ได้โดยตรง หรือกำหนดค่าแบบออฟไลน์โดยใช้ Modelfile ซึ่งระบุเส้นทางไฟล์ GGUF และรูปแบบคำสั่งที่ป้อนเข้า

## **ความเป็นไปได้ของ Generative AI:**
การรวม SLMs อย่าง Phi-3-mini เข้าด้วยกันเปิดโอกาสใหม่ๆ สำหรับ Generative AI การอนุมานเป็นเพียงก้าวแรก โมเดลเหล่านี้สามารถนำไปใช้ในงานต่างๆ ในสภาพแวดล้อมที่มีทรัพยากรจำกัด, มีข้อจำกัดด้านเวลาแฝง, และข้อจำกัดด้านต้นทุน

## **ปลดล็อก Generative AI ด้วย Phi-3-mini: คู่มือการอนุมานและการปรับใช้**
เรียนรู้วิธีใช้ Semantic Kernel, Ollama/LlamaEdge, และ ONNX Runtime เพื่อเข้าถึงและอนุมานโมเดล Phi-3-mini และสำรวจความเป็นไปได้ของ Generative AI ในสถานการณ์การใช้งานที่หลากหลาย

**คุณสมบัติ**
การอนุมานโมเดล phi3-mini ใน:

- [Semantic Kernel](https://github.com/Azure-Samples/Phi-3MiniSamples/tree/main/semantickernel?WT.mc_id=aiml-138114-kinfeylo)
- [Ollama](https://github.com/Azure-Samples/Phi-3MiniSamples/tree/main/ollama?WT.mc_id=aiml-138114-kinfeylo)
- [LlamaEdge WASM](https://github.com/Azure-Samples/Phi-3MiniSamples/tree/main/wasm?WT.mc_id=aiml-138114-kinfeylo)
- [ONNX Runtime](https://github.com/Azure-Samples/Phi-3MiniSamples/tree/main/onnx?WT.mc_id=aiml-138114-kinfeylo)
- [iOS](https://github.com/Azure-Samples/Phi-3MiniSamples/tree/main/ios?WT.mc_id=aiml-138114-kinfeylo)

สรุปได้ว่า Phi-3-mini ช่วยให้นักพัฒนาสามารถสำรวจรูปแบบโมเดลที่หลากหลายและใช้ประโยชน์จาก Generative AI ในสถานการณ์การใช้งานที่หลากหลาย

**ข้อจำกัดความรับผิดชอบ**:  
เอกสารฉบับนี้ได้รับการแปลโดยใช้บริการแปลภาษาด้วย AI อัตโนมัติ แม้ว่าเราจะพยายามให้การแปลมีความถูกต้องที่สุด แต่โปรดทราบว่าการแปลด้วยระบบอัตโนมัติอาจมีข้อผิดพลาดหรือความไม่แม่นยำเกิดขึ้น เอกสารต้นฉบับในภาษาต้นทางควรถูกพิจารณาให้เป็นแหล่งข้อมูลที่เชื่อถือได้ สำหรับข้อมูลที่มีความสำคัญ ขอแนะนำให้ใช้บริการแปลภาษามนุษย์ที่เป็นมืออาชีพ เราไม่รับผิดชอบต่อความเข้าใจผิดหรือการตีความที่คลาดเคลื่อนซึ่งอาจเกิดขึ้นจากการใช้การแปลนี้