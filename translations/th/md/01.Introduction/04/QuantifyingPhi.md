# **การทำควอนไทเซชันตระกูล Phi**

การทำควอนไทเซชันของโมเดลหมายถึงกระบวนการแมปพารามิเตอร์ (เช่น ค่าน้ำหนักและค่าการกระตุ้น) ในโมเดลเครือข่ายประสาทเทียมจากช่วงค่าที่กว้าง (โดยปกติเป็นช่วงค่าต่อเนื่อง) ไปยังช่วงค่าที่เล็กลงและมีขอบเขตจำกัด เทคโนโลยีนี้ช่วยลดขนาดและความซับซ้อนในการคำนวณของโมเดล และเพิ่มประสิทธิภาพการทำงานของโมเดลในสภาพแวดล้อมที่มีทรัพยากรจำกัด เช่น อุปกรณ์มือถือหรือระบบฝังตัว การทำควอนไทเซชันของโมเดลช่วยให้เกิดการบีบอัดโดยการลดความละเอียดของพารามิเตอร์ แต่ก็อาจทำให้เกิดการสูญเสียความแม่นยำบางส่วน ดังนั้น ในกระบวนการควอนไทเซชันจึงจำเป็นต้องสร้างสมดุลระหว่างขนาดโมเดล ความซับซ้อนในการคำนวณ และความแม่นยำ วิธีการควอนไทเซชันที่พบบ่อย ได้แก่ การควอนไทเซชันแบบจุดคงที่ (Fixed-point) และแบบจุดลอยตัว (Floating-point) คุณสามารถเลือกกลยุทธ์ควอนไทเซชันที่เหมาะสมตามสถานการณ์และความต้องการเฉพาะ

เราหวังที่จะนำโมเดล GenAI ไปใช้งานบนอุปกรณ์เอดจ์ เพื่อให้มีอุปกรณ์ที่สามารถเข้าสู่การใช้งาน GenAI ได้มากขึ้น เช่น อุปกรณ์มือถือ, AI PC / Copilot PC และอุปกรณ์ IoT แบบดั้งเดิม ผ่านโมเดลควอนไทเซชัน เราสามารถปรับใช้โมเดลในอุปกรณ์เอดจ์ที่แตกต่างกันได้ตามข้อจำกัดของอุปกรณ์นั้น ๆ โดยการผสานรวมกับเฟรมเวิร์คการเร่งความเร็วของโมเดลและโมเดลควอนไทเซชันที่ผู้ผลิตฮาร์ดแวร์ให้มา เราสามารถสร้างสถานการณ์การใช้งาน SLM ที่ดียิ่งขึ้นได้

ในกรณีของควอนไทเซชัน เรามีความละเอียดที่แตกต่างกัน (INT4, INT8, FP16, FP32) รายละเอียดของความละเอียดที่ใช้บ่อยมีดังนี้

### **INT4**

INT4 เป็นวิธีการควอนไทเซชันที่ค่อนข้างรุนแรง โดยจะทำการควอนไทเซชันน้ำหนักและค่าการกระตุ้นของโมเดลให้เป็นจำนวนเต็ม 4 บิต INT4 มักทำให้เกิดการสูญเสียความแม่นยำมากขึ้นเนื่องจากช่วงค่าที่เล็กลงและความละเอียดที่ต่ำกว่า อย่างไรก็ตาม เมื่อเทียบกับ INT8 แล้ว INT4 สามารถลดความต้องการในการจัดเก็บและความซับซ้อนในการคำนวณของโมเดลได้มากขึ้น ควรสังเกตว่า INT4 ไม่ค่อยถูกใช้งานในทางปฏิบัติ เนื่องจากความแม่นยำที่ต่ำเกินไปอาจทำให้ประสิทธิภาพของโมเดลลดลงอย่างมาก นอกจากนี้ ฮาร์ดแวร์บางชนิดอาจไม่รองรับการทำงานแบบ INT4 ดังนั้นจึงต้องพิจารณาความเข้ากันได้ของฮาร์ดแวร์เมื่อเลือกวิธีการควอนไทเซชัน

### **INT8**

INT8 เป็นกระบวนการแปลงน้ำหนักและค่าการกระตุ้นของโมเดลจากตัวเลขจุดลอยตัวให้เป็นจำนวนเต็ม 8 บิต แม้ว่าช่วงค่าที่ INT8 สามารถแทนได้จะเล็กลงและมีความละเอียดน้อยกว่า แต่ก็ช่วยลดความต้องการในการจัดเก็บและการคำนวณได้อย่างมาก ในการทำควอนไทเซชันแบบ INT8 น้ำหนักและค่าการกระตุ้นของโมเดลจะผ่านกระบวนการควอนไทเซชัน ซึ่งรวมถึงการปรับสเกลและการชดเชย เพื่อรักษาข้อมูลจุดลอยตัวเดิมให้ได้มากที่สุด ในระหว่างการอนุมาน ค่าที่ถูกควอนไทเซชันเหล่านี้จะถูกแปลงกลับเป็นจุดลอยตัวเพื่อใช้ในการคำนวณ และจากนั้นจะถูกควอนไทเซชันกลับไปเป็น INT8 สำหรับขั้นตอนถัดไป วิธีนี้สามารถให้ความแม่นยำเพียงพอในแอปพลิเคชันส่วนใหญ่ พร้อมทั้งรักษาประสิทธิภาพการคำนวณที่สูง

### **FP16**

รูปแบบ FP16 หรือจำนวนจุดลอยตัว 16 บิต (float16) ลดการใช้หน่วยความจำลงครึ่งหนึ่งเมื่อเทียบกับจำนวนจุดลอยตัว 32 บิต (float32) ซึ่งมีข้อได้เปรียบอย่างมากในแอปพลิเคชันการเรียนรู้เชิงลึกขนาดใหญ่ รูปแบบ FP16 ช่วยให้สามารถโหลดโมเดลขนาดใหญ่ขึ้นหรือประมวลผลข้อมูลได้มากขึ้นภายใต้ข้อจำกัดของหน่วยความจำ GPU เดียวกัน ด้วยการสนับสนุนการทำงานแบบ FP16 ที่เพิ่มขึ้นในฮาร์ดแวร์ GPU สมัยใหม่ การใช้รูปแบบ FP16 อาจช่วยเพิ่มความเร็วในการคำนวณได้ อย่างไรก็ตาม FP16 มีข้อเสียโดยธรรมชาติคือความละเอียดที่ต่ำกว่า ซึ่งอาจทำให้เกิดความไม่เสถียรทางตัวเลขหรือการสูญเสียความแม่นยำในบางกรณี

### **FP32**

รูปแบบ FP32 มีความแม่นยำสูงกว่าและสามารถแทนค่าที่หลากหลายได้อย่างแม่นยำ ในกรณีที่ต้องการการคำนวณทางคณิตศาสตร์ที่ซับซ้อนหรือผลลัพธ์ที่มีความแม่นยำสูง รูปแบบ FP32 จะเป็นตัวเลือกที่เหมาะสม อย่างไรก็ตาม ความแม่นยำที่สูงนี้ยังหมายถึงการใช้หน่วยความจำมากขึ้นและเวลาในการคำนวณที่นานขึ้น สำหรับโมเดลการเรียนรู้เชิงลึกขนาดใหญ่ โดยเฉพาะเมื่อมีพารามิเตอร์โมเดลจำนวนมากและข้อมูลจำนวนมหาศาล รูปแบบ FP32 อาจทำให้หน่วยความจำ GPU ไม่เพียงพอหรือความเร็วในการอนุมานลดลง

สำหรับอุปกรณ์มือถือหรืออุปกรณ์ IoT เราสามารถแปลงโมเดล Phi-3.x เป็น INT4 ได้ ในขณะที่ AI PC / Copilot PC สามารถใช้ความละเอียดที่สูงกว่า เช่น INT8, FP16, FP32

ปัจจุบัน ผู้ผลิตฮาร์ดแวร์แต่ละรายมีเฟรมเวิร์คที่แตกต่างกันเพื่อรองรับโมเดล Generative เช่น OpenVINO ของ Intel, QNN ของ Qualcomm, MLX ของ Apple และ CUDA ของ Nvidia โดยสามารถผสานรวมกับการทำควอนไทเซชันของโมเดลเพื่อการปรับใช้ในเครื่อง

ในด้านเทคโนโลยี เรามีรูปแบบการสนับสนุนที่แตกต่างกันหลังจากการควอนไทเซชัน เช่น รูปแบบ PyTorch / Tensorflow, GGUF และ ONNX ฉันได้ทำการเปรียบเทียบรูปแบบและสถานการณ์การใช้งานระหว่าง GGUF และ ONNX ซึ่งแนะนำรูปแบบควอนไทเซชัน ONNX ที่มีการสนับสนุนที่ดีจากเฟรมเวิร์คโมเดลไปจนถึงฮาร์ดแวร์ ในบทนี้ เราจะมุ่งเน้นไปที่ ONNX Runtime สำหรับ GenAI, OpenVINO และ Apple MLX เพื่อทำควอนไทเซชันโมเดล (หากคุณมีวิธีที่ดีกว่า คุณสามารถส่ง PR มาให้เราได้)

**บทนี้ประกอบด้วย**

1. [การทำควอนไทเซชัน Phi-3.5 / 4 ด้วย llama.cpp](./UsingLlamacppQuantifyingPhi.md)

2. [การทำควอนไทเซชัน Phi-3.5 / 4 ด้วย Generative AI extensions สำหรับ onnxruntime](./UsingORTGenAIQuantifyingPhi.md)

3. [การทำควอนไทเซชัน Phi-3.5 / 4 ด้วย Intel OpenVINO](./UsingIntelOpenVINOQuantifyingPhi.md)

4. [การทำควอนไทเซชัน Phi-3.5 / 4 ด้วย Apple MLX Framework](./UsingAppleMLXQuantifyingPhi.md)

**ข้อจำกัดความรับผิดชอบ**:  
เอกสารนี้ได้รับการแปลโดยใช้บริการแปลภาษาอัตโนมัติด้วย AI แม้ว่าเราจะพยายามให้การแปลมีความถูกต้อง แต่โปรดทราบว่าการแปลอัตโนมัติอาจมีข้อผิดพลาดหรือความไม่แม่นยำ เอกสารต้นฉบับในภาษาดั้งเดิมควรถูกพิจารณาเป็นแหล่งข้อมูลที่เชื่อถือได้ สำหรับข้อมูลที่สำคัญ ขอแนะนำให้ใช้บริการแปลภาษาจากผู้เชี่ยวชาญ เราไม่รับผิดชอบต่อความเข้าใจผิดหรือการตีความผิดที่เกิดจากการใช้การแปลนี้