# Kľúčové technológie zahŕňajú

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - nízkoúrovňové API pre hardvérovo akcelerované strojové učenie postavené na DirectX 12.
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - platforma pre paralelné výpočty a aplikačné programovacie rozhranie (API) vyvinuté spoločnosťou Nvidia, ktorá umožňuje všeobecné spracovanie na grafických procesoroch (GPU).
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - otvorený formát navrhnutý na reprezentáciu modelov strojového učenia, ktorý zabezpečuje interoperabilitu medzi rôznymi ML frameworkmi.
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - formát používaný na reprezentáciu a aktualizáciu modelov strojového učenia, obzvlášť užitočný pre menšie jazykové modely, ktoré efektívne fungujú na CPU so 4-8bitovou kvantizáciou.

## DirectML

DirectML je nízkoúrovňové API, ktoré umožňuje hardvérovo akcelerované strojové učenie. Je postavené na DirectX 12, aby využívalo GPU akceleráciu, a je nezávislé od dodávateľa, čo znamená, že nevyžaduje zmeny v kóde na fungovanie s rôznymi výrobcami GPU. Primárne sa používa na tréning a inferenciu modelov na GPU.

Čo sa týka hardvérovej podpory, DirectML je navrhnuté tak, aby fungovalo so širokou škálou GPU, vrátane integrovaných a diskrétnych GPU od AMD, integrovaných GPU od Intelu a diskrétnych GPU od Nvidie. Je súčasťou Windows AI Platform a podporované na Windows 10 a 11, čo umožňuje tréning a inferenciu modelov na akomkoľvek zariadení s Windows.

K DirectML boli pridané aktualizácie a možnosti, ako napríklad podpora až 150 ONNX operátorov a použitie v ONNX runtime a WinML. Je podporované hlavnými výrobcami hardvéru (IHV), pričom každý z nich implementuje rôzne metapríkazy.

## CUDA

CUDA, čo znamená Compute Unified Device Architecture, je platforma pre paralelné výpočty a aplikačné programovacie rozhranie (API) vyvinuté spoločnosťou Nvidia. Umožňuje softvérovým vývojárom využívať CUDA-kompatibilné grafické procesory (GPU) na všeobecné spracovanie – prístup známy ako GPGPU (General-Purpose computing on Graphics Processing Units). CUDA je kľúčovým prvkom akcelerácie GPU od Nvidie a je široko používané v rôznych oblastiach, vrátane strojového učenia, vedeckých výpočtov a spracovania videa.

Hardvérová podpora pre CUDA je špecifická pre GPU od Nvidie, keďže ide o proprietárnu technológiu vyvinutú spoločnosťou Nvidia. Každá architektúra podporuje konkrétne verzie CUDA toolkit, ktorý poskytuje potrebné knižnice a nástroje pre vývojárov na vytváranie a spúšťanie CUDA aplikácií.

## ONNX

ONNX (Open Neural Network Exchange) je otvorený formát navrhnutý na reprezentáciu modelov strojového učenia. Poskytuje definíciu rozšíriteľného výpočtového grafu, ako aj definície vstavaných operátorov a štandardných dátových typov. ONNX umožňuje vývojárom presúvať modely medzi rôznymi ML frameworkmi, čím zabezpečuje interoperabilitu a uľahčuje vytváranie a nasadzovanie AI aplikácií.

Phi3 mini dokáže fungovať s ONNX Runtime na CPU a GPU na rôznych zariadeniach, vrátane serverových platforiem, Windows, Linux a Mac desktopov a mobilných CPU. Optimalizované konfigurácie, ktoré sme pridali, zahŕňajú:

- ONNX modely pre int4 DML: Kvantizované na int4 pomocou AWQ
- ONNX model pre fp16 CUDA
- ONNX model pre int4 CUDA: Kvantizované na int4 pomocou RTN
- ONNX model pre int4 CPU a mobilné zariadenia: Kvantizované na int4 pomocou RTN

## Llama.cpp

Llama.cpp je open-source softvérová knižnica napísaná v C++. Vykonáva inferenciu na rôznych veľkých jazykových modeloch (LLM), vrátane Llama. Bola vyvinutá spolu s knižnicou ggml (všeobecná knižnica pre tenzory) a jej cieľom je poskytovať rýchlejšiu inferenciu a nižšiu spotrebu pamäte v porovnaní s pôvodnou implementáciou v Pythone. Podporuje hardvérovú optimalizáciu, kvantizáciu a ponúka jednoduché API a príklady. Ak máte záujem o efektívnu inferenciu LLM, Llama.cpp stojí za preskúmanie, pretože Phi3 dokáže spustiť Llama.cpp.

## GGUF

GGUF (Generic Graph Update Format) je formát používaný na reprezentáciu a aktualizáciu modelov strojového učenia. Je obzvlášť užitočný pre menšie jazykové modely (SLM), ktoré efektívne fungujú na CPU so 4-8bitovou kvantizáciou. GGUF je výhodný pre rýchle prototypovanie a spúšťanie modelov na edge zariadeniach alebo v dávkových úlohách, ako sú CI/CD pipeline.

**Upozornenie**:  
Tento dokument bol preložený pomocou služieb strojového prekladu AI. Aj keď sa snažíme o presnosť, berte na vedomie, že automatizované preklady môžu obsahovať chyby alebo nepresnosti. Za autoritatívny zdroj by sa mal považovať pôvodný dokument v jeho pôvodnom jazyku. Pre kritické informácie sa odporúča profesionálny preklad od človeka. Nenesieme zodpovednosť za akékoľvek nedorozumenia alebo nesprávne interpretácie vyplývajúce z použitia tohto prekladu.