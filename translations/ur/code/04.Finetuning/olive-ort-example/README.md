# اولیو کے ذریعے Phi3 کو فائن ٹون کریں

اس مثال میں آپ اولیو کا استعمال کریں گے تاکہ:

1. LoRA ایڈاپٹر کو فائن ٹون کریں تاکہ جملوں کو Sad، Joy، Fear، Surprise میں تقسیم کیا جا سکے۔
1. ایڈاپٹر کے وزن کو بیس ماڈل میں ضم کریں۔
1. ماڈل کو `int4` میں بہتر اور کوانٹائز کریں۔

ہم آپ کو یہ بھی دکھائیں گے کہ ONNX Runtime (ORT) Generate API کا استعمال کرتے ہوئے فائن ٹون کیے گئے ماڈل پر کس طرح انفیرنس کریں۔

> **⚠️ فائن ٹوننگ کے لیے، آپ کے پاس ایک مناسب GPU دستیاب ہونا ضروری ہے - جیسے کہ A10، V100، A100۔**

## 💾 انسٹال کریں

ایک نیا پائتھن ورچوئل ماحول بنائیں (مثال کے طور پر، `conda` کا استعمال کرتے ہوئے):

```bash
conda create -n olive-ai python=3.11
conda activate olive-ai
```

اس کے بعد، اولیو اور فائن ٹوننگ ورک فلو کے لیے ضروری انحصارات انسٹال کریں:

```bash
cd Phi-3CookBook/code/04.Finetuning/olive-ort-example
pip install olive-ai[gpu]
pip install -r requirements.txt
```

## 🧪 اولیو کے ذریعے Phi3 کو فائن ٹون کریں
[اولیو کنفیگریشن فائل](../../../../../code/04.Finetuning/olive-ort-example/phrase-classification.json) میں ایک *ورک فلو* موجود ہے جس میں درج ذیل *پاسز* شامل ہیں:

Phi3 -> LoRA -> MergeAdapterWeights -> ModelBuilder

اعلی سطح پر، یہ ورک فلو درج ذیل کام کرے گا:

1. Phi3 کو فائن ٹون کریں (150 مراحل کے لیے، جسے آپ تبدیل کر سکتے ہیں) [dataset/data-classification.json](../../../../../code/04.Finetuning/olive-ort-example/dataset/dataset-classification.json) ڈیٹا کا استعمال کرتے ہوئے۔
1. LoRA ایڈاپٹر کے وزن کو بیس ماڈل میں ضم کریں۔ اس کے نتیجے میں آپ کو ONNX فارمیٹ میں ایک واحد ماڈل آرٹیفیکٹ ملے گا۔
1. ماڈل بلڈر ماڈل کو ONNX رن ٹائم کے لیے بہتر بنائے گا *اور* ماڈل کو `int4` میں کوانٹائز کرے گا۔

ورک فلو کو چلانے کے لیے:

```bash
olive run --config phrase-classification.json
```

جب اولیو مکمل ہو جائے، تو آپ کا بہتر `int4` فائن ٹون کیا ہوا Phi3 ماڈل یہاں دستیاب ہوگا: `code/04.Finetuning/olive-ort-example/models/lora-merge-mb/gpu-cuda_model`۔

## 🧑‍💻 اپنی ایپلی کیشن میں فائن ٹون کیا ہوا Phi3 ضم کریں 

ایپ چلانے کے لیے:

```bash
python app/app.py --phrase "cricket is a wonderful sport!" --model-path models/lora-merge-mb/gpu-cuda_model
```

یہ جواب جملے کی ایک واحد لفظ کی درجہ بندی ہونا چاہیے (Sad/Joy/Fear/Surprise)۔

**ڈسکلیمر**:  
یہ دستاویز مشین پر مبنی AI ترجمہ سروسز کا استعمال کرتے ہوئے ترجمہ کی گئی ہے۔ اگرچہ ہم درستگی کے لیے کوشش کرتے ہیں، براہ کرم آگاہ رہیں کہ خودکار ترجمے میں غلطیاں یا غیر درستیاں ہو سکتی ہیں۔ اصل دستاویز کو اس کی اصل زبان میں مستند ذریعہ سمجھا جانا چاہیے۔ اہم معلومات کے لیے، پیشہ ور انسانی ترجمہ کی سفارش کی جاتی ہے۔ ہم اس ترجمے کے استعمال سے پیدا ہونے والی کسی بھی غلط فہمی یا غلط تشریح کے ذمہ دار نہیں ہیں۔