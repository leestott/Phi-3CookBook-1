# **لیب 2 - AIPC میں Phi-3-mini کے ساتھ پرامپٹ فلو چلائیں**

## **پرامپٹ فلو کیا ہے؟**

پرامپٹ فلو ایک ڈیولپمنٹ ٹولز کا مجموعہ ہے جو LLM پر مبنی AI ایپلیکیشنز کے پورے ڈیولپمنٹ سائیکل کو آسان اور ہموار بناتا ہے، جس میں آئیڈیاز بنانا، پروٹوٹائپنگ، ٹیسٹنگ، ایویلیویشن، پروڈکشن ڈیپلائمنٹ اور مانیٹرنگ شامل ہیں۔ یہ پرامپٹ انجینئرنگ کو نہایت آسان بناتا ہے اور آپ کو پروڈکشن کوالٹی کے ساتھ LLM ایپلیکیشنز بنانے کے قابل بناتا ہے۔

پرامپٹ فلو کے ساتھ، آپ یہ کر سکتے ہیں:

- ایسے فلو بنائیں جو LLMs، پرامپٹس، Python کوڈ اور دیگر ٹولز کو ایک قابل عمل ورک فلو میں جوڑیں۔

- اپنے فلو کو ڈیبگ اور بہتر بنائیں، خاص طور پر LLMs کے ساتھ تعامل کو آسانی سے۔

- اپنے فلو کا تجزیہ کریں، بڑے ڈیٹا سیٹس کے ساتھ کوالٹی اور پرفارمنس میٹرکس کا حساب لگائیں۔

- اپنے CI/CD سسٹم میں ٹیسٹنگ اور ایویلیویشن کو شامل کریں تاکہ آپ کے فلو کی کوالٹی یقینی بنائی جا سکے۔

- اپنے فلو کو اپنی پسند کے سرونگ پلیٹ فارم پر ڈیپلائے کریں یا آسانی سے اپنی ایپ کے کوڈ بیس میں شامل کریں۔

- (اختیاری لیکن سختی سے تجویز کردہ) Azure AI میں پرامپٹ فلو کے کلاؤڈ ورژن کا استعمال کرکے اپنی ٹیم کے ساتھ تعاون کریں۔

## **AIPC کیا ہے؟**

ایک AI PC میں ایک CPU، GPU اور NPU ہوتا ہے، جن میں ہر ایک کے پاس مخصوص AI ایکسیلریشن کی صلاحیتیں ہوتی ہیں۔ NPU، یا نیورل پروسیسنگ یونٹ، ایک خاص ایکسیلریٹر ہے جو مصنوعی ذہانت (AI) اور مشین لرننگ (ML) کے کاموں کو براہ راست آپ کے PC پر انجام دیتا ہے، بجائے اس کے کہ ڈیٹا کلاؤڈ پر بھیجا جائے۔ GPU اور CPU بھی ان کاموں کو انجام دے سکتے ہیں، لیکن NPU خاص طور پر کم پاور AI کیلکولیشنز میں ماہر ہے۔ AI PC ہمارے کمپیوٹرز کے کام کرنے کے طریقے میں ایک بنیادی تبدیلی کی نمائندگی کرتا ہے۔ یہ کسی ایسے مسئلے کا حل نہیں ہے جو پہلے موجود نہ ہو، بلکہ یہ روزمرہ PC کے استعمالات کے لیے ایک بہتری کا وعدہ کرتا ہے۔

تو یہ کیسے کام کرتا ہے؟ جنریٹو AI اور بڑے پیمانے پر بڑے لینگویج ماڈلز (LLMs) کے مقابلے میں، جو عوامی ڈیٹا کے انبار پر تربیت یافتہ ہوتے ہیں، آپ کے PC پر ہونے والا AI تقریباً ہر سطح پر زیادہ قابل رسائی ہے۔ یہ تصور زیادہ آسان ہے، اور چونکہ یہ آپ کے ڈیٹا پر تربیت یافتہ ہوتا ہے، بغیر کلاؤڈ تک رسائی کی ضرورت کے، اس کے فوائد زیادہ وسیع پیمانے پر فوری طور پر پرکشش ہیں۔

قریب المدت میں، AI PC دنیا میں ذاتی معاونین اور چھوٹے AI ماڈلز شامل ہیں جو براہ راست آپ کے PC پر چلتے ہیں، آپ کے ڈیٹا کو استعمال کرتے ہوئے ذاتی، نجی، اور زیادہ محفوظ AI بہتریاں فراہم کرتے ہیں، جو آپ روزمرہ کے کاموں میں کرتے ہیں – جیسے میٹنگ کے نوٹس لینا، فینٹسی فٹبال لیگ کو منظم کرنا، فوٹو اور ویڈیو ایڈیٹنگ کے لیے خودکار بہتریاں فراہم کرنا، یا سب کے آنے اور جانے کے اوقات کی بنیاد پر فیملی ری یونین کے لیے بہترین منصوبہ بندی کرنا۔

## **AIPC پر جنریشن کوڈ فلو بنانا**

***نوٹ***: اگر آپ نے ماحول کی تنصیب مکمل نہیں کی ہے، تو براہ کرم [لیب 0 - انسٹالیشنز](./01.Installations.md) دیکھیں۔

1. Visual Studio Code میں پرامپٹ فلو ایکسٹینشن کھولیں اور ایک خالی فلو پروجیکٹ بنائیں۔

![create](../../../../../../../../../translated_images/pf_create.d6172d8277a78a7fa82cd6ff727ed44e037fa78b662f1f62d5963f36d712d229.ur.png)

2. ان پٹ اور آؤٹ پٹ پیرامیٹرز شامل کریں اور Python کوڈ کو ایک نئے فلو کے طور پر شامل کریں۔

![flow](../../../../../../../../../translated_images/pf_flow.d5646a323fb7f444c0b98b4521057a592325c583e7ba18bc31500bc0415e9ef3.ur.png)

آپ اس اسٹرکچر (flow.dag.yaml) کا حوالہ دے کر اپنا فلو بنا سکتے ہیں:

```yaml

inputs:
  question:
    type: string
    default: how to write Bubble Algorithm
outputs:
  answer:
    type: string
    reference: ${Chat_With_Phi3.output}
nodes:
- name: Chat_With_Phi3
  type: python
  source:
    type: code
    path: Chat_With_Phi3.py
  inputs:
    question: ${inputs.question}


```

3. ***Chat_With_Phi3.py*** میں کوڈ شامل کریں۔

```python


from promptflow.core import tool

# import torch
from transformers import AutoTokenizer, pipeline,TextStreamer
import intel_npu_acceleration_library as npu_lib

import warnings

import asyncio
import platform

class Phi3CodeAgent:
    
    model = None
    tokenizer = None
    text_streamer = None
    
    model_id = "microsoft/Phi-3-mini-4k-instruct"

    @staticmethod
    def init_phi3():
        
        if Phi3CodeAgent.model is None or Phi3CodeAgent.tokenizer is None or Phi3CodeAgent.text_streamer is None:
            Phi3CodeAgent.model = npu_lib.NPUModelForCausalLM.from_pretrained(
                                    Phi3CodeAgent.model_id,
                                    torch_dtype="auto",
                                    dtype=npu_lib.int4,
                                    trust_remote_code=True
                                )
            Phi3CodeAgent.tokenizer = AutoTokenizer.from_pretrained(Phi3CodeAgent.model_id)
            Phi3CodeAgent.text_streamer = TextStreamer(Phi3CodeAgent.tokenizer, skip_prompt=True)

    

    @staticmethod
    def chat_with_phi3(prompt):
        
        Phi3CodeAgent.init_phi3()

        messages = "<|system|>You are a AI Python coding assistant. Please help me to generate code in Python.The answer only genertated Python code, but any comments and instructions do not need to be generated<|end|><|user|>" + prompt +"<|end|><|assistant|>"



        generation_args = {
            "max_new_tokens": 1024,
            "return_full_text": False,
            "temperature": 0.3,
            "do_sample": False,
            "streamer": Phi3CodeAgent.text_streamer,
        }

        pipe = pipeline(
            "text-generation",
            model=Phi3CodeAgent.model,
            tokenizer=Phi3CodeAgent.tokenizer,
            # **generation_args
        )

        result = ''

        with warnings.catch_warnings():
            warnings.simplefilter("ignore")
            response = pipe(messages, **generation_args)
            result =response[0]['generated_text']
            return result


@tool
def my_python_tool(question: str) -> str:
    if platform.system() == 'Windows':
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    return Phi3CodeAgent.chat_with_phi3(question)


```

4. فلو کو ڈیبگ یا رن سے ٹیسٹ کریں تاکہ یہ چیک کیا جا سکے کہ جنریشن کوڈ درست ہے یا نہیں۔

![RUN](../../../../../../../../../translated_images/pf_run.d918637dc00f61e9bdeec37d4cc9646f77d270ac9203bcce13569f3157202b6e.ur.png)

5. ٹرمینل میں فلو کو ڈیولپمنٹ API کے طور پر چلائیں۔

```

pf flow serve --source ./ --port 8080 --host localhost   

```

آپ اسے Postman یا Thunder Client میں ٹیسٹ کر سکتے ہیں۔

### **نوٹ**

1. پہلی بار چلانے میں کافی وقت لگتا ہے۔ Hugging Face CLI سے phi-3 ماڈل ڈاؤن لوڈ کرنے کی سفارش کی جاتی ہے۔

2. Intel NPU کی محدود کمپیوٹنگ پاور کو مدنظر رکھتے ہوئے، Phi-3-mini-4k-instruct استعمال کرنے کی سفارش کی جاتی ہے۔

3. ہم Intel NPU ایکسیلریشن کا استعمال کرتے ہیں تاکہ INT4 کنورژن کو کوانٹائز کیا جا سکے، لیکن اگر آپ سروس کو دوبارہ چلائیں تو آپ کو کیشے اور nc_workshop فولڈرز کو حذف کرنا ہوگا۔

## **وسائل**

1. پرامپٹ فلو سیکھیں [https://microsoft.github.io/promptflow/](https://microsoft.github.io/promptflow/)

2. Intel NPU ایکسیلریشن سیکھیں [https://github.com/intel/intel-npu-acceleration-library](https://github.com/intel/intel-npu-acceleration-library)

3. نمونہ کوڈ ڈاؤن لوڈ کریں [مقامی NPU ایجنٹ نمونہ کوڈ](../../../../../../../../../code/07.Lab/01/AIPC)

**ڈسکلیمر**:  
یہ دستاویز مشین پر مبنی اے آئی ترجمہ خدمات کا استعمال کرتے ہوئے ترجمہ کی گئی ہے۔ اگرچہ ہم درستگی کے لیے کوشش کرتے ہیں، براہ کرم آگاہ رہیں کہ خودکار ترجمے میں غلطیاں یا غلط فہمیاں ہو سکتی ہیں۔ اصل دستاویز، جو اس کی اصل زبان میں ہے، کو مستند ذریعہ سمجھا جانا چاہیے۔ اہم معلومات کے لیے، پیشہ ور انسانی ترجمہ کی سفارش کی جاتی ہے۔ ہم اس ترجمے کے استعمال سے پیدا ہونے والی کسی بھی غلط فہمی یا غلط تشریح کے ذمہ دار نہیں ہیں۔