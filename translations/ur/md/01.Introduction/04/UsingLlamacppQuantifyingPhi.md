# **llama.cpp کا استعمال کرتے ہوئے Phi فیملی کو کوانٹائز کرنا**

## **llama.cpp کیا ہے**

llama.cpp ایک اوپن سورس سافٹ ویئر لائبریری ہے جو بنیادی طور پر C++ میں لکھی گئی ہے اور مختلف بڑے زبان کے ماڈلز (LLMs) جیسے کہ Llama پر انفرنس کرتی ہے۔ اس کا بنیادی مقصد مختلف ہارڈویئر پر جدید ترین کارکردگی کے ساتھ LLM انفرنس کو کم سے کم سیٹ اپ کے ساتھ فراہم کرنا ہے۔ مزید برآں، اس لائبریری کے لیے Python بائنڈنگز بھی دستیاب ہیں، جو ٹیکسٹ کمپلیشن کے لیے ایک ہائی لیول API اور OpenAI کے مطابق ویب سرور پیش کرتی ہیں۔

llama.cpp کا بنیادی مقصد یہ ہے کہ کم سے کم سیٹ اپ کے ساتھ جدید ترین کارکردگی اور مختلف قسم کے ہارڈویئر پر LLM انفرنس کو ممکن بنایا جائے - چاہے وہ مقامی ہو یا کلاؤڈ پر۔

- سادہ C/C++ پر مبنی عمل درآمد، بغیر کسی اضافی ڈپینڈنسی کے  
- Apple silicon کے لیے خصوصی آپٹمائزیشن - ARM NEON، Accelerate اور Metal فریم ورک کے ذریعے  
- x86 آرکیٹیکچرز کے لیے AVX، AVX2 اور AVX512 کی سپورٹ  
- تیز تر انفرنس اور کم میموری استعمال کے لیے 1.5-bit، 2-bit، 3-bit، 4-bit، 5-bit، 6-bit، اور 8-bit انٹیجر کوانٹائزیشن  
- NVIDIA GPUs پر LLM چلانے کے لیے کسٹم CUDA کرنلز (AMD GPUs کے لیے HIP کے ذریعے سپورٹ)  
- Vulkan اور SYCL بیک اینڈ سپورٹ  
- CPU+GPU ہائبرڈ انفرنس، جو ایسے ماڈلز کو جزوی طور پر تیز کرتا ہے جو کل VRAM کی گنجائش سے بڑے ہوں  

## **llama.cpp کے ذریعے Phi-3.5 کو کوانٹائز کرنا**

Phi-3.5-Instruct ماڈل کو llama.cpp کے ذریعے کوانٹائز کیا جا سکتا ہے، لیکن Phi-3.5-Vision اور Phi-3.5-MoE کو ابھی سپورٹ نہیں کیا گیا ہے۔ llama.cpp کے ذریعے کنورٹ کیا گیا فارمیٹ GGUF ہے، جو سب سے زیادہ استعمال ہونے والا کوانٹائزیشن فارمیٹ بھی ہے۔

Hugging Face پر GGUF فارمیٹ میں بڑی تعداد میں کوانٹائزڈ ماڈلز دستیاب ہیں۔ AI Foundry، Ollama، اور LlamaEdge llama.cpp پر انحصار کرتے ہیں، لہذا GGUF ماڈلز بھی اکثر استعمال کیے جاتے ہیں۔

### **GGUF کیا ہے**

GGUF ایک بائنری فارمیٹ ہے جو ماڈلز کو تیزی سے لوڈ اور محفوظ کرنے کے لیے آپٹمائز کیا گیا ہے، اور انفرنس کے لیے اسے انتہائی موثر بناتا ہے۔ GGUF GGML اور دیگر ایگزیکیوٹرز کے ساتھ استعمال کے لیے ڈیزائن کیا گیا ہے۔ GGUF کو @ggerganov نے تیار کیا، جو کہ llama.cpp کے ڈویلپر بھی ہیں، جو ایک مشہور C/C++ LLM انفرنس فریم ورک ہے۔ ایسے ماڈلز جو ابتدا میں PyTorch جیسے فریم ورک میں تیار کیے گئے ہوں، انہیں GGUF فارمیٹ میں کنورٹ کیا جا سکتا ہے تاکہ ان انجنز کے ساتھ استعمال ہو سکیں۔

### **ONNX بمقابلہ GGUF**

ONNX ایک روایتی مشین لرننگ/ڈیپ لرننگ فارمیٹ ہے، جو مختلف AI فریم ورکس میں اچھی طرح سے سپورٹ کیا جاتا ہے اور ایج ڈیوائسز میں عمدہ استعمال کے مواقع فراہم کرتا ہے۔ جہاں تک GGUF کا تعلق ہے، یہ llama.cpp پر مبنی ہے اور اسے GenAI کے دور میں تیار کیا گیا ہے۔ دونوں کے استعمال ایک جیسے ہیں۔ اگر آپ کو ایمبیڈڈ ہارڈویئر اور ایپلیکیشن لیئرز میں بہتر کارکردگی چاہیے تو ONNX آپ کا انتخاب ہو سکتا ہے۔ اگر آپ llama.cpp کے ڈیریویٹو فریم ورک اور ٹیکنالوجی استعمال کرتے ہیں تو GGUF بہتر ہو سکتا ہے۔

### **llama.cpp کے ذریعے Phi-3.5-Instruct کو کوانٹائز کرنا**

**1. ماحول کی ترتیب**  

```bash

git clone https://github.com/ggerganov/llama.cpp.git

cd llama.cpp

make -j8

```  

**2. کوانٹائزیشن**  

llama.cpp کا استعمال کرتے ہوئے Phi-3.5-Instruct کو FP16 GGUF میں تبدیل کریں  

```bash

./convert_hf_to_gguf.py <Your Phi-3.5-Instruct Location> --outfile phi-3.5-128k-mini_fp16.gguf

```  

Phi-3.5 کو INT4 میں کوانٹائز کریں  

```bash

./llama.cpp/llama-quantize <Your phi-3.5-128k-mini_fp16.gguf location> ./gguf/phi-3.5-128k-mini_Q4_K_M.gguf Q4_K_M

```  

**3. جانچ**  

llama-cpp-python انسٹال کریں  

```bash

pip install llama-cpp-python -U

```  

***نوٹ***  

اگر آپ Apple Silicon استعمال کرتے ہیں تو براہ کرم llama-cpp-python کو اس طرح انسٹال کریں  

```bash

CMAKE_ARGS="-DLLAMA_METAL=on" pip install llama-cpp-python -U

```  

جانچ  

```bash

llama.cpp/llama-cli --model <Your phi-3.5-128k-mini_Q4_K_M.gguf location> --prompt "<|user|>\nCan you introduce .NET<|end|>\n<|assistant|>\n"  --gpu-layers 10

```  

## **وسائل**

1. llama.cpp کے بارے میں مزید جانیں [https://onnxruntime.ai/docs/genai/](https://onnxruntime.ai/docs/genai/)  

2. GGUF کے بارے میں مزید جانیں [https://huggingface.co/docs/hub/en/gguf](https://huggingface.co/docs/hub/en/gguf)  

**ڈسکلیمر**:  
یہ دستاویز مشین پر مبنی AI ترجمہ خدمات کا استعمال کرتے ہوئے ترجمہ کی گئی ہے۔ ہم درستگی کے لیے کوشش کرتے ہیں، لیکن براہ کرم آگاہ رہیں کہ خودکار ترجمے میں غلطیاں یا غیر درستیاں ہو سکتی ہیں۔ اصل دستاویز کو اس کی اصل زبان میں مستند ذریعہ سمجھا جانا چاہیے۔ اہم معلومات کے لیے، پیشہ ور انسانی ترجمہ کی سفارش کی جاتی ہے۔ ہم اس ترجمے کے استعمال سے پیدا ہونے والی کسی بھی غلط فہمی یا غلط تشریح کے ذمہ دار نہیں ہیں۔