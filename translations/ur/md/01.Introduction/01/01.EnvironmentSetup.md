# Phi-3 کو مقامی طور پر شروع کریں

یہ گائیڈ آپ کو اپنے مقامی ماحول میں Phi-3 ماڈل کو Ollama کے ذریعے چلانے کے لیے سیٹ اپ کرنے میں مدد کرے گا۔ آپ یہ ماڈل مختلف طریقوں سے چلا سکتے ہیں، جیسے GitHub Codespaces، VS Code Dev Containers، یا اپنے مقامی ماحول میں۔

## ماحول کی ترتیب

### GitHub Codespaces

آپ GitHub Codespaces کا استعمال کرتے ہوئے اس ٹیمپلیٹ کو ورچوئل طور پر چلا سکتے ہیں۔ یہ بٹن آپ کے براؤزر میں ایک ویب بیسڈ VS Code انسٹینس کھول دے گا:

1. ٹیمپلیٹ کھولیں (اس میں چند منٹ لگ سکتے ہیں):

    [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/microsoft/phi-3cookbook)

2. ایک ٹرمینل ونڈو کھولیں۔

### VS Code Dev Containers

⚠️ یہ آپشن صرف اس وقت کام کرے گا اگر آپ کے Docker Desktop کو کم از کم 16 GB RAM الاٹ کیا گیا ہو۔ اگر آپ کے پاس 16 GB RAM سے کم ہے، تو آپ [GitHub Codespaces آپشن](../../../../../md/01.Introduction/01) آزما سکتے ہیں یا [اسے مقامی طور پر سیٹ اپ کریں](../../../../../md/01.Introduction/01)۔

ایک متعلقہ آپشن VS Code Dev Containers ہے، جو آپ کے مقامی VS Code میں [Dev Containers ایکسٹینشن](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers) کا استعمال کرتے ہوئے پروجیکٹ کھول دے گا:

1. Docker Desktop شروع کریں (اگر پہلے سے انسٹال نہیں ہے تو اسے انسٹال کریں)۔
2. پروجیکٹ کھولیں:

    [![Open in Dev Containers](https://img.shields.io/static/v1?style=for-the-badge&label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/microsoft/phi-3cookbook)

3. جو VS Code ونڈو کھلتی ہے، اس میں پروجیکٹ فائلز نظر آنے کے بعد (اس میں چند منٹ لگ سکتے ہیں)، ایک ٹرمینل ونڈو کھولیں۔
4. [ڈپلائمنٹ کے مراحل](../../../../../md/01.Introduction/01) کے ساتھ جاری رکھیں۔

### مقامی ماحول

1. یقینی بنائیں کہ درج ذیل ٹولز انسٹال ہیں:

    * [Ollama](https://ollama.com/)
    * [Python 3.10+](https://www.python.org/downloads/)
    * [OpenAI Python SDK](https://pypi.org/project/openai/)

## ماڈل کو ٹیسٹ کریں

1. Ollama سے phi3:mini ماڈل ڈاؤن لوڈ اور چلانے کو کہیں:

    ```shell
    ollama run phi3:mini
    ```

    ماڈل ڈاؤن لوڈ کرنے میں چند منٹ لگیں گے۔

2. جب آپ آؤٹ پٹ میں "success" دیکھیں، تو آپ اس ماڈل کو پرامپٹ سے پیغام بھیج سکتے ہیں۔

    ```shell
    >>> Write a haiku about hungry hippos
    ```

3. چند سیکنڈز کے بعد، آپ کو ماڈل سے ایک جواب آتا ہوا نظر آئے گا۔

4. زبان کے ماڈلز کے ساتھ استعمال ہونے والی مختلف تکنیکوں کے بارے میں جاننے کے لیے، Python نوٹ بک [ollama.ipynb](../../../../../code/01.Introduce/ollama.ipynb) کھولیں اور ہر سیل کو چلائیں۔ اگر آپ نے 'phi3:mini' کے علاوہ کوئی اور ماڈل استعمال کیا ہے، تو فائل کے اوپر `MODEL_NAME` in the first cell.

5. To have a conversation with the phi3:mini model from Python, open the Python file [chat.py](../../../../../code/01.Introduce/chat.py) and run it. You can change the `MODEL_NAME` کو اپنی ضرورت کے مطابق تبدیل کریں، اور آپ سسٹم میسج یا چند اضافی مثالیں بھی شامل کر سکتے ہیں۔

**ڈسکلیمر**:  
یہ دستاویز مشین پر مبنی AI ترجمہ خدمات کا استعمال کرتے ہوئے ترجمہ کی گئی ہے۔ اگرچہ ہم درستگی کے لیے کوشش کرتے ہیں، براہ کرم آگاہ رہیں کہ خودکار ترجمے میں غلطیاں یا خامیاں ہو سکتی ہیں۔ اصل دستاویز کو اس کی اصل زبان میں مستند ذریعہ سمجھا جانا چاہیے۔ اہم معلومات کے لیے، پیشہ ور انسانی ترجمہ تجویز کیا جاتا ہے۔ ہم اس ترجمے کے استعمال سے پیدا ہونے والی کسی بھی غلط فہمی یا غلط تشریح کے ذمہ دار نہیں ہیں۔