# Mga pangunahing teknolohiya na binanggit ay kinabibilangan ng

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - isang low-level API para sa hardware-accelerated machine learning na nakabase sa ibabaw ng DirectX 12.
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - isang parallel computing platform at application programming interface (API) model na binuo ng Nvidia, na nagpapahintulot sa general-purpose processing sa graphics processing units (GPUs).
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - isang bukas na format na idinisenyo upang kumatawan sa mga modelo ng machine learning na nagbibigay ng interoperability sa pagitan ng iba't ibang ML frameworks.
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - isang format na ginagamit para sa representasyon at pag-update ng mga modelo ng machine learning, partikular na kapaki-pakinabang para sa mas maliliit na language models na epektibong tumatakbo sa CPUs gamit ang 4-8bit quantization.

## DirectML

Ang DirectML ay isang low-level API na nagbibigay-daan sa hardware-accelerated machine learning. Ito ay nakabase sa ibabaw ng DirectX 12 upang magamit ang GPU acceleration at vendor-agnostic, ibig sabihin, hindi kinakailangang baguhin ang code upang gumana sa iba't ibang GPU vendors. Karaniwan itong ginagamit para sa model training at inferencing workloads sa GPUs.

Pagdating sa hardware support, ang DirectML ay idinisenyo upang gumana sa malawak na hanay ng GPUs, kabilang ang AMD integrated at discrete GPUs, Intel integrated GPUs, at NVIDIA discrete GPUs. Ito ay bahagi ng Windows AI Platform at sinusuportahan sa Windows 10 & 11, na nagpapahintulot sa model training at inferencing sa anumang Windows device.

May mga update at oportunidad na kaugnay ng DirectML, tulad ng pagsuporta sa hanggang 150 ONNX operators at paggamit nito sa parehong ONNX runtime at WinML. Sinusuportahan ito ng mga pangunahing Integrated Hardware Vendors (IHVs), na bawat isa ay nag-implement ng iba't ibang metacommands.

## CUDA

Ang CUDA, na nangangahulugang Compute Unified Device Architecture, ay isang parallel computing platform at application programming interface (API) model na nilikha ng Nvidia. Pinapayagan nito ang mga software developers na gamitin ang isang CUDA-enabled graphics processing unit (GPU) para sa general-purpose processing â€“ isang pamamaraan na tinatawag na GPGPU (General-Purpose computing on Graphics Processing Units). Ang CUDA ay isang mahalagang teknolohiya para sa GPU acceleration ng Nvidia at malawakang ginagamit sa iba't ibang larangan, kabilang ang machine learning, scientific computing, at video processing.

Ang hardware support para sa CUDA ay eksklusibo sa mga GPUs ng Nvidia, dahil ito ay isang proprietary na teknolohiya na binuo ng Nvidia. Ang bawat architecture ay sumusuporta sa partikular na bersyon ng CUDA toolkit, na nagbibigay ng kinakailangang libraries at tools para sa mga developer upang bumuo at magpatakbo ng CUDA applications.

## ONNX

Ang ONNX (Open Neural Network Exchange) ay isang bukas na format na idinisenyo upang kumatawan sa mga modelo ng machine learning. Nagbibigay ito ng depinisyon ng isang extensible computation graph model, gayundin ng mga depinisyon ng built-in operators at standard data types. Pinapayagan ng ONNX ang mga developer na ilipat ang mga modelo sa pagitan ng iba't ibang ML frameworks, na nagbibigay-daan sa interoperability at nagpapadali sa paglikha at pag-deploy ng mga AI applications.

Ang Phi3 mini ay maaaring tumakbo gamit ang ONNX Runtime sa CPU at GPU sa iba't ibang devices, kabilang ang server platforms, Windows, Linux at Mac desktops, at mobile CPUs. Ang mga na-optimize na configuration na idinagdag namin ay:

- ONNX models para sa int4 DML: Na-quantize sa int4 gamit ang AWQ
- ONNX model para sa fp16 CUDA
- ONNX model para sa int4 CUDA: Na-quantize sa int4 gamit ang RTN
- ONNX model para sa int4 CPU at Mobile: Na-quantize sa int4 gamit ang RTN

## Llama.cpp

Ang Llama.cpp ay isang open-source na software library na isinulat sa C++. Gumagawa ito ng inference sa iba't ibang Large Language Models (LLMs), kabilang ang Llama. Ito ay binuo kasama ang ggml library (isang general-purpose tensor library), at layunin ng llama.cpp na magbigay ng mas mabilis na inference at mas mababang memory usage kumpara sa orihinal na Python implementation. Sinusuportahan nito ang hardware optimization, quantization, at nag-aalok ng simpleng API at mga halimbawa. Kung interesado ka sa efficient LLM inference, ang llama.cpp ay dapat subukan dahil ang Phi3 ay maaaring tumakbo gamit ang Llama.cpp.

## GGUF

Ang GGUF (Generic Graph Update Format) ay isang format na ginagamit para sa representasyon at pag-update ng mga modelo ng machine learning. Ito ay partikular na kapaki-pakinabang para sa mas maliliit na language models (SLMs) na maaaring epektibong tumakbo sa CPUs gamit ang 4-8bit quantization. Ang GGUF ay kapaki-pakinabang para sa mabilisang prototyping at pagpapatakbo ng mga modelo sa edge devices o sa batch jobs tulad ng CI/CD pipelines.

**Paunawa**:  
Ang dokumentong ito ay isinalin gamit ang mga serbisyo ng AI-based na pagsasalin. Bagama't sinisikap naming maging wasto, pakitandaan na ang mga awtomatikong pagsasalin ay maaaring maglaman ng mga pagkakamali o kamalian. Ang orihinal na dokumento sa sariling wika nito ang dapat ituring na pangunahing batayan. Para sa mahahalagang impormasyon, inirerekomenda ang propesyonal na pagsasalin ng tao. Hindi kami mananagot sa anumang hindi pagkakaunawaan o maling interpretasyon na dulot ng paggamit ng pagsasaling ito.