# Ključne omenjene tehnologije vključujejo

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) – nizkonivojski API za strojno pospešeno strojno učenje, zgrajen na osnovi DirectX 12.
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) – platforma za vzporedno računalništvo in model programskega vmesnika (API), ki jo je razvilo podjetje Nvidia in omogoča splošno namensko procesiranje na grafičnih procesorjih (GPU).
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) – odprti format, zasnovan za predstavitev modelov strojnega učenja, ki omogoča interoperabilnost med različnimi okviri za strojno učenje.
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) – format za predstavitev in posodabljanje modelov strojnega učenja, posebej uporaben za manjše jezikovne modele, ki učinkovito delujejo na CPU-jih z 4-8-bitno kvantizacijo.

## DirectML

DirectML je nizkonivojski API, ki omogoča strojno pospešeno strojno učenje. Zgrajen je na osnovi DirectX 12, da izkoristi pospeševanje GPU-jev, in je neodvisen od ponudnika, kar pomeni, da ne zahteva sprememb kode za delovanje na različnih GPU-jih. Uporablja se predvsem za učenje in izvajanje modelov na GPU-jih.

Kar zadeva strojno podporo, je DirectML zasnovan za delovanje na širokem spektru GPU-jev, vključno z integriranimi in diskretnimi GPU-ji AMD, integriranimi GPU-ji Intel in diskretnimi GPU-ji NVIDIA. Je del platforme Windows AI in je podprt na sistemih Windows 10 in 11, kar omogoča učenje in izvajanje modelov na katerikoli napravi z operacijskim sistemom Windows.

Prišlo je do posodobitev in priložnosti, povezanih z DirectML, kot je podpora do 150 operaterjev ONNX in uporaba v okviru ONNX Runtime in WinML. Podpirajo ga večji proizvajalci strojne opreme (IHV), od katerih vsak implementira različne meta-ukaze.

## CUDA

CUDA, kar pomeni Compute Unified Device Architecture, je platforma za vzporedno računalništvo in model programskega vmesnika (API), ki ga je ustvarilo podjetje Nvidia. Razvijalcem programske opreme omogoča uporabo CUDA-podprtih grafičnih procesorjev (GPU) za splošno namensko procesiranje – pristop, imenovan GPGPU (splošno namensko računalništvo na grafičnih procesorjih). CUDA je ključno orodje za pospeševanje Nvidia GPU-jev in se široko uporablja na različnih področjih, vključno s strojno učenjem, znanstvenim računalništvom in obdelavo videa.

Strojna podpora za CUDA je specifična za Nvidia GPU-je, saj gre za lastniško tehnologijo podjetja Nvidia. Vsaka arhitektura podpira specifične različice orodij CUDA Toolkit, ki zagotavljajo potrebne knjižnice in orodja za razvijalce za gradnjo in izvajanje CUDA aplikacij.

## ONNX

ONNX (Open Neural Network Exchange) je odprti format, zasnovan za predstavitev modelov strojnega učenja. Ponuja definicijo razširljivega modela grafov izračunov ter definicije vgrajenih operaterjev in standardnih podatkovnih tipov. ONNX omogoča razvijalcem premikanje modelov med različnimi okviri za strojno učenje, kar omogoča interoperabilnost in olajša ustvarjanje ter uvajanje AI aplikacij.

Phi3 mini lahko deluje z ONNX Runtime na CPU-jih in GPU-jih na različnih napravah, vključno s strežniškimi platformami, namiznimi računalniki z Windows, Linux in Mac ter mobilnimi CPU-ji.
Optimizirane konfiguracije, ki smo jih dodali, so:

- ONNX modeli za int4 DML: Kvantizirani na int4 prek AWQ
- ONNX model za fp16 CUDA
- ONNX model za int4 CUDA: Kvantizirani na int4 prek RTN
- ONNX model za int4 CPU in mobilne naprave: Kvantizirani na int4 prek RTN

## Llama.cpp

Llama.cpp je odprtokodna programska knjižnica, napisana v C++. Izvaja sklepanje na različnih velikih jezikovnih modelih (LLM), vključno z Llama. Razvita je skupaj s knjižnico ggml (splošno namensko knjižnico za tenzorje), cilj llama.cpp pa je zagotoviti hitrejše sklepanje in manjšo porabo pomnilnika v primerjavi z izvirno implementacijo v Pythonu. Podpira strojno optimizacijo, kvantizacijo in ponuja preprost API ter primere. Če vas zanima učinkovito sklepanje z LLM, je llama.cpp vredna raziskovanja, saj Phi3 lahko uporablja Llama.cpp.

## GGUF

GGUF (Generic Graph Update Format) je format, uporabljen za predstavitev in posodabljanje modelov strojnega učenja. Posebej je uporaben za manjše jezikovne modele (SLM), ki učinkovito delujejo na CPU-jih z 4-8-bitno kvantizacijo. GGUF je koristen za hitro prototipiranje in izvajanje modelov na robnih napravah ali v serijskih opravilih, kot so CI/CD pipelines.

**Izjava o omejitvi odgovornosti**:  
Ta dokument je bil preveden s pomočjo strojnih prevajalskih storitev z umetno inteligenco. Čeprav si prizadevamo za natančnost, vas prosimo, da upoštevate, da lahko avtomatizirani prevodi vsebujejo napake ali netočnosti. Izvirni dokument v njegovem maternem jeziku je treba obravnavati kot avtoritativni vir. Za ključne informacije je priporočljivo uporabiti strokovni človeški prevod. Ne prevzemamo odgovornosti za morebitna nesporazuma ali napačne razlage, ki izhajajo iz uporabe tega prevoda.