# Aloita Phi-3:n käyttö paikallisesti

Tämä opas auttaa sinua asettamaan paikallisen ympäristön Phi-3-mallin ajamiseksi Ollaman avulla. Mallia voi käyttää eri tavoin, kuten GitHub Codespacesin, VS Code Dev Containersin tai oman paikallisen ympäristön kautta.

## Ympäristön asennus

### GitHub Codespaces

Voit ajaa tämän mallin virtuaalisesti käyttämällä GitHub Codespacesia. Painike avaa selainpohjaisen VS Code -instanssin:

1. Avaa malli (tämä voi kestää muutaman minuutin):

    [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/microsoft/phi-3cookbook)

2. Avaa terminaali-ikkuna.

### VS Code Dev Containers

⚠️ Tämä vaihtoehto toimii vain, jos Docker Desktopille on varattu vähintään 16 GB RAM-muistia. Jos sinulla on vähemmän kuin 16 GB RAM-muistia, voit kokeilla [GitHub Codespaces -vaihtoehtoa](../../../../../md/01.Introduction/01) tai [asentaa sen paikallisesti](../../../../../md/01.Introduction/01).

Toinen vaihtoehto on VS Code Dev Containers, joka avaa projektin paikallisessa VS Code -instanssissasi käyttämällä [Dev Containers -laajennusta](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers):

1. Käynnistä Docker Desktop (asenna se, jos sitä ei ole vielä asennettu).
2. Avaa projekti:

    [![Open in Dev Containers](https://img.shields.io/static/v1?style=for-the-badge&label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/microsoft/phi-3cookbook)

3. Kun VS Code -ikkuna avautuu ja projektitiedostot tulevat näkyviin (tämä voi kestää muutaman minuutin), avaa terminaali-ikkuna.
4. Jatka [käyttöönoton vaiheista](../../../../../md/01.Introduction/01).

### Paikallinen ympäristö

1. Varmista, että seuraavat työkalut on asennettu:

    * [Ollama](https://ollama.com/)
    * [Python 3.10+](https://www.python.org/downloads/)
    * [OpenAI Python SDK](https://pypi.org/project/openai/)

## Testaa mallia

1. Pyydä Ollamaa lataamaan ja suorittamaan phi3:mini-malli:

    ```shell
    ollama run phi3:mini
    ```

    Tämän lataaminen kestää muutaman minuutin.

2. Kun näet tulosteessa "success", voit lähettää viestin mallille suoraan komentokehotteesta.

    ```shell
    >>> Write a haiku about hungry hippos
    ```

3. Muutaman sekunnin kuluttua sinun pitäisi nähdä mallin tuottama vastaus.

4. Jos haluat oppia lisää kielimallien kanssa käytettävistä tekniikoista, avaa Python-notebook [ollama.ipynb](../../../../../code/01.Introduce/ollama.ipynb) ja suorita jokainen solu. Jos käytit jotain muuta mallia kuin 'phi3:mini', muuta tiedoston alussa olevaa `MODEL_NAME` in the first cell.

5. To have a conversation with the phi3:mini model from Python, open the Python file [chat.py](../../../../../code/01.Introduce/chat.py) and run it. You can change the `MODEL_NAME`-kohtaa tarpeen mukaan. Voit myös muokata järjestelmäviestiä tai lisätä esimerkkisyötteitä, jos haluat.

**Vastuuvapauslauseke**:  
Tämä asiakirja on käännetty konepohjaisilla tekoälyyn perustuvilla käännöspalveluilla. Pyrimme tarkkuuteen, mutta huomioithan, että automaattiset käännökset voivat sisältää virheitä tai epätarkkuuksia. Alkuperäistä asiakirjaa sen alkuperäisellä kielellä tulee pitää ensisijaisena lähteenä. Kriittisen tiedon osalta suositellaan ammattimaista ihmisen tekemää käännöstä. Emme ole vastuussa väärinkäsityksistä tai tulkintavirheistä, jotka johtuvat tämän käännöksen käytöstä.