# Keskeiset teknologiat sisältävät

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - matalan tason API laitteistokiihdytettyyn koneoppimiseen, rakennettu DirectX 12:n päälle.
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - rinnakkaislaskentaympäristö ja sovellusohjelmointirajapinta (API), jonka Nvidia on kehittänyt mahdollistaen yleiskäyttöisen laskennan grafiikkasuorittimilla (GPU).
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - avoin formaatti, joka on suunniteltu edustamaan koneoppimismalleja ja tarjoamaan yhteensopivuutta eri ML-kehysten välillä.
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - formaatti, jota käytetään koneoppimismallien edustamiseen ja päivittämiseen, erityisen hyödyllinen pienemmille kielimalleille, jotka toimivat tehokkaasti CPU:lla 4-8-bittisen kvantisoinnin avulla.

## DirectML

DirectML on matalan tason API, joka mahdollistaa laitteistokiihdytetyn koneoppimisen. Se on rakennettu DirectX 12:n päälle hyödyntämään GPU-kiihdytystä ja on riippumaton laitevalmistajasta, mikä tarkoittaa, ettei koodia tarvitse muuttaa eri GPU-valmistajien välillä. Sitä käytetään pääasiassa mallien koulutukseen ja inferenssiin GPU:illa.

Laitteistotuen osalta DirectML on suunniteltu toimimaan monenlaisten GPU:iden kanssa, mukaan lukien AMD:n integroidut ja erilliset GPU:t, Intelin integroidut GPU:t sekä Nvidian erilliset GPU:t. Se on osa Windows AI -alustaa ja tuettu Windows 10:ssä ja 11:ssä, mahdollistaen mallien koulutuksen ja inferenssin millä tahansa Windows-laitteella.

DirectML:ään liittyy päivityksiä ja mahdollisuuksia, kuten tuki jopa 150 ONNX-operaattorille, ja sitä käyttävät sekä ONNX runtime että WinML. Sen takana ovat suuret laitteistovalmistajat (IHV), jotka toteuttavat erilaisia metakomentoja.

## CUDA

CUDA, joka tarkoittaa Compute Unified Device Architecturea, on rinnakkaislaskentaympäristö ja sovellusohjelmointirajapinta (API), jonka Nvidia on luonut. Se mahdollistaa ohjelmistokehittäjille CUDA-yhteensopivan grafiikkasuorittimen (GPU) käytön yleiskäyttöiseen laskentaan – lähestymistapaa kutsutaan nimellä GPGPU (General-Purpose computing on Graphics Processing Units). CUDA on keskeinen tekijä Nvidian GPU-kiihdytyksessä ja sitä käytetään laajasti eri aloilla, kuten koneoppimisessa, tieteellisessä laskennassa ja videonkäsittelyssä.

Laitteistotuki CUDA:lle on rajattu Nvidian GPU:ille, sillä se on Nvidian kehittämä patentoitu teknologia. Jokainen arkkitehtuuri tukee tiettyjä CUDA-työkalupakin versioita, jotka tarjoavat tarvittavat kirjastot ja työkalut kehittäjille CUDA-sovellusten rakentamiseen ja suorittamiseen.

## ONNX

ONNX (Open Neural Network Exchange) on avoin formaatti, joka on suunniteltu edustamaan koneoppimismalleja. Se tarjoaa määritelmän laajennettavalle laskentakaaviomallille sekä sisäänrakennettujen operaattorien ja standardidatatyypien määritelmät. ONNX mahdollistaa kehittäjille mallien siirtämisen eri ML-kehysten välillä, mikä parantaa yhteensopivuutta ja helpottaa tekoälysovellusten luomista ja käyttöönottoa.

Phi3 mini voi käyttää ONNX Runtimea CPU:lla ja GPU:lla eri laitteilla, mukaan lukien palvelinalustat, Windows-, Linux- ja Mac-työpöydät sekä mobiililaitteiden CPU:t.
Optimoidut kokoonpanot, jotka olemme lisänneet, ovat:

- ONNX-mallit int4 DML:lle: Kvantisoitu int4:ksi AWQ:n kautta
- ONNX-malli fp16 CUDA:lle
- ONNX-malli int4 CUDA:lle: Kvantisoitu int4:ksi RTN:n kautta
- ONNX-malli int4 CPU:lle ja mobiililaitteille: Kvantisoitu int4:ksi RTN:n kautta

## Llama.cpp

Llama.cpp on avoimen lähdekoodin ohjelmistokirjasto, joka on kirjoitettu C++:lla. Se suorittaa inferenssin erilaisilla suurilla kielimalleilla (LLM), mukaan lukien Llama. Kehitetty yhdessä ggml-kirjaston (yleiskäyttöinen tensorikirjasto) kanssa, llama.cpp pyrkii tarjoamaan nopeampaa inferenssiä ja pienempää muistinkäyttöä verrattuna alkuperäiseen Python-toteutukseen. Se tukee laitteisto-optimointia, kvantisointia ja tarjoaa yksinkertaisen API:n sekä esimerkkejä. Jos olet kiinnostunut tehokkaasta LLM-inferenssistä, llama.cpp on tutkimisen arvoinen, sillä Phi3 voi käyttää Llama.cpp:tä.

## GGUF

GGUF (Generic Graph Update Format) on formaatti, jota käytetään koneoppimismallien edustamiseen ja päivittämiseen. Se on erityisen hyödyllinen pienemmille kielimalleille (SLM), jotka toimivat tehokkaasti CPU:illa 4-8-bittisen kvantisoinnin avulla. GGUF on hyödyllinen nopeassa prototyyppauksessa ja mallien suorittamisessa reunalaitteilla tai eräajoissa, kuten CI/CD-putkistoissa.

**Vastuuvapauslauseke**:  
Tämä asiakirja on käännetty konepohjaisia tekoälyyn perustuvia käännöspalveluita käyttäen. Pyrimme tarkkuuteen, mutta huomioithan, että automaattiset käännökset voivat sisältää virheitä tai epätarkkuuksia. Alkuperäistä asiakirjaa sen alkuperäisellä kielellä tulee pitää ensisijaisena lähteenä. Kriittisen tiedon osalta suositellaan ammattimaista, ihmisen tekemää käännöstä. Emme ole vastuussa tämän käännöksen käytöstä aiheutuvista väärinkäsityksistä tai virheellisistä tulkinnoista.