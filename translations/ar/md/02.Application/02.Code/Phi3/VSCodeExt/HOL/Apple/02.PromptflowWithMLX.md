# **المعمل 2 - تشغيل Prompt flow باستخدام Phi-3-mini في AIPC**

## **ما هو Prompt flow**

Prompt flow هو مجموعة من الأدوات المصممة لتسهيل دورة التطوير الشاملة لتطبيقات الذكاء الاصطناعي القائمة على نماذج اللغة الكبيرة (LLM)، بدءًا من توليد الأفكار، والنماذج الأولية، والاختبار، والتقييم، وصولًا إلى النشر في الإنتاج والمراقبة. يسهل هندسة المطالبات (Prompt Engineering) ويتيح لك بناء تطبيقات LLM بجودة إنتاجية.

مع Prompt flow، يمكنك:

- إنشاء تدفقات تربط بين LLMs، والمطالبات، وأكواد Python، وأدوات أخرى في سير عمل قابل للتنفيذ.

- تصحيح الأخطاء وتكرار تدفقاتك بسهولة، خاصة في التفاعل مع LLMs.

- تقييم تدفقاتك، وحساب معايير الجودة والأداء باستخدام مجموعات بيانات أكبر.

- دمج الاختبار والتقييم في نظام CI/CD الخاص بك لضمان جودة التدفق.

- نشر تدفقاتك على منصة الخدمة التي تختارها أو دمجها بسهولة في قاعدة كود التطبيق الخاص بك.

- (اختياري ولكنه موصى به بشدة) التعاون مع فريقك من خلال الاستفادة من النسخة السحابية لـ Prompt flow في Azure AI.



## **إنشاء تدفقات أكواد التوليد على Apple Silicon**

***ملاحظة***: إذا لم تكمل تثبيت البيئة، يرجى زيارة [المعمل 0 - التثبيتات](./01.Installations.md)

1. افتح امتداد Prompt flow في Visual Studio Code وقم بإنشاء مشروع تدفق فارغ.

![create](../../../../../../../../../translated_images/pf_create.d6172d8277a78a7fa82cd6ff727ed44e037fa78b662f1f62d5963f36d712d229.ar.png)

2. أضف معلمات الإدخال والإخراج وأضف كود Python كتدفق جديد.

![flow](../../../../../../../../../translated_images/pf_flow.d5646a323fb7f444c0b98b4521057a592325c583e7ba18bc31500bc0415e9ef3.ar.png)

يمكنك الرجوع إلى هذا الهيكل (flow.dag.yaml) لإنشاء التدفق الخاص بك.

```yaml

inputs:
  prompt:
    type: string
    default: Write python code for Fibonacci serie. Please use markdown as output
outputs:
  result:
    type: string
    reference: ${gen_code_by_phi3.output}
nodes:
- name: gen_code_by_phi3
  type: python
  source:
    type: code
    path: gen_code_by_phi3.py
  inputs:
    prompt: ${inputs.prompt}


```

3. تقليل حجم phi-3-mini

نهدف إلى تشغيل SLM بشكل أفضل على الأجهزة المحلية. بشكل عام، نقوم بتقليل حجم النموذج (INT4, FP16, FP32).

```bash

python -m mlx_lm.convert --hf-path microsoft/Phi-3-mini-4k-instruct

```

**ملاحظة:** المجلد الافتراضي هو mlx_model 

4. أضف الكود في ***Chat_With_Phi3.py***

```python


from promptflow import tool

from mlx_lm import load, generate


# The inputs section will change based on the arguments of the tool function, after you save the code
# Adding type to arguments and return value will help the system show the types properly
# Please update the function name/signature per need
@tool
def my_python_tool(prompt: str) -> str:

    model_id = './mlx_model_phi3_mini'

    model, tokenizer = load(model_id)

    # <|user|>\nWrite python code for Fibonacci serie. Please use markdown as output<|end|>\n<|assistant|>

    response = generate(model, tokenizer, prompt="<|user|>\n" + prompt  + "<|end|>\n<|assistant|>", max_tokens=2048, verbose=True)

    return response


```

4. يمكنك اختبار التدفق من خلال Debug أو Run للتحقق مما إذا كان كود التوليد يعمل بشكل صحيح.

![RUN](../../../../../../../../../translated_images/pf_run.d918637dc00f61e9bdeec37d4cc9646f77d270ac9203bcce13569f3157202b6e.ar.png)

5. قم بتشغيل التدفق كـ API تطوير في الطرفية.

```

pf flow serve --source ./ --port 8080 --host localhost   

```

يمكنك اختباره باستخدام Postman / Thunder Client.


### **ملاحظات**

1. التشغيل الأول يستغرق وقتًا طويلًا. يُوصى بتنزيل نموذج phi-3 من خلال Hugging Face CLI.

2. نظرًا لقدرات الحوسبة المحدودة لوحدة Intel NPU، يُوصى باستخدام Phi-3-mini-4k-instruct.

3. نستخدم تسريع Intel NPU لتحويل INT4، ولكن إذا أعدت تشغيل الخدمة، ستحتاج إلى حذف مجلدي cache وnc_workshop.



## **المصادر**

1. تعرف على Promptflow [https://microsoft.github.io/promptflow/](https://microsoft.github.io/promptflow/)

2. تعرف على تسريع Intel NPU [https://github.com/intel/intel-npu-acceleration-library](https://github.com/intel/intel-npu-acceleration-library)

3. عينة كود، تنزيل [كود عينة لوكيل NPU المحلي](../../../../../../../../../code/07.Lab/01/AIPC/local-npu-agent)

**إخلاء المسؤولية**:  
تمت ترجمة هذا المستند باستخدام خدمات الترجمة الآلية بالذكاء الاصطناعي. بينما نسعى لتحقيق الدقة، يُرجى العلم أن الترجمات الآلية قد تحتوي على أخطاء أو معلومات غير دقيقة. يجب اعتبار المستند الأصلي بلغته الأصلية هو المصدر الموثوق. بالنسبة للمعلومات الحساسة، يُوصى بالاستعانة بترجمة بشرية احترافية. نحن غير مسؤولين عن أي سوء فهم أو تفسيرات خاطئة ناتجة عن استخدام هذه الترجمة.