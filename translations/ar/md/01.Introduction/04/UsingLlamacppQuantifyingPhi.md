# **كمية عائلة Phi باستخدام llama.cpp**

## **ما هو llama.cpp**

llama.cpp هي مكتبة برمجية مفتوحة المصدر مكتوبة أساسًا بلغة C++ تُستخدم لتنفيذ استنتاج على نماذج اللغة الكبيرة (LLMs) مثل Llama. الهدف الرئيسي لها هو تقديم أداء متقدم للاستنتاج على LLMs عبر مجموعة واسعة من الأجهزة مع إعداد بسيط للغاية. بالإضافة إلى ذلك، هناك روابط برمجية بلغة Python لهذه المكتبة، توفر واجهة برمجية عالية المستوى لإكمال النصوص وخادم ويب متوافق مع OpenAI.

الهدف الرئيسي من llama.cpp هو تمكين استنتاج LLM بأقل إعداد ممكن وأداء متقدم على مجموعة متنوعة من الأجهزة - سواء محليًا أو في السحابة.

- تنفيذ بسيط بلغة C/C++ بدون أي تبعيات
- دعم كامل لتقنية Apple Silicon - محسنة باستخدام ARM NEON، Accelerate، وإطارات Metal
- دعم AVX، AVX2، وAVX512 للمعماريات x86
- كميات صحيحة بـ 1.5 بت، 2 بت، 3 بت، 4 بت، 5 بت، 6 بت، و8 بت لاستنتاج أسرع وتقليل استخدام الذاكرة
- نوى CUDA مخصصة لتشغيل LLMs على وحدات معالجة الرسوميات من NVIDIA (دعم وحدات AMD عبر HIP)
- دعم خلفيات Vulkan وSYCL
- استنتاج هجين CPU+GPU لتسريع النماذج الأكبر من سعة VRAM الإجمالية

## **كمية Phi-3.5 باستخدام llama.cpp**

يمكنكم تحويل نموذج Phi-3.5-Instruct إلى كميات باستخدام llama.cpp، لكن نماذج Phi-3.5-Vision وPhi-3.5-MoE غير مدعومة بعد. التنسيق الذي يتم تحويله باستخدام llama.cpp هو gguf، وهو أيضًا التنسيق الأكثر استخدامًا للكميات.

يوجد عدد كبير من النماذج بتنسيق GGUF الكمي على منصة Hugging Face. AI Foundry وOllama وLlamaEdge يعتمدون على llama.cpp، لذا يتم استخدام نماذج GGUF بشكل متكرر.

### **ما هو GGUF**

GGUF هو تنسيق ثنائي مصمم للتحميل والحفظ السريع للنماذج، مما يجعله فعالًا للغاية لأغراض الاستنتاج. تم تصميم GGUF للاستخدام مع GGML والمنفذين الآخرين. تم تطوير GGUF بواسطة @ggerganov، الذي يعد أيضًا مطور llama.cpp، وهو إطار عمل شائع لاستنتاج LLM مكتوب بلغة C/C++. يمكن تحويل النماذج التي تم تطويرها في البداية باستخدام أطر عمل مثل PyTorch إلى تنسيق GGUF لاستخدامها مع تلك المحركات.

### **ONNX مقابل GGUF**

ONNX هو تنسيق تقليدي للتعلم الآلي/التعلم العميق، ويحظى بدعم جيد في أطر الذكاء الاصطناعي المختلفة وله سيناريوهات استخدام جيدة في الأجهزة الطرفية. أما بالنسبة لـ GGUF، فهو يعتمد على llama.cpp ويمكن القول إنه منتج من عصر الذكاء الاصطناعي التوليدي (GenAI). لكلا التنسيقين استخدامات مشابهة. إذا كنت تبحث عن أداء أفضل في الأجهزة المدمجة وطبقات التطبيقات، فقد يكون ONNX هو خيارك. وإذا كنت تستخدم إطار العمل والتكنولوجيا المشتقة من llama.cpp، فقد يكون GGUF أفضل.

### **كمية Phi-3.5-Instruct باستخدام llama.cpp**

**1. إعداد البيئة**


```bash

git clone https://github.com/ggerganov/llama.cpp.git

cd llama.cpp

make -j8

```


**2. الكمية**

استخدام llama.cpp لتحويل Phi-3.5-Instruct إلى FP16 GGUF


```bash

./convert_hf_to_gguf.py <Your Phi-3.5-Instruct Location> --outfile phi-3.5-128k-mini_fp16.gguf

```

تحويل Phi-3.5 إلى INT4


```bash

./llama.cpp/llama-quantize <Your phi-3.5-128k-mini_fp16.gguf location> ./gguf/phi-3.5-128k-mini_Q4_K_M.gguf Q4_K_M

```


**3. الاختبار**

تثبيت llama-cpp-python


```bash

pip install llama-cpp-python -U

```

***ملاحظة*** 

إذا كنت تستخدم Apple Silicon، قم بتثبيت llama-cpp-python بهذه الطريقة


```bash

CMAKE_ARGS="-DLLAMA_METAL=on" pip install llama-cpp-python -U

```

الاختبار 


```bash

llama.cpp/llama-cli --model <Your phi-3.5-128k-mini_Q4_K_M.gguf location> --prompt "<|user|>\nCan you introduce .NET<|end|>\n<|assistant|>\n"  --gpu-layers 10

```



## **الموارد**

1. لمعرفة المزيد عن llama.cpp [https://onnxruntime.ai/docs/genai/](https://onnxruntime.ai/docs/genai/)

2. لمعرفة المزيد عن GGUF [https://huggingface.co/docs/hub/en/gguf](https://huggingface.co/docs/hub/en/gguf)

**إخلاء المسؤولية**:  
تمت ترجمة هذا المستند باستخدام خدمات الترجمة الآلية المعتمدة على الذكاء الاصطناعي. بينما نسعى لتحقيق الدقة، يرجى العلم أن الترجمات الآلية قد تحتوي على أخطاء أو معلومات غير دقيقة. يجب اعتبار المستند الأصلي بلغته الأصلية المصدر الموثوق والأساسي. للحصول على معلومات حساسة أو حاسمة، يُوصى بالاستعانة بترجمة بشرية احترافية. نحن غير مسؤولين عن أي سوء فهم أو تفسير خاطئ ينشأ عن استخدام هذه الترجمة.