# **قياس عائلة Phi**

يشير تقليل دقة النماذج إلى عملية تحويل المعاملات (مثل الأوزان وقيم التفعيل) في نموذج الشبكة العصبية من نطاق قيم كبير (عادةً ما يكون نطاق قيم مستمر) إلى نطاق قيم صغير ومحدود. تتيح هذه التقنية تقليل حجم النموذج وتعقيد العمليات الحسابية، مما يحسن كفاءة تشغيل النموذج في البيئات ذات الموارد المحدودة مثل الأجهزة المحمولة أو الأنظمة المدمجة. يحقق تقليل دقة النماذج ضغطًا من خلال تقليل دقة المعاملات، لكنه أيضًا يؤدي إلى فقدان معين في الدقة. لذلك، أثناء عملية تقليل الدقة، يجب تحقيق توازن بين حجم النموذج وتعقيد العمليات الحسابية والدقة. تشمل طرق تقليل الدقة الشائعة تقليل الدقة إلى القيم الصحيحة الثابتة أو القيم العائمة. يمكنك اختيار استراتيجية تقليل الدقة المناسبة بناءً على السيناريوهات والاحتياجات المحددة.

نهدف إلى نشر نموذج GenAI على الأجهزة الطرفية وتمكين المزيد من الأجهزة من الدخول إلى سيناريوهات GenAI، مثل الأجهزة المحمولة، أجهزة AI PC / Copilot+PC، والأجهزة التقليدية لإنترنت الأشياء. من خلال النماذج ذات الدقة المخفضة، يمكننا نشرها على أجهزة طرفية مختلفة بناءً على الأجهزة المتوفرة. وبالاقتران مع إطار تسريع النماذج والنماذج ذات الدقة المخفضة التي تقدمها الشركات المصنعة للأجهزة، يمكننا بناء سيناريوهات أفضل لتطبيقات SLM.

في سيناريو تقليل الدقة، لدينا دقة مختلفة (INT4، INT8، FP16، FP32). فيما يلي شرح للدقة المستخدمة بشكل شائع:

### **INT4**

تقليل الدقة إلى INT4 هو طريقة جذرية تقوم بتحويل أوزان وقيم تفعيل النموذج إلى أعداد صحيحة مكونة من 4 بتات. عادةً ما يؤدي تقليل الدقة إلى INT4 إلى فقدان أكبر في الدقة بسبب النطاق التمثيلي الأصغر والدقة الأقل. ومع ذلك، بالمقارنة مع تقليل الدقة إلى INT8، يمكن أن يقلل INT4 بشكل أكبر من متطلبات التخزين وتعقيد العمليات الحسابية للنموذج. يجب ملاحظة أن تقليل الدقة إلى INT4 نادر نسبيًا في التطبيقات العملية، لأن الدقة المنخفضة جدًا قد تؤدي إلى تدهور كبير في أداء النموذج. بالإضافة إلى ذلك، ليس كل الأجهزة تدعم عمليات INT4، لذلك يجب أخذ التوافق مع الأجهزة في الاعتبار عند اختيار طريقة تقليل الدقة.

### **INT8**

تقليل الدقة إلى INT8 هو عملية تحويل أوزان وتفعيلات النموذج من أرقام عائمة إلى أعداد صحيحة مكونة من 8 بتات. على الرغم من أن النطاق التمثيلي الذي توفره أعداد INT8 أقل وأقل دقة، إلا أنه يمكن أن يقلل بشكل كبير من متطلبات التخزين والحساب. في تقليل الدقة إلى INT8، تمر أوزان وقيم التفعيل الخاصة بالنموذج بعملية تقليل دقة، بما في ذلك التدرج والإزاحة، للحفاظ على أكبر قدر ممكن من المعلومات الأصلية للأرقام العائمة. أثناء الاستدلال، يتم إعادة تحويل هذه القيم المخفضة الدقة إلى أرقام عائمة للحساب، ثم يتم تحويلها مرة أخرى إلى INT8 للخطوة التالية. يمكن لهذه الطريقة أن توفر دقة كافية في معظم التطبيقات مع الحفاظ على كفاءة حسابية عالية.

### **FP16**

تنسيق FP16، وهو أرقام عائمة مكونة من 16 بت (float16)، يقلل من استهلاك الذاكرة بمقدار النصف مقارنةً بالأرقام العائمة المكونة من 32 بت (float32)، مما يوفر مزايا كبيرة في تطبيقات التعلم العميق واسعة النطاق. يسمح تنسيق FP16 بتحميل نماذج أكبر أو معالجة المزيد من البيانات ضمن حدود ذاكرة GPU نفسها. مع استمرار تطور أجهزة GPU الحديثة لدعم عمليات FP16، قد يؤدي استخدام تنسيق FP16 أيضًا إلى تحسينات في سرعة الحساب. ومع ذلك، يحتوي تنسيق FP16 على عيوبه الكامنة، وهي دقة أقل، مما قد يؤدي إلى عدم استقرار رقمي أو فقدان الدقة في بعض الحالات.

### **FP32**

يوفر تنسيق FP32 دقة أعلى ويمكنه تمثيل نطاق واسع من القيم بدقة. في السيناريوهات التي يتم فيها إجراء عمليات رياضية معقدة أو تتطلب نتائج عالية الدقة، يفضل استخدام تنسيق FP32. ومع ذلك، فإن الدقة العالية تعني أيضًا استهلاكًا أكبر للذاكرة ووقتًا أطول للحساب. بالنسبة للنماذج الكبيرة للتعلم العميق، خاصةً عندما تكون هناك العديد من معاملات النموذج وكمية هائلة من البيانات، قد يؤدي تنسيق FP32 إلى نقص في ذاكرة GPU أو انخفاض في سرعة الاستدلال.

على الأجهزة المحمولة أو أجهزة إنترنت الأشياء، يمكننا تحويل نماذج Phi-3.x إلى INT4، بينما يمكن لأجهزة AI PC / Copilot PC استخدام دقة أعلى مثل INT8، FP16، FP32.

حاليًا، لدى الشركات المصنعة للأجهزة المختلفة أطر عمل متنوعة لدعم النماذج التوليدية، مثل OpenVINO من Intel، وQNN من Qualcomm، وMLX من Apple، وCUDA من Nvidia، وغيرها، والتي يمكن دمجها مع تقليل الدقة لإكمال النشر المحلي.

من الناحية التقنية، لدينا دعم لتنسيقات مختلفة بعد تقليل الدقة، مثل تنسيقات PyTorch / Tensorflow، وGGUF، وONNX. لقد قمت بمقارنة التنسيقات وسيناريوهات التطبيق بين GGUF وONNX. هنا أوصي بتنسيق ONNX لتقليل الدقة، حيث يتمتع بدعم جيد من إطار العمل إلى الأجهزة. في هذا الفصل، سنركز على ONNX Runtime لـ GenAI، وOpenVINO، وApple MLX لتنفيذ تقليل دقة النموذج (إذا كان لديك طريقة أفضل، يمكنك تقديمها لنا من خلال إرسال PR).

**يتضمن هذا الفصل**

1. [تقليل دقة Phi-3.5 / 4 باستخدام llama.cpp](./UsingLlamacppQuantifyingPhi.md)

2. [تقليل دقة Phi-3.5 / 4 باستخدام إضافات Generative AI لـ onnxruntime](./UsingORTGenAIQuantifyingPhi.md)

3. [تقليل دقة Phi-3.5 / 4 باستخدام Intel OpenVINO](./UsingIntelOpenVINOQuantifyingPhi.md)

4. [تقليل دقة Phi-3.5 / 4 باستخدام إطار عمل Apple MLX](./UsingAppleMLXQuantifyingPhi.md)

**إخلاء مسؤولية**:  
تمت ترجمة هذا المستند باستخدام خدمات الترجمة الآلية المدعومة بالذكاء الاصطناعي. بينما نسعى لتحقيق الدقة، يرجى العلم أن الترجمات الآلية قد تحتوي على أخطاء أو معلومات غير دقيقة. يجب اعتبار المستند الأصلي بلغته الأصلية المصدر الموثوق. للحصول على معلومات حاسمة، يُوصى بالاستعانة بترجمة بشرية احترافية. نحن غير مسؤولين عن أي سوء فهم أو تفسير خاطئ ناتج عن استخدام هذه الترجمة.