# Principais tecnologias mencionadas

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - uma API de baixo nível para aprendizado de máquina acelerado por hardware, construída sobre o DirectX 12.
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - uma plataforma de computação paralela e modelo de interface de programação de aplicativos (API) desenvolvida pela Nvidia, que permite o processamento de propósito geral em unidades de processamento gráfico (GPUs).
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - um formato aberto projetado para representar modelos de aprendizado de máquina, oferecendo interoperabilidade entre diferentes frameworks de ML.
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - um formato usado para representar e atualizar modelos de aprendizado de máquina, especialmente útil para modelos de linguagem menores que podem ser executados de forma eficiente em CPUs com quantização de 4-8 bits.

## DirectML

O DirectML é uma API de baixo nível que possibilita o aprendizado de máquina acelerado por hardware. Ele é construído sobre o DirectX 12 para aproveitar a aceleração da GPU e é independente do fornecedor, ou seja, não requer alterações no código para funcionar com diferentes fabricantes de GPU. É utilizado principalmente para cargas de trabalho de treinamento e inferência de modelos em GPUs.

Quanto ao suporte de hardware, o DirectML foi projetado para funcionar com uma ampla gama de GPUs, incluindo GPUs integradas e discretas da AMD, GPUs integradas da Intel e GPUs discretas da NVIDIA. Ele faz parte da Plataforma de IA do Windows e é compatível com o Windows 10 e 11, permitindo o treinamento e a inferência de modelos em qualquer dispositivo Windows.

Houve atualizações e oportunidades relacionadas ao DirectML, como o suporte a até 150 operadores ONNX e seu uso pelo runtime ONNX e pelo WinML. Ele conta com o suporte de grandes fornecedores de hardware integrado (IHVs), cada um implementando vários metacomandos.

## CUDA

CUDA, que significa Compute Unified Device Architecture, é uma plataforma de computação paralela e modelo de interface de programação de aplicativos (API) criada pela Nvidia. Ela permite que desenvolvedores de software utilizem uma unidade de processamento gráfico (GPU) compatível com CUDA para processamento de propósito geral – uma abordagem chamada GPGPU (General-Purpose computing on Graphics Processing Units). CUDA é um componente essencial da aceleração de GPU da Nvidia e é amplamente utilizada em diversas áreas, incluindo aprendizado de máquina, computação científica e processamento de vídeo.

O suporte de hardware para CUDA é específico para GPUs da Nvidia, já que se trata de uma tecnologia proprietária desenvolvida pela empresa. Cada arquitetura suporta versões específicas do toolkit CUDA, que fornece as bibliotecas e ferramentas necessárias para que os desenvolvedores criem e executem aplicativos CUDA.

## ONNX

ONNX (Open Neural Network Exchange) é um formato aberto projetado para representar modelos de aprendizado de máquina. Ele fornece uma definição de um modelo de gráfico de computação extensível, bem como definições de operadores embutidos e tipos de dados padrão. O ONNX permite que desenvolvedores movam modelos entre diferentes frameworks de ML, promovendo a interoperabilidade e facilitando a criação e o desenvolvimento de aplicações de IA.

O Phi3 mini pode ser executado com o ONNX Runtime em CPU e GPU em diversos dispositivos, incluindo plataformas de servidores, desktops com Windows, Linux e Mac, e CPUs móveis. As configurações otimizadas que adicionamos são:

- Modelos ONNX para int4 DML: Quantizados para int4 via AWQ
- Modelo ONNX para fp16 CUDA
- Modelo ONNX para int4 CUDA: Quantizados para int4 via RTN
- Modelo ONNX para int4 CPU e Mobile: Quantizados para int4 via RTN

## Llama.cpp

Llama.cpp é uma biblioteca de software de código aberto escrita em C++. Ela realiza inferência em vários Modelos de Linguagem de Grande Escala (LLMs), incluindo o Llama. Desenvolvida juntamente com a biblioteca ggml (uma biblioteca de tensores de uso geral), a Llama.cpp tem como objetivo oferecer inferência mais rápida e menor uso de memória em comparação com a implementação original em Python. Ela suporta otimização de hardware, quantização e oferece uma API simples e exemplos. Se você está interessado em inferência eficiente de LLMs, a Llama.cpp vale a pena ser explorada, já que o Phi3 pode executar a Llama.cpp.

## GGUF

GGUF (Generic Graph Update Format) é um formato usado para representar e atualizar modelos de aprendizado de máquina. Ele é particularmente útil para modelos de linguagem menores (SLMs) que podem ser executados de forma eficiente em CPUs com quantização de 4-8 bits. O GGUF é vantajoso para prototipagem rápida e execução de modelos em dispositivos de borda ou em tarefas em lote, como pipelines de CI/CD.

**Aviso Legal**:  
Este documento foi traduzido utilizando serviços de tradução automática baseados em IA. Embora nos esforcemos para alcançar precisão, esteja ciente de que traduções automatizadas podem conter erros ou imprecisões. O documento original em seu idioma nativo deve ser considerado a fonte oficial. Para informações críticas, recomenda-se a tradução humana profissional. Não nos responsabilizamos por quaisquer mal-entendidos ou interpretações equivocadas decorrentes do uso desta tradução.