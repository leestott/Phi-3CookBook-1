# Comece a usar o Phi-3 localmente

Este guia ajudará você a configurar seu ambiente local para executar o modelo Phi-3 usando o Ollama. Você pode executar o modelo de algumas maneiras diferentes, incluindo o uso do GitHub Codespaces, VS Code Dev Containers ou seu ambiente local.

## Configuração do ambiente

### GitHub Codespaces

Você pode executar este template virtualmente usando o GitHub Codespaces. O botão abrirá uma instância do VS Code baseada na web em seu navegador:

1. Abra o template (isso pode levar alguns minutos):

    [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/microsoft/phi-3cookbook)

2. Abra uma janela de terminal.

### VS Code Dev Containers

⚠️ Esta opção só funcionará se o seu Docker Desktop tiver pelo menos 16 GB de RAM alocados. Se você tiver menos de 16 GB de RAM, pode tentar a [opção GitHub Codespaces](../../../../../md/01.Introduction/01) ou [configurá-lo localmente](../../../../../md/01.Introduction/01).

Uma opção relacionada é o VS Code Dev Containers, que abrirá o projeto no seu VS Code local usando a [extensão Dev Containers](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers):

1. Inicie o Docker Desktop (instale-o se ainda não estiver instalado).
2. Abra o projeto:

    [![Open in Dev Containers](https://img.shields.io/static/v1?style=for-the-badge&label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/microsoft/phi-3cookbook)

3. Na janela do VS Code que abrir, assim que os arquivos do projeto aparecerem (isso pode levar alguns minutos), abra uma janela de terminal.
4. Continue com os [passos de implantação](../../../../../md/01.Introduction/01).

### Ambiente Local

1. Certifique-se de que as seguintes ferramentas estejam instaladas:

    * [Ollama](https://ollama.com/)
    * [Python 3.10+](https://www.python.org/downloads/)
    * [OpenAI Python SDK](https://pypi.org/project/openai/)

## Teste o modelo

1. Peça ao Ollama para baixar e executar o modelo phi3:mini:

    ```shell
    ollama run phi3:mini
    ```

    Isso levará alguns minutos para baixar o modelo.

2. Assim que você vir "success" no output, poderá enviar uma mensagem para esse modelo a partir do prompt.

    ```shell
    >>> Write a haiku about hungry hippos
    ```

3. Após alguns segundos, você deverá ver uma resposta sendo exibida pelo modelo.

4. Para aprender sobre diferentes técnicas usadas com modelos de linguagem, abra o notebook Python [ollama.ipynb](../../../../../code/01.Introduce/ollama.ipynb) e execute cada célula. Se você usou um modelo diferente de 'phi3:mini', altere o `MODEL_NAME` in the first cell.

5. To have a conversation with the phi3:mini model from Python, open the Python file [chat.py](../../../../../code/01.Introduce/chat.py) and run it. You can change the `MODEL_NAME` no topo do arquivo conforme necessário, e você também pode modificar a mensagem do sistema ou adicionar exemplos few-shot, se desejar.

**Aviso Legal**:  
Este documento foi traduzido utilizando serviços de tradução automática baseados em IA. Embora nos esforcemos para garantir a precisão, esteja ciente de que traduções automáticas podem conter erros ou imprecisões. O documento original em seu idioma nativo deve ser considerado a fonte oficial. Para informações críticas, recomenda-se a tradução profissional realizada por humanos. Não nos responsabilizamos por quaisquer mal-entendidos ou interpretações equivocadas decorrentes do uso desta tradução.