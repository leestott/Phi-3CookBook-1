# Teknolojia Muhimu Zilizotajwa

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - API ya kiwango cha chini kwa ajili ya ujifunzaji wa mashine unaoharakishwa na vifaa, iliyojengwa juu ya DirectX 12.
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - jukwaa la kompyuta sambamba na modeli ya API iliyotengenezwa na Nvidia, inayowezesha usindikaji wa matumizi ya jumla kwenye vitengo vya usindikaji wa picha (GPUs).
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - muundo wa wazi ulioundwa kuwakilisha mifano ya ujifunzaji wa mashine ambao hutoa muingiliano kati ya mifumo tofauti ya ML.
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - muundo unaotumika kuwakilisha na kusasisha mifano ya ujifunzaji wa mashine, hasa muhimu kwa mifano midogo ya lugha inayoweza kuendeshwa kwa ufanisi kwenye CPU kwa kutumia uquantization wa 4-8bit.

## DirectML

DirectML ni API ya kiwango cha chini inayowezesha ujifunzaji wa mashine unaoharakishwa na vifaa. Imejengwa juu ya DirectX 12 ili kutumia kasi ya GPU na haina upendeleo wa muuzaji, maana yake haina haja ya mabadiliko ya msimbo ili kufanya kazi kwenye wauzaji tofauti wa GPU. Inatumika hasa kwa mafunzo ya modeli na kazi za utabiri kwenye GPU.

Kuhusu msaada wa vifaa, DirectML imeundwa kufanya kazi na GPU mbalimbali, ikiwa ni pamoja na GPU zilizounganishwa na zilizojitegemea za AMD, GPU zilizounganishwa za Intel, na GPU zilizojitegemea za NVIDIA. Ni sehemu ya Jukwaa la AI la Windows na inasaidiwa kwenye Windows 10 & 11, ikiruhusu mafunzo ya modeli na utabiri kwenye kifaa chochote cha Windows.

Kumekuwa na masasisho na fursa zinazohusiana na DirectML, kama vile kusaidia hadi waendeshaji 150 wa ONNX na kutumiwa na ONNX runtime na WinML. Inasaidiwa na Wauzaji Wakuu wa Vifaa Vilivyounganishwa (IHVs), kila mmoja akitekeleza metakomandi mbalimbali.

## CUDA

CUDA, ambayo inasimama kwa Compute Unified Device Architecture, ni jukwaa la kompyuta sambamba na modeli ya API iliyoundwa na Nvidia. Inaruhusu watengenezaji wa programu kutumia GPU inayounga mkono CUDA kwa usindikaji wa matumizi ya jumla â€“ mbinu inayojulikana kama GPGPU (General-Purpose computing on Graphics Processing Units). CUDA ni kichocheo kikuu cha kasi ya GPU ya Nvidia na inatumiwa sana katika nyanja mbalimbali, ikiwa ni pamoja na ujifunzaji wa mashine, kompyuta za kisayansi, na usindikaji wa video.

Msaada wa vifaa kwa CUDA ni maalum kwa GPU za Nvidia, kwa kuwa ni teknolojia ya umiliki iliyotengenezwa na Nvidia. Kila usanifu unasaidia matoleo maalum ya kifaa cha zana cha CUDA, ambacho hutoa maktaba na zana zinazohitajika kwa watengenezaji kujenga na kuendesha programu za CUDA.

## ONNX

ONNX (Open Neural Network Exchange) ni muundo wa wazi ulioundwa kuwakilisha mifano ya ujifunzaji wa mashine. Unatoa ufafanuzi wa modeli ya grafu ya hesabu inayoweza kupanuliwa, pamoja na ufafanuzi wa waendeshaji wa kujengwa ndani na aina za data za kawaida. ONNX inaruhusu watengenezaji kuhamisha mifano kati ya mifumo tofauti ya ML, ikirahisisha muingiliano na kuwezesha uundaji na utekelezaji wa programu za AI.

Phi3 mini inaweza kuendeshwa na ONNX Runtime kwenye CPU na GPU kwenye vifaa mbalimbali, ikiwa ni pamoja na majukwaa ya seva, Windows, Linux na Mac desktops, na CPU za simu.
Marekebisho yaliyoboreshwa ambayo tumeongeza ni:

- Mifano ya ONNX kwa int4 DML: Imehesabiwa upya kuwa int4 kupitia AWQ  
- Modeli ya ONNX kwa fp16 CUDA  
- Modeli ya ONNX kwa int4 CUDA: Imehesabiwa upya kuwa int4 kupitia RTN  
- Modeli ya ONNX kwa int4 CPU na Simu: Imehesabiwa upya kuwa int4 kupitia RTN  

## Llama.cpp

Llama.cpp ni maktaba ya programu huria iliyoandikwa kwa C++. Inafanya utabiri kwenye Mifano Mikubwa ya Lugha (LLMs) mbalimbali, ikiwa ni pamoja na Llama. Imeendelezwa pamoja na maktaba ya ggml (maktaba ya tensor ya matumizi ya jumla), llama.cpp inalenga kutoa utabiri wa haraka na matumizi ya chini ya kumbukumbu ikilinganishwa na utekelezaji wa awali wa Python. Inasaidia uboreshaji wa vifaa, uquantization, na inatoa API rahisi pamoja na mifano. Ikiwa una nia ya utabiri wa LLM wa ufanisi, llama.cpp inafaa kuchunguzwa kwani Phi3 inaweza kuendesha Llama.cpp.

## GGUF

GGUF (Generic Graph Update Format) ni muundo unaotumika kuwakilisha na kusasisha mifano ya ujifunzaji wa mashine. Ni muhimu hasa kwa mifano midogo ya lugha (SLMs) inayoweza kuendeshwa kwa ufanisi kwenye CPU kwa kutumia uquantization wa 4-8bit. GGUF ni faida kwa prototyping ya haraka na kuendesha mifano kwenye vifaa vya pembezoni au katika kazi za kundi kama vile mabomba ya CI/CD.

**Kanusho**:  
Hati hii imetafsiriwa kwa kutumia huduma za kutafsiri zinazotegemea AI ya mashine. Ingawa tunajitahidi kwa usahihi, tafadhali fahamu kuwa tafsiri za kiotomatiki zinaweza kuwa na makosa au kutokuwa sahihi. Hati ya asili katika lugha yake ya awali inapaswa kuzingatiwa kama chanzo rasmi. Kwa maelezo muhimu, inashauriwa kutumia huduma za utafsiri wa binadamu wa kitaalam. Hatutawajibika kwa maelewano mabaya au tafsiri zisizo sahihi zinazotokana na matumizi ya tafsiri hii.