# इंटरएक्टिव Phi 3 Mini 4K इंस्ट्रक्ट चैटबॉट विथ व्हिस्पर

## अवलोकन

इंटरएक्टिव Phi 3 Mini 4K इंस्ट्रक्ट चैटबॉट एक ऐसा टूल है जो उपयोगकर्ताओं को Microsoft Phi 3 Mini 4K इंस्ट्रक्ट डेमो के साथ टेक्स्ट या ऑडियो इनपुट का उपयोग करके बातचीत करने की अनुमति देता है। इस चैटबॉट का उपयोग विभिन्न कार्यों के लिए किया जा सकता है, जैसे अनुवाद, मौसम अपडेट, और सामान्य जानकारी प्राप्त करना।

### शुरुआत कैसे करें

इस चैटबॉट का उपयोग करने के लिए निम्नलिखित निर्देशों का पालन करें:

1. एक नया [E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb](https://github.com/microsoft/Phi-3CookBook/blob/main/code/06.E2E/E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb) खोलें।
2. नोटबुक की मुख्य विंडो में, आपको एक चैटबॉक्स इंटरफ़ेस दिखाई देगा जिसमें एक टेक्स्ट इनपुट बॉक्स और "Send" बटन होगा।
3. टेक्स्ट-आधारित चैटबॉट का उपयोग करने के लिए, बस अपना संदेश टेक्स्ट इनपुट बॉक्स में टाइप करें और "Send" बटन पर क्लिक करें। चैटबॉट एक ऑडियो फाइल के साथ प्रतिक्रिया देगा जिसे नोटबुक के भीतर ही चलाया जा सकता है।

**नोट**: इस टूल को GPU और Microsoft Phi-3 और OpenAI Whisper मॉडल्स की आवश्यकता होती है, जो स्पीच रिकग्निशन और अनुवाद के लिए उपयोग किए जाते हैं।

### GPU आवश्यकताएँ

इस डेमो को चलाने के लिए आपको 12GB GPU मेमोरी की आवश्यकता होगी।

**Microsoft-Phi-3-Mini-4K इंस्ट्रक्ट** डेमो को GPU पर चलाने के लिए मेमोरी की आवश्यकताएँ कई कारकों पर निर्भर करती हैं, जैसे इनपुट डेटा (ऑडियो या टेक्स्ट) का आकार, अनुवाद के लिए उपयोग की जाने वाली भाषा, मॉडल की गति, और GPU पर उपलब्ध मेमोरी।

सामान्यतः, Whisper मॉडल को GPU पर चलाने के लिए डिज़ाइन किया गया है। Whisper मॉडल को चलाने के लिए अनुशंसित न्यूनतम GPU मेमोरी 8 GB है, लेकिन आवश्यकता पड़ने पर यह बड़ी मेमोरी को भी संभाल सकता है।

यह ध्यान रखना महत्वपूर्ण है कि यदि मॉडल पर बड़ी मात्रा में डेटा या अनुरोधों की उच्च मात्रा चलाई जाती है, तो अधिक GPU मेमोरी की आवश्यकता हो सकती है और/या प्रदर्शन में समस्याएँ आ सकती हैं। यह अनुशंसा की जाती है कि आप अपने उपयोग के मामले का विभिन्न कॉन्फ़िगरेशन के साथ परीक्षण करें और मेमोरी उपयोग की निगरानी करें ताकि आपके विशिष्ट आवश्यकताओं के लिए इष्टतम सेटिंग्स निर्धारित की जा सकें।

## इंटरएक्टिव Phi 3 Mini 4K इंस्ट्रक्ट चैटबॉट विथ व्हिस्पर के लिए E2E सैंपल

[Interactive Phi 3 Mini 4K Instruct Chatbot with Whisper](https://github.com/microsoft/Phi-3CookBook/blob/main/code/06.E2E/E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb) नामक जुपिटर नोटबुक यह दर्शाता है कि Microsoft Phi 3 Mini 4K इंस्ट्रक्ट डेमो का उपयोग ऑडियो या लिखित टेक्स्ट इनपुट से टेक्स्ट उत्पन्न करने के लिए कैसे किया जाए। नोटबुक में कई फंक्शन परिभाषित किए गए हैं:

1. `tts_file_name(text)`: यह फंक्शन इनपुट टेक्स्ट के आधार पर एक फाइल का नाम जनरेट करता है ताकि उत्पन्न ऑडियो फाइल को सेव किया जा सके।
2. `edge_free_tts(chunks_list,speed,voice_name,save_path)`: यह फंक्शन Edge TTS API का उपयोग करके इनपुट टेक्स्ट के हिस्सों की सूची से एक ऑडियो फाइल बनाता है। इनपुट पैरामीटर हैं: टेक्स्ट के हिस्सों की सूची, स्पीच रेट, वॉइस का नाम, और आउटपुट पथ जहाँ ऑडियो फाइल को सेव किया जाएगा।
3. `talk(input_text)`: यह फंक्शन Edge TTS API का उपयोग करके एक ऑडियो फाइल जनरेट करता है और इसे /content/audio डायरेक्टरी में एक रैंडम फाइल नाम के साथ सेव करता है। इनपुट पैरामीटर वह टेक्स्ट है जिसे स्पीच में कन्वर्ट किया जाना है।
4. `run_text_prompt(message, chat_history)`: यह फंक्शन Microsoft Phi 3 Mini 4K इंस्ट्रक्ट डेमो का उपयोग करके मैसेज इनपुट से एक ऑडियो फाइल जनरेट करता है और इसे चैट हिस्ट्री में जोड़ता है।
5. `run_audio_prompt(audio, chat_history)`: यह फंक्शन Whisper मॉडल API का उपयोग करके एक ऑडियो फाइल को टेक्स्ट में कन्वर्ट करता है और इसे `run_text_prompt()` फंक्शन को पास करता है।
6. कोड एक Gradio ऐप लॉन्च करता है जो उपयोगकर्ताओं को Phi 3 Mini 4K इंस्ट्रक्ट डेमो के साथ या तो मैसेज टाइप करके या ऑडियो फाइल अपलोड करके इंटरैक्ट करने की अनुमति देता है। आउटपुट ऐप के भीतर एक टेक्स्ट मैसेज के रूप में प्रदर्शित होता है।

## समस्या निवारण

Cuda GPU ड्राइवर इंस्टॉल करना

1. सुनिश्चित करें कि आपके Linux एप्लिकेशन अपडेटेड हैं।

    ```bash
    sudo apt update
    ```

2. Cuda ड्राइवर इंस्टॉल करें।

    ```bash
    sudo apt install nvidia-cuda-toolkit
    ```

3. Cuda ड्राइवर लोकेशन को रजिस्टर करें।

    ```bash
    echo /usr/lib64-nvidia/ >/etc/ld.so.conf.d/libcuda.conf; ldconfig
    ```

4. Nvidia GPU मेमोरी साइज की जाँच करें (12GB GPU मेमोरी आवश्यक है)।

    ```bash
    nvidia-smi
    ```

5. कैश खाली करें: यदि आप PyTorch का उपयोग कर रहे हैं, तो आप torch.cuda.empty_cache() कॉल करके सभी अप्रयुक्त कैश्ड मेमोरी को रिलीज़ कर सकते हैं ताकि इसे अन्य GPU एप्लिकेशन द्वारा उपयोग किया जा सके।

    ```python
    torch.cuda.empty_cache() 
    ```

6. Nvidia Cuda की जाँच करें।

    ```bash
    nvcc --version
    ```

7. Hugging Face टोकन बनाने के लिए निम्नलिखित कार्य करें:

    - [Hugging Face Token Settings page](https://huggingface.co/settings/tokens?WT.mc_id=aiml-137032-kinfeylo) पर जाएँ।
    - **New token** चुनें।
    - वह प्रोजेक्ट **Name** दर्ज करें जिसे आप उपयोग करना चाहते हैं।
    - **Type** को **Write** पर सेट करें।

> **नोट**
>
> यदि आपको निम्नलिखित त्रुटि का सामना करना पड़ता है:
>
> ```bash
> /sbin/ldconfig.real: Can't create temporary cache file /etc/ld.so.cache~: Permission denied 
> ```
>
> इसे हल करने के लिए, अपने टर्मिनल में निम्नलिखित कमांड टाइप करें:
>
> ```bash
> sudo ldconfig
> ```

**अस्वीकरण**:  
यह दस्तावेज़ मशीन-आधारित एआई अनुवाद सेवाओं का उपयोग करके अनुवादित किया गया है। जबकि हम सटीकता सुनिश्चित करने का प्रयास करते हैं, कृपया ध्यान दें कि स्वचालित अनुवाद में त्रुटियाँ या अशुद्धियाँ हो सकती हैं। मूल दस्तावेज़ को उसकी मूल भाषा में प्रामाणिक स्रोत माना जाना चाहिए। महत्वपूर्ण जानकारी के लिए, पेशेवर मानव अनुवाद की सिफारिश की जाती है। इस अनुवाद के उपयोग से उत्पन्न किसी भी गलतफहमी या गलत व्याख्या के लिए हम जिम्मेदार नहीं हैं।