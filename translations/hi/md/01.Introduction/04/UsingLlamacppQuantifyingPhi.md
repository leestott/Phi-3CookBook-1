# **लामा.cpp का उपयोग करके Phi परिवार को क्वांटाइज़ करना**

## **लामा.cpp क्या है**

लामा.cpp एक ओपन-सोर्स सॉफ़्टवेयर लाइब्रेरी है, जो मुख्य रूप से C++ में लिखी गई है और विभिन्न बड़े भाषा मॉडल्स (LLMs), जैसे Llama, पर इंफ़रेंस करती है। इसका मुख्य उद्देश्य न्यूनतम सेटअप के साथ विभिन्न हार्डवेयर पर LLM इंफ़रेंस के लिए अत्याधुनिक प्रदर्शन प्रदान करना है। इसके अतिरिक्त, इस लाइब्रेरी के लिए पायथन बाइंडिंग्स भी उपलब्ध हैं, जो टेक्स्ट पूर्णता के लिए एक उच्च-स्तरीय API और OpenAI संगत वेब सर्वर प्रदान करती हैं।

लामा.cpp का मुख्य उद्देश्य न्यूनतम सेटअप और विभिन्न प्रकार के हार्डवेयर पर स्थानीय और क्लाउड में अत्याधुनिक प्रदर्शन के साथ LLM इंफ़रेंस को सक्षम बनाना है।

- बिना किसी निर्भरता के साधारण C/C++ कार्यान्वयन
- Apple सिलिकॉन को प्राथमिकता दी गई है - ARM NEON, Accelerate और Metal फ्रेमवर्क्स के माध्यम से अनुकूलित
- x86 आर्किटेक्चर के लिए AVX, AVX2 और AVX512 समर्थन
- तेज़ इंफ़रेंस और कम मेमोरी उपयोग के लिए 1.5-बिट, 2-बिट, 3-बिट, 4-बिट, 5-बिट, 6-बिट और 8-बिट पूर्णांक क्वांटाइज़ेशन
- NVIDIA GPUs पर LLMs चलाने के लिए कस्टम CUDA कर्नल (AMD GPUs के लिए HIP समर्थन)
- Vulkan और SYCL बैकएंड समर्थन
- CPU+GPU हाइब्रिड इंफ़रेंस, जो VRAM क्षमता से बड़े मॉडलों को आंशिक रूप से तेज़ करता है

## **Phi-3.5 को लामा.cpp के साथ क्वांटाइज़ करना**

Phi-3.5-Instruct मॉडल को लामा.cpp का उपयोग करके क्वांटाइज़ किया जा सकता है, लेकिन Phi-3.5-Vision और Phi-3.5-MoE अभी तक समर्थित नहीं हैं। लामा.cpp द्वारा परिवर्तित प्रारूप GGUF है, जो सबसे व्यापक रूप से उपयोग किया जाने वाला क्वांटाइज़ेशन प्रारूप भी है।

Hugging Face पर GGUF प्रारूप में बड़ी संख्या में क्वांटाइज़्ड मॉडल उपलब्ध हैं। AI Foundry, Ollama, और LlamaEdge लामा.cpp पर निर्भर हैं, इसलिए GGUF मॉडल का भी अक्सर उपयोग किया जाता है।

### **GGUF क्या है**

GGUF एक बाइनरी प्रारूप है जो मॉडलों को जल्दी लोड और सेव करने के लिए अनुकूलित है, जिससे यह इंफ़रेंस उद्देश्यों के लिए अत्यधिक कुशल बनता है। GGUF को GGML और अन्य एग्जीक्यूटर्स के साथ उपयोग के लिए डिज़ाइन किया गया है। GGUF को @ggerganov द्वारा विकसित किया गया था, जो लामा.cpp के डेवलपर भी हैं, जो एक लोकप्रिय C/C++ LLM इंफ़रेंस फ्रेमवर्क है। PyTorch जैसे फ्रेमवर्क्स में प्रारंभिक रूप से विकसित मॉडलों को GGUF प्रारूप में परिवर्तित किया जा सकता है ताकि वे उन इंजनों के साथ उपयोग किए जा सकें।

### **ONNX बनाम GGUF**

ONNX एक पारंपरिक मशीन लर्निंग/डीप लर्निंग प्रारूप है, जो विभिन्न AI फ्रेमवर्क्स में अच्छी तरह से समर्थित है और एज डिवाइसों में अच्छे उपयोग परिदृश्य प्रदान करता है। GGUF, लामा.cpp पर आधारित है और इसे GenAI युग में विकसित किया गया है। दोनों का उपयोग समान है। यदि आप एम्बेडेड हार्डवेयर और एप्लिकेशन लेयर में बेहतर प्रदर्शन चाहते हैं, तो ONNX आपका विकल्प हो सकता है। यदि आप लामा.cpp के डेरिवेटिव फ्रेमवर्क और तकनीक का उपयोग करते हैं, तो GGUF बेहतर हो सकता है।

### **लामा.cpp का उपयोग करके Phi-3.5-Instruct को क्वांटाइज़ करना**

**1. एनवायरनमेंट कॉन्फ़िगरेशन**


```bash

git clone https://github.com/ggerganov/llama.cpp.git

cd llama.cpp

make -j8

```


**2. क्वांटाइज़ेशन**

लामा.cpp का उपयोग करके Phi-3.5-Instruct को FP16 GGUF में बदलना


```bash

./convert_hf_to_gguf.py <Your Phi-3.5-Instruct Location> --outfile phi-3.5-128k-mini_fp16.gguf

```

Phi-3.5 को INT4 में क्वांटाइज़ करना


```bash

./llama.cpp/llama-quantize <Your phi-3.5-128k-mini_fp16.gguf location> ./gguf/phi-3.5-128k-mini_Q4_K_M.gguf Q4_K_M

```


**3. परीक्षण**

लामा-cpp-python इंस्टॉल करें


```bash

pip install llama-cpp-python -U

```

***ध्यान दें*** 

यदि आप Apple Silicon का उपयोग कर रहे हैं, तो कृपया लामा-cpp-python को इस तरह इंस्टॉल करें


```bash

CMAKE_ARGS="-DLLAMA_METAL=on" pip install llama-cpp-python -U

```

परीक्षण 


```bash

llama.cpp/llama-cli --model <Your phi-3.5-128k-mini_Q4_K_M.gguf location> --prompt "<|user|>\nCan you introduce .NET<|end|>\n<|assistant|>\n"  --gpu-layers 10

```



## **संसाधन**

1. लामा.cpp के बारे में और जानें [https://onnxruntime.ai/docs/genai/](https://onnxruntime.ai/docs/genai/)

2. GGUF के बारे में और जानें [https://huggingface.co/docs/hub/en/gguf](https://huggingface.co/docs/hub/en/gguf)

**अस्वीकरण**:  
यह दस्तावेज़ मशीन-आधारित एआई अनुवाद सेवाओं का उपयोग करके अनुवादित किया गया है। जबकि हम सटीकता के लिए प्रयास करते हैं, कृपया ध्यान दें कि स्वचालित अनुवाद में त्रुटियां या अशुद्धियां हो सकती हैं। मूल भाषा में मूल दस्तावेज़ को आधिकारिक स्रोत माना जाना चाहिए। महत्वपूर्ण जानकारी के लिए, पेशेवर मानव अनुवाद की सिफारिश की जाती है। इस अनुवाद के उपयोग से उत्पन्न किसी भी गलतफहमी या गलत व्याख्या के लिए हम उत्तरदायी नहीं हैं।