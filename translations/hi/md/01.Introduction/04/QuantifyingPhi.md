# **Phi परिवार को मापना**

मॉडल क्वांटाइज़ेशन का मतलब है न्यूरल नेटवर्क मॉडल के पैरामीटर्स (जैसे वेट्स और एक्टिवेशन वैल्यूज) को एक बड़े वैल्यू रेंज (आमतौर पर एक सतत वैल्यू रेंज) से एक छोटे सीमित वैल्यू रेंज में मैप करना। यह तकनीक मॉडल के आकार और गणना की जटिलता को कम कर सकती है और मोबाइल डिवाइस या एम्बेडेड सिस्टम जैसे संसाधन-सीमित वातावरण में मॉडल की संचालन क्षमता में सुधार कर सकती है। मॉडल क्वांटाइज़ेशन पैरामीटर्स की प्रिसिशन को घटाकर कंप्रेशन हासिल करता है, लेकिन यह कुछ हद तक प्रिसिशन का नुकसान भी लाता है। इसलिए, क्वांटाइज़ेशन प्रक्रिया में मॉडल के आकार, गणना की जटिलता, और प्रिसिशन के बीच संतुलन बनाना आवश्यक है। सामान्य क्वांटाइज़ेशन विधियों में फिक्स्ड-पॉइंट क्वांटाइज़ेशन, फ्लोटिंग-पॉइंट क्वांटाइज़ेशन आदि शामिल हैं। आप विशिष्ट परिदृश्य और आवश्यकताओं के अनुसार उपयुक्त क्वांटाइज़ेशन रणनीति चुन सकते हैं।

हम GenAI मॉडल को एज डिवाइस पर डिप्लॉय करना चाहते हैं और अधिक डिवाइस को GenAI परिदृश्यों में लाना चाहते हैं, जैसे मोबाइल डिवाइस, AI PC / Copilot+PC, और पारंपरिक IoT डिवाइस। क्वांटाइज़ेशन मॉडल के माध्यम से, हम इसे विभिन्न एज डिवाइसों पर विभिन्न डिवाइसों के आधार पर डिप्लॉय कर सकते हैं। हार्डवेयर निर्माताओं द्वारा प्रदान किए गए मॉडल एक्सेलेरेशन फ्रेमवर्क और क्वांटाइज़ेशन मॉडल को मिलाकर, हम बेहतर SLM एप्लिकेशन परिदृश्य बना सकते हैं।

क्वांटाइज़ेशन परिदृश्य में, हमारे पास विभिन्न प्रिसिशन (INT4, INT8, FP16, FP32) होते हैं। नीचे सामान्यतः उपयोग की जाने वाली क्वांटाइज़ेशन प्रिसिशन का विवरण दिया गया है:

### **INT4**

INT4 क्वांटाइज़ेशन एक उन्नत क्वांटाइज़ेशन विधि है जो मॉडल के वेट्स और एक्टिवेशन वैल्यूज को 4-बिट इंटीजर में बदल देती है। INT4 क्वांटाइज़ेशन आमतौर पर प्रिसिशन में अधिक नुकसान करता है क्योंकि इसकी प्रतिनिधित्व सीमा छोटी होती है और प्रिसिशन कम होती है। हालांकि, INT8 क्वांटाइज़ेशन की तुलना में, INT4 क्वांटाइज़ेशन मॉडल की स्टोरेज आवश्यकताओं और गणना की जटिलता को और कम कर सकता है। ध्यान दें कि व्यावहारिक अनुप्रयोगों में INT4 क्वांटाइज़ेशन अपेक्षाकृत दुर्लभ है, क्योंकि अत्यधिक कम प्रिसिशन मॉडल के प्रदर्शन में महत्वपूर्ण गिरावट का कारण बन सकती है। इसके अलावा, सभी हार्डवेयर INT4 ऑपरेशंस का समर्थन नहीं करते, इसलिए क्वांटाइज़ेशन विधि चुनते समय हार्डवेयर संगतता पर विचार करना आवश्यक है।

### **INT8**

INT8 क्वांटाइज़ेशन मॉडल के वेट्स और एक्टिवेशन को फ्लोटिंग पॉइंट नंबर से 8-बिट इंटीजर में बदलने की प्रक्रिया है। हालांकि INT8 इंटीजर द्वारा प्रस्तुत संख्यात्मक सीमा छोटी और कम प्रिसाइज होती है, यह स्टोरेज और गणना आवश्यकताओं को काफी हद तक कम कर सकती है। INT8 क्वांटाइज़ेशन में, मॉडल के वेट्स और एक्टिवेशन वैल्यूज एक क्वांटाइज़ेशन प्रक्रिया से गुजरते हैं, जिसमें स्केलिंग और ऑफसेट शामिल होता है, ताकि मूल फ्लोटिंग पॉइंट जानकारी को यथासंभव संरक्षित किया जा सके। इनफेरेंस के दौरान, इन क्वांटाइज़ वैल्यूज को गणना के लिए फ्लोटिंग पॉइंट नंबर में वापस डी-क्वांटाइज़ किया जाएगा और फिर अगले चरण के लिए दोबारा INT8 में क्वांटाइज़ किया जाएगा। यह विधि अधिकांश अनुप्रयोगों में पर्याप्त प्रिसिशन प्रदान कर सकती है जबकि उच्च गणना दक्षता बनाए रखती है।

### **FP16**

FP16 फॉर्मेट, यानी 16-बिट फ्लोटिंग पॉइंट नंबर (float16), 32-बिट फ्लोटिंग पॉइंट नंबर (float32) की तुलना में मेमोरी फुटप्रिंट को आधा कर देता है, जो बड़े पैमाने पर डीप लर्निंग अनुप्रयोगों में महत्वपूर्ण लाभ प्रदान करता है। FP16 फॉर्मेट समान GPU मेमोरी सीमाओं के भीतर बड़े मॉडल को लोड करने या अधिक डेटा प्रोसेस करने की अनुमति देता है। जैसे-जैसे आधुनिक GPU हार्डवेयर FP16 ऑपरेशंस का समर्थन करते हैं, FP16 फॉर्मेट का उपयोग करने से कंप्यूटिंग गति में सुधार भी हो सकता है। हालांकि, FP16 फॉर्मेट की अपनी अंतर्निहित सीमाएँ भी हैं, जैसे कम प्रिसिशन, जो कुछ मामलों में संख्यात्मक अस्थिरता या प्रिसिशन के नुकसान का कारण बन सकती हैं।

### **FP32**

FP32 फॉर्मेट उच्च प्रिसिशन प्रदान करता है और एक विस्तृत वैल्यू रेंज को सटीक रूप से दर्शा सकता है। ऐसे परिदृश्यों में जहाँ जटिल गणितीय ऑपरेशंस किए जाते हैं या उच्च-प्रिसिशन परिणामों की आवश्यकता होती है, FP32 फॉर्मेट को प्राथमिकता दी जाती है। हालांकि, उच्च प्रिसिशन का मतलब अधिक मेमोरी उपयोग और लंबा गणना समय भी है। बड़े पैमाने के डीप लर्निंग मॉडल्स के लिए, विशेष रूप से जब मॉडल पैरामीटर्स की संख्या अधिक हो और डेटा की मात्रा बहुत बड़ी हो, FP32 फॉर्मेट GPU मेमोरी की कमी या इनफेरेंस गति में गिरावट का कारण बन सकता है।

मोबाइल डिवाइस या IoT डिवाइस पर, हम Phi-3.x मॉडल को INT4 में बदल सकते हैं, जबकि AI PC / Copilot PC जैसे डिवाइस उच्च प्रिसिशन का उपयोग कर सकते हैं जैसे INT8, FP16, FP32।

वर्तमान में, विभिन्न हार्डवेयर निर्माताओं के पास जनरेटिव मॉडल का समर्थन करने के लिए विभिन्न फ्रेमवर्क हैं, जैसे Intel का OpenVINO, Qualcomm का QNN, Apple का MLX, और Nvidia का CUDA। इन फ्रेमवर्क्स को मॉडल क्वांटाइज़ेशन के साथ मिलाकर स्थानीय डिप्लॉयमेंट पूरा किया जा सकता है।

तकनीकी दृष्टिकोण से, क्वांटाइज़ेशन के बाद हमारे पास विभिन्न फॉर्मेट्स का समर्थन है, जैसे PyTorch / Tensorflow फॉर्मेट, GGUF, और ONNX। मैंने GGUF और ONNX के बीच फॉर्मेट तुलना और एप्लिकेशन परिदृश्य का अध्ययन किया है। यहाँ मैं ONNX क्वांटाइज़ेशन फॉर्मेट की सिफारिश करता हूँ, जिसे मॉडल फ्रेमवर्क से हार्डवेयर तक अच्छा समर्थन प्राप्त है। इस अध्याय में, हम GenAI के लिए ONNX Runtime, OpenVINO, और Apple MLX का उपयोग करके मॉडल क्वांटाइज़ेशन पर ध्यान केंद्रित करेंगे (यदि आपके पास कोई बेहतर तरीका है, तो आप हमें PR सबमिट करके दे सकते हैं)।

**इस अध्याय में शामिल है**

1. [Phi-3.5 / 4 को llama.cpp का उपयोग करके क्वांटाइज़ करना](./UsingLlamacppQuantifyingPhi.md)

2. [Phi-3.5 / 4 को onnxruntime के लिए Generative AI एक्सटेंशन्स का उपयोग करके क्वांटाइज़ करना](./UsingORTGenAIQuantifyingPhi.md)

3. [Phi-3.5 / 4 को Intel OpenVINO का उपयोग करके क्वांटाइज़ करना](./UsingIntelOpenVINOQuantifyingPhi.md)

4. [Phi-3.5 / 4 को Apple MLX Framework का उपयोग करके क्वांटाइज़ करना](./UsingAppleMLXQuantifyingPhi.md)

**अस्वीकरण**:  
यह दस्तावेज़ मशीन-आधारित एआई अनुवाद सेवाओं का उपयोग करके अनुवादित किया गया है। जबकि हम सटीकता सुनिश्चित करने का प्रयास करते हैं, कृपया ध्यान दें कि स्वचालित अनुवादों में त्रुटियाँ या अशुद्धियाँ हो सकती हैं। मूल दस्तावेज़, जो इसकी मूल भाषा में है, को आधिकारिक स्रोत माना जाना चाहिए। महत्वपूर्ण जानकारी के लिए, पेशेवर मानव अनुवाद की सिफारिश की जाती है। इस अनुवाद के उपयोग से उत्पन्न किसी भी गलतफहमी या गलत व्याख्या के लिए हम उत्तरदायी नहीं हैं।