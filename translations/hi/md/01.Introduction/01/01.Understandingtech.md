# प्रमुख प्रौद्योगिकियों में शामिल हैं

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - DirectX 12 पर आधारित हार्डवेयर-त्वरित मशीन लर्निंग के लिए एक लो-लेवल API।
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - Nvidia द्वारा विकसित एक समानांतर कंप्यूटिंग प्लेटफॉर्म और एप्लिकेशन प्रोग्रामिंग इंटरफ़ेस (API) मॉडल, जो ग्राफिक्स प्रोसेसिंग यूनिट्स (GPUs) पर सामान्य-उद्देश्यीय प्रोसेसिंग को सक्षम बनाता है।
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - एक ओपन फॉर्मेट जो मशीन लर्निंग मॉडल्स को रिप्रेजेंट करने के लिए डिज़ाइन किया गया है और विभिन्न ML फ्रेमवर्क्स के बीच इंटरऑपरेबिलिटी प्रदान करता है।
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - मशीन लर्निंग मॉडल्स को रिप्रेजेंट और अपडेट करने के लिए उपयोग किया जाने वाला एक फॉर्मेट, विशेष रूप से छोटे भाषा मॉडल्स के लिए उपयोगी है जो 4-8 बिट क्वांटाइजेशन के साथ CPUs पर प्रभावी ढंग से चल सकते हैं।

## DirectML

DirectML एक लो-लेवल API है जो हार्डवेयर-त्वरित मशीन लर्निंग को सक्षम बनाता है। यह DirectX 12 पर आधारित है ताकि GPU एक्सेलेरेशन का उपयोग किया जा सके और यह वेंडर-अज्ञेय (vendor-agnostic) है, जिसका मतलब है कि यह विभिन्न GPU वेंडर्स पर बिना कोड परिवर्तन के काम कर सकता है। इसका मुख्य उपयोग मॉडल ट्रेनिंग और GPU पर इन्फेरेंस वर्कलोड्स के लिए होता है।

हार्डवेयर सपोर्ट की बात करें तो, DirectML को AMD इंटीग्रेटेड और डिस्क्रीट GPUs, Intel इंटीग्रेटेड GPUs, और NVIDIA डिस्क्रीट GPUs सहित कई प्रकार के GPUs के साथ काम करने के लिए डिज़ाइन किया गया है। यह Windows AI प्लेटफ़ॉर्म का हिस्सा है और Windows 10 और 11 पर समर्थित है, जिससे किसी भी Windows डिवाइस पर मॉडल ट्रेनिंग और इन्फेरेंस संभव हो जाता है।

DirectML से संबंधित अपडेट और अवसर भी उपलब्ध हैं, जैसे कि 150 ONNX ऑपरेटर्स तक का समर्थन और इसे ONNX रनटाइम और WinML दोनों द्वारा उपयोग किया जाता है। इसे प्रमुख Integrated Hardware Vendors (IHVs) द्वारा समर्थित किया गया है, जो विभिन्न मेटाकमांड्स को लागू करते हैं।

## CUDA

CUDA, जिसका पूरा नाम Compute Unified Device Architecture है, Nvidia द्वारा बनाया गया एक समानांतर कंप्यूटिंग प्लेटफ़ॉर्म और एप्लिकेशन प्रोग्रामिंग इंटरफ़ेस (API) मॉडल है। यह सॉफ़्टवेयर डेवलपर्स को CUDA-सक्षम ग्राफिक्स प्रोसेसिंग यूनिट (GPU) का उपयोग सामान्य उद्देश्य प्रोसेसिंग (GPGPU) के लिए करने की अनुमति देता है। CUDA Nvidia के GPU एक्सेलेरेशन का एक प्रमुख घटक है और इसे मशीन लर्निंग, वैज्ञानिक कंप्यूटिंग, और वीडियो प्रोसेसिंग जैसे विभिन्न क्षेत्रों में व्यापक रूप से उपयोग किया जाता है।

CUDA का हार्डवेयर सपोर्ट Nvidia के GPUs तक ही सीमित है, क्योंकि यह Nvidia द्वारा विकसित एक स्वामित्व वाली तकनीक है। प्रत्येक आर्किटेक्चर CUDA टूलकिट के विशिष्ट संस्करणों का समर्थन करता है, जो डेवलपर्स को CUDA एप्लिकेशन बनाने और चलाने के लिए आवश्यक लाइब्रेरी और उपकरण प्रदान करता है।

## ONNX

ONNX (Open Neural Network Exchange) एक ओपन फॉर्मेट है जिसे मशीन लर्निंग मॉडल्स को रिप्रेजेंट करने के लिए डिज़ाइन किया गया है। यह एक एक्स्टेंसिबल कम्प्यूटेशन ग्राफ मॉडल की परिभाषा के साथ-साथ बिल्ट-इन ऑपरेटर्स और मानक डेटा प्रकारों की परिभाषा प्रदान करता है। ONNX डेवलपर्स को विभिन्न ML फ्रेमवर्क्स के बीच मॉडल्स को स्थानांतरित करने की अनुमति देता है, जिससे इंटरऑपरेबिलिटी सक्षम होती है और AI एप्लिकेशन बनाना और तैनात करना आसान हो जाता है।

Phi3 mini CPU और GPU पर ONNX Runtime के साथ विभिन्न डिवाइसेस पर चल सकता है, जिसमें सर्वर प्लेटफ़ॉर्म, Windows, Linux और Mac डेस्कटॉप, और मोबाइल CPUs शामिल हैं। हमने जो अनुकूलित कॉन्फ़िगरेशन जोड़े हैं वे हैं:

- int4 DML के लिए ONNX मॉडल: AWQ के माध्यम से int4 में क्वांटाइज़ किया गया
- fp16 CUDA के लिए ONNX मॉडल
- int4 CUDA के लिए ONNX मॉडल: RTN के माध्यम से int4 में क्वांटाइज़ किया गया
- int4 CPU और मोबाइल के लिए ONNX मॉडल: RTN के माध्यम से int4 में क्वांटाइज़ किया गया

## Llama.cpp

Llama.cpp एक ओपन-सोर्स सॉफ़्टवेयर लाइब्रेरी है जो C++ में लिखी गई है। यह Llama सहित विभिन्न बड़े भाषा मॉडल्स (LLMs) पर इन्फेरेंस करता है। ggml लाइब्रेरी (एक सामान्य उद्देश्य टेंसर लाइब्रेरी) के साथ विकसित किया गया, llama.cpp मूल Python इम्प्लीमेंटेशन की तुलना में तेज़ इन्फेरेंस और कम मेमोरी उपयोग प्रदान करने का प्रयास करता है। यह हार्डवेयर ऑप्टिमाइज़ेशन, क्वांटाइजेशन को सपोर्ट करता है और एक सरल API और उदाहरण प्रदान करता है। यदि आप कुशल LLM इन्फेरेंस में रुचि रखते हैं, तो llama.cpp को देखना फायदेमंद हो सकता है क्योंकि Phi3 Llama.cpp चला सकता है।

## GGUF

GGUF (Generic Graph Update Format) मशीन लर्निंग मॉडल्स को रिप्रेजेंट और अपडेट करने के लिए उपयोग किया जाने वाला एक फॉर्मेट है। यह विशेष रूप से छोटे भाषा मॉडल्स (SLMs) के लिए उपयोगी है जो 4-8 बिट क्वांटाइजेशन के साथ CPUs पर प्रभावी ढंग से चल सकते हैं। GGUF त्वरित प्रोटोटाइपिंग और एज डिवाइसेस पर या CI/CD पाइपलाइनों जैसे बैच जॉब्स में मॉडल्स चलाने के लिए लाभकारी है।

**अस्वीकरण**:  
यह दस्तावेज़ मशीन-आधारित एआई अनुवाद सेवाओं का उपयोग करके अनुवादित किया गया है। जबकि हम सटीकता सुनिश्चित करने का प्रयास करते हैं, कृपया ध्यान दें कि स्वचालित अनुवादों में त्रुटियां या अशुद्धियां हो सकती हैं। मूल दस्तावेज़ को उसकी मूल भाषा में प्रामाणिक स्रोत माना जाना चाहिए। महत्वपूर्ण जानकारी के लिए, पेशेवर मानव अनुवाद की सिफारिश की जाती है। इस अनुवाद के उपयोग से उत्पन्न किसी भी गलतफहमी या गलत व्याख्या के लिए हम उत्तरदायी नहीं हैं।