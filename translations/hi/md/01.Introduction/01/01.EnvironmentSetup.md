# Phi-3 को लोकल रूप से शुरू करें

यह गाइड आपको अपने लोकल वातावरण में Ollama का उपयोग करके Phi-3 मॉडल चलाने के लिए सेटअप करने में मदद करेगा। आप इस मॉडल को कई तरीकों से चला सकते हैं, जैसे GitHub Codespaces, VS Code Dev Containers, या अपने लोकल वातावरण में।

## वातावरण सेटअप

### GitHub Codespaces

आप GitHub Codespaces का उपयोग करके इस टेम्पलेट को वर्चुअली चला सकते हैं। बटन आपके ब्राउज़र में एक वेब-आधारित VS Code इंस्टेंस खोलेगा:

1. टेम्पलेट खोलें (इसमें कुछ मिनट लग सकते हैं):

    [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/microsoft/phi-3cookbook)

2. एक टर्मिनल विंडो खोलें।

### VS Code Dev Containers

⚠️ यह विकल्प तभी काम करेगा जब आपके Docker Desktop को कम से कम 16 GB RAM आवंटित की गई हो। अगर आपके पास 16 GB RAM से कम है, तो आप [GitHub Codespaces विकल्प](../../../../../md/01.Introduction/01) आज़मा सकते हैं या [लोकल सेटअप](../../../../../md/01.Introduction/01) कर सकते हैं।

एक संबंधित विकल्प VS Code Dev Containers है, जो आपके लोकल VS Code में [Dev Containers एक्सटेंशन](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers) का उपयोग करके प्रोजेक्ट खोलेगा:

1. Docker Desktop शुरू करें (अगर यह पहले से इंस्टॉल नहीं है, तो इसे इंस्टॉल करें)।
2. प्रोजेक्ट खोलें:

    [![Open in Dev Containers](https://img.shields.io/static/v1?style=for-the-badge&label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/microsoft/phi-3cookbook)

3. जब प्रोजेक्ट फाइल्स VS Code विंडो में दिखाई दें (इसमें कुछ मिनट लग सकते हैं), तो एक टर्मिनल विंडो खोलें।
4. [डिप्लॉयमेंट स्टेप्स](../../../../../md/01.Introduction/01) के साथ आगे बढ़ें।

### लोकल वातावरण

1. सुनिश्चित करें कि निम्नलिखित टूल्स इंस्टॉल हैं:

    * [Ollama](https://ollama.com/)
    * [Python 3.10+](https://www.python.org/downloads/)
    * [OpenAI Python SDK](https://pypi.org/project/openai/)

## मॉडल का परीक्षण करें

1. Ollama को phi3:mini मॉडल डाउनलोड और चलाने के लिए कहें:

    ```shell
    ollama run phi3:mini
    ```

    मॉडल डाउनलोड करने में कुछ मिनट लगेंगे।

2. जैसे ही आउटपुट में "success" दिखाई दे, आप उस मॉडल को प्रॉम्प्ट से एक संदेश भेज सकते हैं।

    ```shell
    >>> Write a haiku about hungry hippos
    ```

3. कुछ सेकंड में, आपको मॉडल से एक प्रतिक्रिया स्ट्रीम में दिखनी शुरू हो जाएगी।

4. भाषा मॉडल्स के साथ उपयोग की जाने वाली विभिन्न तकनीकों के बारे में जानने के लिए, Python नोटबुक [ollama.ipynb](../../../../../code/01.Introduce/ollama.ipynb) खोलें और प्रत्येक सेल चलाएं। यदि आपने 'phi3:mini' के अलावा कोई और मॉडल उपयोग किया है, तो फाइल के शीर्ष पर `MODEL_NAME` in the first cell.

5. To have a conversation with the phi3:mini model from Python, open the Python file [chat.py](../../../../../code/01.Introduce/chat.py) and run it. You can change the `MODEL_NAME` को आवश्यकतानुसार बदलें। आप सिस्टम संदेश को संशोधित कर सकते हैं या यदि चाहें तो कुछ शॉट उदाहरण भी जोड़ सकते हैं।

**अस्वीकरण**:  
यह दस्तावेज़ मशीन-आधारित एआई अनुवाद सेवाओं का उपयोग करके अनुवादित किया गया है। जबकि हम सटीकता के लिए प्रयास करते हैं, कृपया ध्यान दें कि स्वचालित अनुवादों में त्रुटियां या अशुद्धियां हो सकती हैं। मूल भाषा में दस्तावेज़ को प्रामाणिक स्रोत माना जाना चाहिए। महत्वपूर्ण जानकारी के लिए, पेशेवर मानव अनुवाद की सिफारिश की जाती है। इस अनुवाद के उपयोग से उत्पन्न किसी भी गलतफहमी या गलत व्याख्या के लिए हम उत्तरदायी नहीं हैं।