Phi-3-mini के संदर्भ में, इंफेरेंस उस प्रक्रिया को संदर्भित करता है जिसमें मॉडल का उपयोग इनपुट डेटा के आधार पर प्रेडिक्शन करने या आउटपुट जनरेट करने के लिए किया जाता है। आइए, मैं आपको Phi-3-mini और इसकी इंफेरेंस क्षमताओं के बारे में अधिक जानकारी देता हूं।

Phi-3-mini, Microsoft द्वारा जारी किए गए Phi-3 मॉडल श्रृंखला का हिस्सा है। ये मॉडल छोटे भाषा मॉडलों (SLMs) के साथ संभावनाओं को पुनः परिभाषित करने के लिए डिज़ाइन किए गए हैं। 

यहाँ Phi-3-mini और इसकी इंफेरेंस क्षमताओं के कुछ मुख्य बिंदु दिए गए हैं:

## **Phi-3-mini का अवलोकन:**
- Phi-3-mini में 3.8 बिलियन पैरामीटर्स हैं।
- यह पारंपरिक कंप्यूटिंग उपकरणों के साथ-साथ मोबाइल डिवाइस और IoT डिवाइस जैसे एज डिवाइस पर भी चल सकता है।
- Phi-3-mini का विमोचन व्यक्तियों और उद्यमों को विभिन्न हार्डवेयर उपकरणों पर SLMs को तैनात करने में सक्षम बनाता है, विशेष रूप से संसाधन-सीमित वातावरण में।
- यह विभिन्न मॉडल फॉर्मैट्स को कवर करता है, जिनमें पारंपरिक PyTorch फॉर्मैट, gguf फॉर्मैट का क्वांटाइज़्ड वर्शन, और ONNX-आधारित क्वांटाइज़्ड वर्शन शामिल हैं।

## **Phi-3-mini तक पहुंच:**
Phi-3-mini तक पहुंचने के लिए, आप [Semantic Kernel](https://github.com/microsoft/SemanticKernelCookBook?WT.mc_id=aiml-138114-kinfeylo) का उपयोग एक Copilot एप्लिकेशन में कर सकते हैं। Semantic Kernel आम तौर पर Azure OpenAI Service, Hugging Face पर उपलब्ध ओपन-सोर्स मॉडल्स, और लोकल मॉडल्स के साथ संगत है।  
आप [Ollama](https://ollama.com) या [LlamaEdge](https://llamaedge.com) का उपयोग करके क्वांटाइज़्ड मॉडल्स को कॉल कर सकते हैं। Ollama व्यक्तिगत उपयोगकर्ताओं को विभिन्न क्वांटाइज़्ड मॉडल्स कॉल करने की अनुमति देता है, जबकि LlamaEdge GGUF मॉडल्स के लिए क्रॉस-प्लेटफ़ॉर्म उपलब्धता प्रदान करता है।

## **क्वांटाइज़्ड मॉडल्स:**
कई उपयोगकर्ता लोकल इंफेरेंस के लिए क्वांटाइज़्ड मॉडल्स का उपयोग करना पसंद करते हैं। उदाहरण के लिए, आप सीधे Ollama run Phi-3 चला सकते हैं या इसे ऑफलाइन Modelfile का उपयोग करके कॉन्फ़िगर कर सकते हैं। Modelfile GGUF फाइल पथ और प्रॉम्प्ट फॉर्मैट को निर्दिष्ट करता है।

## **जनरेटिव AI की संभावनाएं:**
Phi-3-mini जैसे SLMs को जोड़ने से जनरेटिव AI के लिए नई संभावनाएं खुलती हैं। इंफेरेंस केवल पहला कदम है; इन मॉडलों का उपयोग संसाधन-सीमित, लेटेंसी-बाउंड, और लागत-सीमित परिदृश्यों में विभिन्न कार्यों के लिए किया जा सकता है।

## **Phi-3-mini के साथ जनरेटिव AI को अनलॉक करना: इंफेरेंस और डिप्लॉयमेंट के लिए गाइड** 
Semantic Kernel, Ollama/LlamaEdge, और ONNX Runtime का उपयोग करके Phi-3-mini मॉडल्स तक पहुंचें और इंफेरेंस करें, और विभिन्न एप्लिकेशन परिदृश्यों में जनरेटिव AI की संभावनाओं का अन्वेषण करें।

**विशेषताएँ**  
Phi-3-mini मॉडल में इंफेरेंस करें:

- [Semantic Kernel](https://github.com/Azure-Samples/Phi-3MiniSamples/tree/main/semantickernel?WT.mc_id=aiml-138114-kinfeylo)  
- [Ollama](https://github.com/Azure-Samples/Phi-3MiniSamples/tree/main/ollama?WT.mc_id=aiml-138114-kinfeylo)  
- [LlamaEdge WASM](https://github.com/Azure-Samples/Phi-3MiniSamples/tree/main/wasm?WT.mc_id=aiml-138114-kinfeylo)  
- [ONNX Runtime](https://github.com/Azure-Samples/Phi-3MiniSamples/tree/main/onnx?WT.mc_id=aiml-138114-kinfeylo)  
- [iOS](https://github.com/Azure-Samples/Phi-3MiniSamples/tree/main/ios?WT.mc_id=aiml-138114-kinfeylo)  

सारांश में, Phi-3-mini डेवलपर्स को विभिन्न मॉडल फॉर्मैट्स का अन्वेषण करने और विभिन्न एप्लिकेशन परिदृश्यों में जनरेटिव AI का लाभ उठाने की अनुमति देता है।

**अस्वीकरण**:  
यह दस्तावेज़ मशीन-आधारित एआई अनुवाद सेवाओं का उपयोग करके अनुवादित किया गया है। जबकि हम सटीकता के लिए प्रयास करते हैं, कृपया ध्यान दें कि स्वचालित अनुवाद में त्रुटियाँ या अशुद्धियाँ हो सकती हैं। मूल दस्तावेज़ को उसकी मूल भाषा में प्रामाणिक स्रोत माना जाना चाहिए। महत्वपूर्ण जानकारी के लिए, पेशेवर मानव अनुवाद की सिफारिश की जाती है। इस अनुवाद के उपयोग से उत्पन्न किसी भी गलतफहमी या व्याख्या के लिए हम उत्तरदायी नहीं हैं।