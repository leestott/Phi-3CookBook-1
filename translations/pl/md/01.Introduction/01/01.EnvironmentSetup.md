# Rozpocznij pracę z Phi-3 lokalnie

Ten przewodnik pomoże Ci skonfigurować lokalne środowisko do uruchamiania modelu Phi-3 za pomocą Ollama. Model można uruchomić na kilka różnych sposobów, w tym korzystając z GitHub Codespaces, kontenerów Dev w VS Code lub swojego lokalnego środowiska.

## Konfiguracja środowiska

### GitHub Codespaces

Możesz uruchomić ten szablon wirtualnie, korzystając z GitHub Codespaces. Kliknięcie przycisku otworzy instancję VS Code w przeglądarce:

1. Otwórz szablon (może to zająć kilka minut):

    [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/microsoft/phi-3cookbook)

2. Otwórz okno terminala.

### Kontenery Dev w VS Code

⚠️ Ta opcja zadziała tylko wtedy, gdy w Docker Desktop przydzielono co najmniej 16 GB pamięci RAM. Jeśli masz mniej niż 16 GB RAM, możesz spróbować opcji [GitHub Codespaces](../../../../../md/01.Introduction/01) lub [skonfigurować środowisko lokalnie](../../../../../md/01.Introduction/01).

Alternatywą są kontenery Dev w VS Code, które otworzą projekt w lokalnym VS Code za pomocą [rozszerzenia Dev Containers](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers):

1. Uruchom Docker Desktop (zainstaluj, jeśli jeszcze go nie masz).
2. Otwórz projekt:

    [![Open in Dev Containers](https://img.shields.io/static/v1?style=for-the-badge&label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/microsoft/phi-3cookbook)

3. W otwartym oknie VS Code, gdy pliki projektu się załadują (może to potrwać kilka minut), otwórz okno terminala.
4. Kontynuuj zgodnie z [krokami wdrożenia](../../../../../md/01.Introduction/01).

### Środowisko lokalne

1. Upewnij się, że zainstalowane są następujące narzędzia:

    * [Ollama](https://ollama.com/)
    * [Python 3.10+](https://www.python.org/downloads/)
    * [OpenAI Python SDK](https://pypi.org/project/openai/)

## Testowanie modelu

1. Poproś Ollama o pobranie i uruchomienie modelu phi3:mini:

    ```shell
    ollama run phi3:mini
    ```

    Pobranie modelu zajmie kilka minut.

2. Gdy w wyjściu pojawi się "success", możesz wysłać wiadomość do modelu z wiersza poleceń.

    ```shell
    >>> Write a haiku about hungry hippos
    ```

3. Po kilku sekundach powinieneś zobaczyć odpowiedź modelu w strumieniu.

4. Aby dowiedzieć się więcej o różnych technikach stosowanych z modelami językowymi, otwórz notebook Pythona [ollama.ipynb](../../../../../code/01.Introduce/ollama.ipynb) i uruchom każdą komórkę. Jeśli używasz innego modelu niż 'phi3:mini', zmień `MODEL_NAME` in the first cell.

5. To have a conversation with the phi3:mini model from Python, open the Python file [chat.py](../../../../../code/01.Introduce/chat.py) and run it. You can change the `MODEL_NAME` na górze pliku w razie potrzeby. Możesz także zmodyfikować wiadomość systemową lub dodać przykłady few-shot, jeśli chcesz.

**Zastrzeżenie**:  
Niniejszy dokument został przetłumaczony za pomocą usług tłumaczenia maszynowego opartego na sztucznej inteligencji. Chociaż dokładamy starań, aby zapewnić precyzję, prosimy mieć na uwadze, że tłumaczenia automatyczne mogą zawierać błędy lub nieścisłości. Oryginalny dokument w jego rodzimym języku powinien być uznawany za wiarygodne źródło. W przypadku informacji krytycznych zaleca się skorzystanie z profesjonalnego tłumaczenia przez człowieka. Nie ponosimy odpowiedzialności za jakiekolwiek nieporozumienia lub błędne interpretacje wynikające z użycia tego tłumaczenia.