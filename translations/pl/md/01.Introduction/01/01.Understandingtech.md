# Kluczowe technologie obejmują

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) – niskopoziomowe API dla przyspieszonego sprzętowo uczenia maszynowego, zbudowane na bazie DirectX 12.
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) – platforma obliczeń równoległych i model interfejsu API opracowany przez firmę Nvidia, umożliwiający przetwarzanie ogólnego przeznaczenia na procesorach graficznych (GPU).
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) – otwarty format zaprojektowany do reprezentowania modeli uczenia maszynowego, zapewniający interoperacyjność między różnymi frameworkami ML.
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) – format używany do reprezentowania i aktualizowania modeli uczenia maszynowego, szczególnie przydatny dla mniejszych modeli językowych, które mogą działać efektywnie na CPU z kwantyzacją 4-8bit.

## DirectML

DirectML to niskopoziomowe API umożliwiające przyspieszone sprzętowo uczenie maszynowe. Jest zbudowane na bazie DirectX 12, aby wykorzystać akcelerację GPU i jest niezależne od dostawcy sprzętu, co oznacza, że nie wymaga zmian w kodzie, aby działać na różnych GPU. Jest głównie używane do trenowania modeli i ich wnioskowania na GPU.

Jeśli chodzi o wsparcie sprzętowe, DirectML zostało zaprojektowane tak, aby współpracować z szeroką gamą GPU, w tym zintegrowanymi i dedykowanymi GPU AMD, zintegrowanymi GPU Intel oraz dedykowanymi GPU NVIDIA. Jest częścią platformy Windows AI i obsługiwane na systemach Windows 10 i 11, umożliwiając trenowanie i wnioskowanie modeli na dowolnym urządzeniu z Windows.

Wprowadzono aktualizacje i nowe możliwości związane z DirectML, takie jak wsparcie dla aż 150 operatorów ONNX oraz wykorzystanie przez ONNX runtime i WinML. Jest wspierane przez głównych dostawców sprzętu (IHV), z których każdy implementuje różne metakomendy.

## CUDA

CUDA, czyli Compute Unified Device Architecture, to platforma obliczeń równoległych i model interfejsu API stworzony przez firmę Nvidia. Pozwala programistom na wykorzystanie procesora graficznego (GPU) obsługującego CUDA do przetwarzania ogólnego przeznaczenia – podejścia określanego jako GPGPU (General-Purpose computing on Graphics Processing Units). CUDA jest kluczowym elementem akceleracji GPU firmy Nvidia i jest szeroko stosowane w różnych dziedzinach, takich jak uczenie maszynowe, obliczenia naukowe i przetwarzanie wideo.

Wsparcie sprzętowe dla CUDA jest specyficzne dla GPU firmy Nvidia, ponieważ jest to technologia własnościowa opracowana przez tę firmę. Każda architektura obsługuje określone wersje zestawu narzędzi CUDA, który dostarcza niezbędne biblioteki i narzędzia dla programistów do tworzenia i uruchamiania aplikacji CUDA.

## ONNX

ONNX (Open Neural Network Exchange) to otwarty format zaprojektowany do reprezentowania modeli uczenia maszynowego. Oferuje definicję rozszerzalnego modelu grafu obliczeniowego, a także definicje wbudowanych operatorów i standardowych typów danych. ONNX umożliwia programistom przenoszenie modeli między różnymi frameworkami ML, zapewniając interoperacyjność i ułatwiając tworzenie oraz wdrażanie aplikacji AI.

Phi3 mini może działać z ONNX Runtime na CPU i GPU na różnych urządzeniach, w tym na platformach serwerowych, komputerach stacjonarnych z systemami Windows, Linux i Mac oraz procesorach mobilnych. Dodaliśmy zoptymalizowane konfiguracje:

- Modele ONNX dla int4 DML: Kwantyzowane do int4 za pomocą AWQ
- Model ONNX dla fp16 CUDA
- Model ONNX dla int4 CUDA: Kwantyzowane do int4 za pomocą RTN
- Model ONNX dla int4 CPU i urządzeń mobilnych: Kwantyzowane do int4 za pomocą RTN

## Llama.cpp

Llama.cpp to otwartoźródłowa biblioteka programistyczna napisana w C++. Umożliwia wnioskowanie na różnych dużych modelach językowych (LLM), w tym na Llama. Opracowana wraz z biblioteką ggml (ogólnego przeznaczenia biblioteka tensorów), llama.cpp ma na celu zapewnienie szybszego wnioskowania i mniejszego zużycia pamięci w porównaniu do pierwotnej implementacji w Pythonie. Obsługuje optymalizację sprzętową, kwantyzację i oferuje prosty interfejs API oraz przykłady. Jeśli interesuje Cię wydajne wnioskowanie z użyciem LLM, warto zapoznać się z Llama.cpp, ponieważ Phi3 może działać z Llama.cpp.

## GGUF

GGUF (Generic Graph Update Format) to format używany do reprezentowania i aktualizowania modeli uczenia maszynowego. Jest szczególnie przydatny dla mniejszych modeli językowych (SLM), które mogą działać efektywnie na CPU z kwantyzacją 4-8bit. GGUF jest korzystny w przypadku szybkiego prototypowania oraz uruchamiania modeli na urządzeniach brzegowych lub w zadaniach wsadowych, takich jak potoki CI/CD.

**Zastrzeżenie**:  
Ten dokument został przetłumaczony za pomocą usług tłumaczenia opartego na sztucznej inteligencji. Chociaż staramy się zapewnić dokładność, prosimy pamiętać, że tłumaczenia automatyczne mogą zawierać błędy lub nieścisłości. Oryginalny dokument w jego rodzimym języku powinien być uznawany za wiarygodne źródło. W przypadku istotnych informacji zaleca się skorzystanie z profesjonalnego tłumaczenia wykonanego przez człowieka. Nie ponosimy odpowiedzialności za jakiekolwiek nieporozumienia lub błędne interpretacje wynikające z korzystania z tego tłumaczenia.