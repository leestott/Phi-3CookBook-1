# **Lab 2 - Uruchamianie Prompt flow z Phi-3-mini w AIPC**

## **Czym jest Prompt flow**

Prompt flow to zestaw narzędzi deweloperskich zaprojektowanych, aby usprawnić cały cykl tworzenia aplikacji AI opartych na LLM, od pomysłu, prototypowania, testowania, oceny, aż po wdrożenie i monitorowanie w produkcji. Ułatwia inżynierię promptów i umożliwia budowanie aplikacji LLM o jakości produkcyjnej.

Dzięki Prompt flow możesz:

- Tworzyć przepływy, które łączą LLM, prompty, kod Pythona i inne narzędzia w wykonalny workflow.

- Debugować i iterować swoje przepływy, zwłaszcza interakcje z LLM, w prosty sposób.

- Ocenić swoje przepływy, obliczać metryki jakości i wydajności na większych zbiorach danych.

- Zintegrować testowanie i ocenę w swoim systemie CI/CD, aby zapewnić jakość przepływu.

- Wdrażać swoje przepływy na wybraną platformę serwerową lub łatwo integrować je z kodem aplikacji.

- (Opcjonalnie, ale wysoce zalecane) Współpracować z zespołem, korzystając z wersji chmurowej Prompt flow w Azure AI.



## **Tworzenie przepływów generowania kodu na Apple Silicon**

***Uwaga***: Jeśli nie ukończyłeś instalacji środowiska, odwiedź [Lab 0 - Installations](./01.Installations.md)

1. Otwórz rozszerzenie Prompt flow w Visual Studio Code i utwórz pusty projekt przepływu.

![create](../../../../../../../../../translated_images/pf_create.d6172d8277a78a7fa82cd6ff727ed44e037fa78b662f1f62d5963f36d712d229.pl.png)

2. Dodaj parametry wejścia i wyjścia oraz dodaj kod Pythona jako nowy przepływ.

![flow](../../../../../../../../../translated_images/pf_flow.d5646a323fb7f444c0b98b4521057a592325c583e7ba18bc31500bc0415e9ef3.pl.png)

Możesz odnieść się do tej struktury (flow.dag.yaml), aby skonstruować swój przepływ.

```yaml

inputs:
  prompt:
    type: string
    default: Write python code for Fibonacci serie. Please use markdown as output
outputs:
  result:
    type: string
    reference: ${gen_code_by_phi3.output}
nodes:
- name: gen_code_by_phi3
  type: python
  source:
    type: code
    path: gen_code_by_phi3.py
  inputs:
    prompt: ${inputs.prompt}


```

3. Kwantyzuj phi-3-mini.

Chcemy lepiej uruchamiać SLM na urządzeniach lokalnych. Zazwyczaj kwantyzujemy model (INT4, FP16, FP32).

```bash

python -m mlx_lm.convert --hf-path microsoft/Phi-3-mini-4k-instruct

```

**Uwaga:** domyślny folder to mlx_model.

4. Dodaj kod w ***Chat_With_Phi3.py***.

```python


from promptflow import tool

from mlx_lm import load, generate


# The inputs section will change based on the arguments of the tool function, after you save the code
# Adding type to arguments and return value will help the system show the types properly
# Please update the function name/signature per need
@tool
def my_python_tool(prompt: str) -> str:

    model_id = './mlx_model_phi3_mini'

    model, tokenizer = load(model_id)

    # <|user|>\nWrite python code for Fibonacci serie. Please use markdown as output<|end|>\n<|assistant|>

    response = generate(model, tokenizer, prompt="<|user|>\n" + prompt  + "<|end|>\n<|assistant|>", max_tokens=2048, verbose=True)

    return response


```

4. Możesz przetestować przepływ, korzystając z Debug lub Run, aby sprawdzić, czy generowanie kodu działa poprawnie.

![RUN](../../../../../../../../../translated_images/pf_run.d918637dc00f61e9bdeec37d4cc9646f77d270ac9203bcce13569f3157202b6e.pl.png)

5. Uruchom przepływ jako API deweloperskie w terminalu.

```

pf flow serve --source ./ --port 8080 --host localhost   

```

Możesz przetestować go w Postman / Thunder Client.


### **Uwaga**

1. Pierwsze uruchomienie zajmuje dużo czasu. Zaleca się pobranie modelu phi-3 z Hugging Face CLI.

2. Ze względu na ograniczoną moc obliczeniową Intel NPU, zaleca się użycie Phi-3-mini-4k-instruct.

3. Używamy akceleracji Intel NPU do konwersji kwantyzacji INT4, ale jeśli ponownie uruchomisz usługę, musisz usunąć foldery cache i nc_workshop.



## **Zasoby**

1. Naucz się Promptflow [https://microsoft.github.io/promptflow/](https://microsoft.github.io/promptflow/)

2. Naucz się akceleracji Intel NPU [https://github.com/intel/intel-npu-acceleration-library](https://github.com/intel/intel-npu-acceleration-library)

3. Przykładowy kod, pobierz [Local NPU Agent Sample Code](../../../../../../../../../code/07.Lab/01/AIPC/local-npu-agent)

**Zastrzeżenie**:  
Ten dokument został przetłumaczony za pomocą usług tłumaczenia maszynowego opartego na sztucznej inteligencji. Chociaż staramy się zapewnić dokładność, prosimy mieć na uwadze, że automatyczne tłumaczenia mogą zawierać błędy lub nieścisłości. Oryginalny dokument w jego rodzimym języku powinien być uznawany za wiarygodne źródło. W przypadku istotnych informacji zaleca się skorzystanie z profesjonalnego tłumaczenia wykonanego przez człowieka. Nie ponosimy odpowiedzialności za jakiekolwiek nieporozumienia lub błędne interpretacje wynikające z użycia tego tłumaczenia.