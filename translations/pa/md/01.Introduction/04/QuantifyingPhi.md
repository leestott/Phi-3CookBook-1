# **ਫਾਈ ਫੈਮਿਲੀ ਦੀ ਗਿਣਤੀ**

ਮਾਡਲ ਕੁਆੰਟਾਈਜ਼ੇਸ਼ਨ ਦਾ ਅਰਥ ਹੈ ਨਿਊਰਲ ਨੈੱਟਵਰਕ ਮਾਡਲ ਵਿੱਚ ਪੈਰਾਮੀਟਰਾਂ (ਜਿਵੇਂ ਕਿ ਵਜ਼ਨ ਅਤੇ ਐਕਟੀਵੇਸ਼ਨ ਮੁੱਲਾਂ) ਨੂੰ ਇੱਕ ਵੱਡੀ ਮੁੱਲ ਰੇਂਜ (ਆਮ ਤੌਰ 'ਤੇ ਇੱਕ ਲਗਾਤਾਰ ਮੁੱਲ ਰੇਂਜ) ਤੋਂ ਛੋਟੇ ਸੀਮਤ ਮੁੱਲ ਰੇਂਜ ਵਿੱਚ ਮੈਪ ਕਰਨਾ। ਇਹ ਤਕਨਾਲੋਜੀ ਮਾਡਲ ਦੇ ਆਕਾਰ ਅਤੇ ਗਣਨਾਤਮਕ ਜਟਿਲਤਾ ਨੂੰ ਘਟਾ ਸਕਦੀ ਹੈ ਅਤੇ ਮੋਬਾਈਲ ਡਿਵਾਈਸ ਜਾਂ ਐਂਬੇਡਡ ਸਿਸਟਮ ਵਰਗੇ ਸਰੋਤ-ਸੀਮਤ ਵਾਤਾਵਰਣਾਂ ਵਿੱਚ ਮਾਡਲ ਦੀ ਕੰਮ ਕਰਨ ਦੀ ਕੁਸ਼ਲਤਾ ਵਿੱਚ ਸੁਧਾਰ ਕਰ ਸਕਦੀ ਹੈ। ਮਾਡਲ ਕੁਆੰਟਾਈਜ਼ੇਸ਼ਨ ਪੈਰਾਮੀਟਰਾਂ ਦੀ ਸ਼ੁੱਧਤਾ ਨੂੰ ਘਟਾ ਕੇ ਕੰਪ੍ਰੈਸ਼ਨ ਪ੍ਰਾਪਤ ਕਰਦੀ ਹੈ, ਪਰ ਇਸ ਨਾਲ ਕੁਝ ਸ਼ੁੱਧਤਾ ਦੀ ਹਾਨੀ ਵੀ ਹੁੰਦੀ ਹੈ। ਇਸ ਲਈ, ਕੁਆੰਟਾਈਜ਼ੇਸ਼ਨ ਪ੍ਰਕਿਰਿਆ ਵਿੱਚ, ਮਾਡਲ ਦੇ ਆਕਾਰ, ਗਣਨਾਤਮਕ ਜਟਿਲਤਾ, ਅਤੇ ਸ਼ੁੱਧਤਾ ਵਿੱਚ ਸੰਤੁਲਨ ਬਣਾਉਣਾ ਲਾਜ਼ਮੀ ਹੈ। ਆਮ ਕੁਆੰਟਾਈਜ਼ੇਸ਼ਨ ਤਰੀਕਿਆਂ ਵਿੱਚ ਫਿਕਸਡ-ਪੌਇੰਟ ਕੁਆੰਟਾਈਜ਼ੇਸ਼ਨ, ਫਲੋਟਿੰਗ-ਪੌਇੰਟ ਕੁਆੰਟਾਈਜ਼ੇਸ਼ਨ ਆਦਿ ਸ਼ਾਮਲ ਹਨ। ਤੁਸੀਂ ਖਾਸ ਸਥਿਤੀ ਅਤੇ ਜ਼ਰੂਰਤਾਂ ਦੇ ਅਨੁਸਾਰ ਉਚਿਤ ਕੁਆੰਟਾਈਜ਼ੇਸ਼ਨ ਰਣਨੀਤੀ ਚੁਣ ਸਕਦੇ ਹੋ।

ਅਸੀਂ ਚਾਹੁੰਦੇ ਹਾਂ ਕਿ GenAI ਮਾਡਲ ਨੂੰ ਐਜ ਡਿਵਾਈਸਾਂ 'ਤੇ ਡਿਪਲੌਇ ਕੀਤਾ ਜਾਵੇ ਅਤੇ ਵੱਧ ਡਿਵਾਈਸਾਂ ਨੂੰ GenAI ਸਥਿਤੀਆਂ ਵਿੱਚ ਸ਼ਾਮਲ ਕੀਤਾ ਜਾਵੇ, ਜਿਵੇਂ ਕਿ ਮੋਬਾਈਲ ਡਿਵਾਈਸ, AI PC/Copilot+PC, ਅਤੇ ਪਰੰਪਰਾਗਤ IoT ਡਿਵਾਈਸ। ਕੁਆੰਟਾਈਜ਼ੇਸ਼ਨ ਮਾਡਲ ਦੇ ਜ਼ਰੀਏ, ਅਸੀਂ ਵੱਖ-ਵੱਖ ਡਿਵਾਈਸਾਂ ਦੇ ਅਧਾਰ 'ਤੇ ਇਸਨੂੰ ਵੱਖ-ਵੱਖ ਐਜ ਡਿਵਾਈਸਾਂ 'ਤੇ ਡਿਪਲੌਇ ਕਰ ਸਕਦੇ ਹਾਂ। ਹਾਰਡਵੇਅਰ ਨਿਰਮਾਤਾਵਾਂ ਦੁਆਰਾ ਪ੍ਰਦਾਨ ਕੀਤੇ ਮਾਡਲ ਐਕਸਲੇਰੇਸ਼ਨ ਫਰੇਮਵਰਕ ਅਤੇ ਕੁਆੰਟਾਈਜ਼ੇਸ਼ਨ ਮਾਡਲ ਨੂੰ ਜੋੜਕੇ, ਅਸੀਂ ਵਧੀਆ SLM ਐਪਲੀਕੇਸ਼ਨ ਸਥਿਤੀਆਂ ਬਣਾਉਣ ਦੇ ਯੋਗ ਹੋ ਸਕਦੇ ਹਾਂ।

ਕੁਆੰਟਾਈਜ਼ੇਸ਼ਨ ਸਥਿਤੀ ਵਿੱਚ, ਸਾਡੇ ਕੋਲ ਵੱਖ-ਵੱਖ ਸ਼ੁੱਧਤਾਵਾਂ ਹਨ (INT4, INT8, FP16, FP32)। ਹੇਠਾਂ ਆਮ ਵਰਤੇ ਜਾਂਦੇ ਕੁਆੰਟਾਈਜ਼ੇਸ਼ਨ ਸ਼ੁੱਧਤਾਵਾਂ ਦੀ ਵਿਆਖਿਆ ਦਿੱਤੀ ਗਈ ਹੈ।

### **INT4**

INT4 ਕੁਆੰਟਾਈਜ਼ੇਸ਼ਨ ਇੱਕ ਤਿੱਖੀ ਕੁਆੰਟਾਈਜ਼ੇਸ਼ਨ ਵਿਧੀ ਹੈ ਜੋ ਮਾਡਲ ਦੇ ਵਜ਼ਨਾਂ ਅਤੇ ਐਕਟੀਵੇਸ਼ਨ ਮੁੱਲਾਂ ਨੂੰ 4-ਬਿਟ ਪੂਰਨ ਅੰਕਾਂ ਵਿੱਚ ਕੁਆੰਟਾਈਜ਼ ਕਰਦੀ ਹੈ। INT4 ਕੁਆੰਟਾਈਜ਼ੇਸ਼ਨ ਆਮ ਤੌਰ 'ਤੇ ਵੱਡੇ ਪੱਧਰ ਦੀ ਸ਼ੁੱਧਤਾ ਹਾਨੀ ਦਾ ਕਾਰਨ ਬਣਦੀ ਹੈ, ਕਿਉਂਕਿ ਇਸਦੀ ਪ੍ਰਸਤੁਤੀ ਰੇਂਜ ਛੋਟੀ ਹੁੰਦੀ ਹੈ ਅਤੇ ਸ਼ੁੱਧਤਾ ਘੱਟ ਹੁੰਦੀ ਹੈ। ਪਰ, INT8 ਕੁਆੰਟਾਈਜ਼ੇਸ਼ਨ ਦੇ ਮੁਕਾਬਲੇ, INT4 ਕੁਆੰਟਾਈਜ਼ੇਸ਼ਨ ਮਾਡਲ ਦੀ ਸਟੋਰੇਜ ਦੀ ਲੋੜਾਂ ਅਤੇ ਗਣਨਾਤਮਕ ਜਟਿਲਤਾ ਨੂੰ ਹੋਰ ਘਟਾ ਸਕਦੀ ਹੈ। ਇਹ ਧਿਆਨ ਦੇਣ ਯੋਗ ਹੈ ਕਿ ਅਮਲੀ ਐਪਲੀਕੇਸ਼ਨਾਂ ਵਿੱਚ INT4 ਕੁਆੰਟਾਈਜ਼ੇਸ਼ਨ ਸੰਬੰਧਿਤ ਕਮ ਹੀ ਵਰਤੀ ਜਾਂਦੀ ਹੈ, ਕਿਉਂਕਿ ਬਹੁਤ ਘੱਟ ਸ਼ੁੱਧਤਾ ਮਾਡਲ ਦੇ ਪ੍ਰਦਰਸ਼ਨ ਵਿੱਚ ਵੱਡੇ ਪੱਧਰ ਦੀ ਗਿਰਾਵਟ ਦਾ ਕਾਰਨ ਬਣ ਸਕਦੀ ਹੈ। ਇਸ ਤੋਂ ਇਲਾਵਾ, ਸਾਰੇ ਹਾਰਡਵੇਅਰ INT4 ਓਪਰੇਸ਼ਨਾਂ ਨੂੰ ਸਮਰਥਨ ਨਹੀਂ ਕਰਦੇ, ਇਸ ਲਈ ਕੁਆੰਟਾਈਜ਼ੇਸ਼ਨ ਵਿਧੀ ਚੁਣਦੇ ਸਮੇਂ ਹਾਰਡਵੇਅਰ ਅਨੁਕੂਲਤਾ ਨੂੰ ਧਿਆਨ ਵਿੱਚ ਰੱਖਣਾ ਪੈਂਦਾ ਹੈ।

### **INT8**

INT8 ਕੁਆੰਟਾਈਜ਼ੇਸ਼ਨ ਮਾਡਲ ਦੇ ਵਜ਼ਨਾਂ ਅਤੇ ਐਕਟੀਵੇਸ਼ਨ ਨੂੰ ਫਲੋਟਿੰਗ ਪੌਇੰਟ ਅੰਕਾਂ ਤੋਂ 8-ਬਿਟ ਪੂਰਨ ਅੰਕਾਂ ਵਿੱਚ ਬਦਲਣ ਦੀ ਪ੍ਰਕਿਰਿਆ ਹੈ। ਹਾਲਾਂਕਿ INT8 ਪੂਰਨ ਅੰਕਾਂ ਦੁਆਰਾ ਦਰਸਾਏ ਜਾਣ ਵਾਲੇ ਗਿਣਤੀਆਂ ਦੇ ਰੇਂਜ ਛੋਟੇ ਹੁੰਦੇ ਹਨ ਅਤੇ ਘੱਟ ਸ਼ੁੱਧਤਾ ਵਾਲੇ ਹੁੰਦੇ ਹਨ, ਇਹ ਸਟੋਰੇਜ ਅਤੇ ਗਣਨਾ ਦੀਆਂ ਲੋੜਾਂ ਨੂੰ ਮਹੱਤਵਪੂਰਨ ਰੂਪ ਵਿੱਚ ਘਟਾ ਸਕਦੇ ਹਨ। INT8 ਕੁਆੰਟਾਈਜ਼ੇਸ਼ਨ ਵਿੱਚ, ਮਾਡਲ ਦੇ ਵਜ਼ਨਾਂ ਅਤੇ ਐਕਟੀਵੇਸ਼ਨ ਮੁੱਲਾਂ ਨੂੰ ਇੱਕ ਕੁਆੰਟਾਈਜ਼ੇਸ਼ਨ ਪ੍ਰਕਿਰਿਆ ਵਿੱਚ ਲੰਘਾਇਆ ਜਾਂਦਾ ਹੈ, ਜਿਸ ਵਿੱਚ ਸਕੇਲਿੰਗ ਅਤੇ ਆਫਸੈਟ ਸ਼ਾਮਲ ਹੁੰਦੇ ਹਨ, ਤਾਂ ਜੋ ਮੁਲਾਂਕਣ ਦੌਰਾਨ ਮੂਲ ਫਲੋਟਿੰਗ ਪੌਇੰਟ ਜਾਣਕਾਰੀ ਨੂੰ ਸੰਭਵ ਤੌਰ 'ਤੇ ਸੰਭਾਲਿਆ ਜਾ ਸਕੇ। ਇਸ ਵਿਧੀ ਨਾਲ ਜ਼ਿਆਦਾਤਰ ਐਪਲੀਕੇਸ਼ਨਾਂ ਵਿੱਚ ਕਾਫੀ ਸ਼ੁੱਧਤਾ ਪ੍ਰਾਪਤ ਕੀਤੀ ਜਾ ਸਕਦੀ ਹੈ ਜਦਕਿ ਗਣਨਾਤਮਕ ਕੁਸ਼ਲਤਾ ਉੱਚੀ ਰਹਿੰਦੀ ਹੈ।

### **FP16**

FP16 ਫਾਰਮੈਟ, ਅਰਥਾਤ 16-ਬਿਟ ਫਲੋਟਿੰਗ ਪੌਇੰਟ ਅੰਕ (float16), 32-ਬਿਟ ਫਲੋਟਿੰਗ ਪੌਇੰਟ ਅੰਕਾਂ (float32) ਨਾਲੋਂ ਮੈਮੋਰੀ ਦੇ ਖਪਤ ਨੂੰ ਅੱਧਾ ਘਟਾਉਂਦਾ ਹੈ, ਜੋ ਵੱਡੇ ਪੱਧਰ ਦੇ ਡੀਪ ਲਰਨਿੰਗ ਐਪਲੀਕੇਸ਼ਨਾਂ ਵਿੱਚ ਮਹੱਤਵਪੂਰਨ ਫਾਇਦੇ ਦਿੰਦਾ ਹੈ। FP16 ਫਾਰਮੈਟ ਇੱਕੋ GPU ਮੈਮੋਰੀ ਸੀਮਾਵਾਂ ਦੇ ਅੰਦਰ ਵੱਡੇ ਮਾਡਲ ਲੋਡ ਕਰਨ ਜਾਂ ਹੋਰ ਡਾਟਾ ਪ੍ਰੋਸੈਸ ਕਰਨ ਦੀ ਇਜਾਜ਼ਤ ਦਿੰਦਾ ਹੈ। ਜਿਵੇਂ ਕਿ ਆਧੁਨਿਕ GPU ਹਾਰਡਵੇਅਰ FP16 ਓਪਰੇਸ਼ਨਾਂ ਨੂੰ ਸਮਰਥਨ ਕਰਦਾ ਹੈ, FP16 ਫਾਰਮੈਟ ਵਰਤਣ ਨਾਲ ਗਣਨਾਤਮਕ ਗਤੀ ਵਿੱਚ ਵੀ ਸੁਧਾਰ ਹੋ ਸਕਦਾ ਹੈ। ਹਾਲਾਂਕਿ, FP16 ਫਾਰਮੈਟ ਦੀ ਆਪਣੀ ਕੁਝ ਘਟਨਾਵਾਂ ਹਨ, ਜਿਵੇਂ ਕਿ ਘੱਟ ਸ਼ੁੱਧਤਾ, ਜੋ ਕੁਝ ਹਾਲਾਤਾਂ ਵਿੱਚ ਅੰਕੀ ਅਸਥਿਰਤਾ ਜਾਂ ਸ਼ੁੱਧਤਾ ਦੀ ਹਾਨੀ ਦਾ ਕਾਰਨ ਬਣ ਸਕਦੀ ਹੈ।

### **FP32**

FP32 ਫਾਰਮੈਟ ਵੱਧ ਸ਼ੁੱਧਤਾ ਪ੍ਰਦਾਨ ਕਰਦਾ ਹੈ ਅਤੇ ਮੁਲਾਂਕਣ ਲਈ ਵਿਆਪਕ ਰੇਂਜ ਨੂੰ ਸਹੀ ਤਰੀਕੇ ਨਾਲ ਦਰਸਾ ਸਕਦਾ ਹੈ। ਜਿਨ੍ਹਾਂ ਸਥਿਤੀਆਂ ਵਿੱਚ ਜਟਿਲ ਗਣਿਤੀਕਲ ਕਾਰਵਾਈਆਂ ਕੀਤੀਆਂ ਜਾਂਦੀਆਂ ਹਨ ਜਾਂ ਉੱਚ-ਸ਼ੁੱਧਤਾ ਵਾਲੇ ਨਤੀਜੇ ਲੋੜੀਂਦੇ ਹਨ, ਉਥੇ FP32 ਫਾਰਮੈਟ ਨੂੰ ਤਰਜੀਹ ਦਿੱਤੀ ਜਾਂਦੀ ਹੈ। ਹਾਲਾਂਕਿ, ਵਧੀਕ ਸ਼ੁੱਧਤਾ ਦਾ ਅਰਥ ਵਧੇਰੇ ਮੈਮੋਰੀ ਦੀ ਵਰਤੋਂ ਅਤੇ ਲੰਬੇ ਗਣਨਾਤਮਕ ਸਮੇਂ ਨਾਲ ਹੁੰਦਾ ਹੈ। ਵੱਡੇ ਪੱਧਰ ਦੇ ਡੀਪ ਲਰਨਿੰਗ ਮਾਡਲਾਂ ਲਈ, ਖਾਸ ਕਰਕੇ ਜਦੋਂ ਮਾਡਲ ਪੈਰਾਮੀਟਰਾਂ ਦੀ ਗਿਣਤੀ ਜ਼ਿਆਦਾ ਹੁੰਦੀ ਹੈ ਅਤੇ ਡਾਟਾ ਦੀ ਮਾਤਰਾ ਬਹੁਤ ਵੱਡੀ ਹੁੰਦੀ ਹੈ, FP32 ਫਾਰਮੈਟ GPU ਮੈਮੋਰੀ ਦੀ ਕਮੀ ਜਾਂ ਇਨਫਰੈਂਸ ਗਤੀ ਵਿੱਚ ਕਮੀ ਦਾ ਕਾਰਨ ਬਣ ਸਕਦਾ ਹੈ।

ਮੋਬਾਈਲ ਡਿਵਾਈਸਾਂ ਜਾਂ IoT ਡਿਵਾਈਸਾਂ 'ਤੇ, ਅਸੀਂ Phi-3.x ਮਾਡਲਾਂ ਨੂੰ INT4 ਵਿੱਚ ਬਦਲ ਸਕਦੇ ਹਾਂ, ਜਦਕਿ AI PC / Copilot PC ਵਧੇਰੇ ਸ਼ੁੱਧਤਾ ਵਾਲੇ ਜਿਵੇਂ ਕਿ INT8, FP16, FP32 ਵਰਗੇ ਫਾਰਮੈਟ ਵਰਤ ਸਕਦੇ ਹਨ।

ਵਰਤਮਾਨ ਵਿੱਚ, ਵੱਖ-ਵੱਖ ਹਾਰਡਵੇਅਰ ਨਿਰਮਾਤਾਵਾਂ ਕੋਲ ਜਨਰੇਟਿਵ ਮਾਡਲਾਂ ਦਾ ਸਮਰਥਨ ਕਰਨ ਲਈ ਵੱਖ-ਵੱਖ ਫਰੇਮਵਰਕ ਹਨ, ਜਿਵੇਂ ਕਿ Intel ਦਾ OpenVINO, Qualcomm ਦਾ QNN, Apple ਦਾ MLX, ਅਤੇ Nvidia ਦਾ CUDA ਆਦਿ। ਇਨ੍ਹਾਂ ਨੂੰ ਮਾਡਲ ਕੁਆੰਟਾਈਜ਼ੇਸ਼ਨ ਨਾਲ ਜੋੜਕੇ ਸਥਾਨਕ ਡਿਪਲੌਇਮੈਂਟ ਪੂਰਾ ਕੀਤਾ ਜਾ ਸਕਦਾ ਹੈ।

ਤਕਨਾਲੋਜੀ ਦੇ ਪੱਖ ਤੋਂ, ਕੁਆੰਟਾਈਜ਼ੇਸ਼ਨ ਤੋਂ ਬਾਅਦ ਸਾਡੇ ਕੋਲ ਵੱਖ-ਵੱਖ ਫਾਰਮੈਟਾਂ ਦਾ ਸਮਰਥਨ ਹੈ, ਜਿਵੇਂ ਕਿ PyTorch / Tensorflow ਫਾਰਮੈਟ, GGUF, ਅਤੇ ONNX। ਮੈਂ GGUF ਅਤੇ ONNX ਦੇ ਵਿਚਕਾਰ ਇੱਕ ਫਾਰਮੈਟ ਤੁਲਨਾ ਅਤੇ ਐਪਲੀਕੇਸ਼ਨ ਸਥਿਤੀਆਂ ਦੀ ਵਿਸਥਾਰ ਵਿੱਚ ਚਰਚਾ ਕੀਤੀ ਹੈ। ਇੱਥੇ ਮੈਂ ONNX ਕੁਆੰਟਾਈਜ਼ੇਸ਼ਨ ਫਾਰਮੈਟ ਦੀ ਸਿਫਾਰਸ਼ ਕਰਦਾ ਹਾਂ, ਜਿਸਦਾ ਮਾਡਲ ਫਰੇਮਵਰਕ ਤੋਂ ਹਾਰਡਵੇਅਰ ਤੱਕ ਵਧੀਆ ਸਮਰਥਨ ਹੈ। ਇਸ ਅਧਿਆਇ ਵਿੱਚ, ਅਸੀਂ GenAI ਲਈ ONNX Runtime, OpenVINO, ਅਤੇ Apple MLX ਦੇ ਨਾਲ ਮਾਡਲ ਕੁਆੰਟਾਈਜ਼ੇਸ਼ਨ ਕਰਨ 'ਤੇ ਧਿਆਨ ਦੇਵਾਂਗੇ (ਜੇ ਤੁਹਾਡੇ ਕੋਲ ਕੋਈ ਵਧੀਆ ਤਰੀਕਾ ਹੈ, ਤਾਂ ਤੁਸੀਂ PR ਭੇਜ ਕੇ ਸਾਡੇ ਨਾਲ ਸਾਂਝਾ ਕਰ ਸਕਦੇ ਹੋ)।

**ਇਸ ਅਧਿਆਇ ਵਿੱਚ ਸ਼ਾਮਲ ਹੈ**

1. [Phi-3.5 / 4 ਨੂੰ llama.cpp ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਕੁਆੰਟਾਈਜ਼ ਕਰਨਾ](./UsingLlamacppQuantifyingPhi.md)

2. [Phi-3.5 / 4 ਨੂੰ Generative AI extensions ਲਈ onnxruntime ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਕੁਆੰਟਾਈਜ਼ ਕਰਨਾ](./UsingORTGenAIQuantifyingPhi.md)

3. [Phi-3.5 / 4 ਨੂੰ Intel OpenVINO ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਕੁਆੰਟਾਈਜ਼ ਕਰਨਾ](./UsingIntelOpenVINOQuantifyingPhi.md)

4. [Phi-3.5 / 4 ਨੂੰ Apple MLX Framework ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਕੁਆੰਟਾਈਜ਼ ਕਰਨਾ](./UsingAppleMLXQuantifyingPhi.md)

**ਅਸਵੀਕਤੀ**:  
ਇਹ ਦਸਤਾਵੇਜ਼ ਮਸ਼ੀਨ-ਅਧਾਰਿਤ AI ਅਨੁਵਾਦ ਸੇਵਾਵਾਂ ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਅਨੁਵਾਦਿਤ ਕੀਤਾ ਗਿਆ ਹੈ। ਜਦੋਂ ਕਿ ਅਸੀਂ ਸਹੀ ਹੋਣ ਦਾ ਯਤਨ ਕਰਦੇ ਹਾਂ, ਕਿਰਪਾ ਕਰਕੇ ਧਿਆਨ ਦਿਓ ਕਿ ਆਟੋਮੇਟਿਕ ਅਨੁਵਾਦਾਂ ਵਿੱਚ ਗਲਤੀਆਂ ਜਾਂ ਅਸੁਣਤਾਂ ਹੋ ਸਕਦੀਆਂ ਹਨ। ਮੂਲ ਦਸਤਾਵੇਜ਼ ਨੂੰ ਇਸਦੀ ਮੂਲ ਭਾਸ਼ਾ ਵਿੱਚ ਪ੍ਰਮਾਣਿਕ ਸਰੋਤ ਮੰਨਿਆ ਜਾਣਾ ਚਾਹੀਦਾ ਹੈ। ਮਹੱਤਵਪੂਰਨ ਜਾਣਕਾਰੀ ਲਈ, ਪੇਸ਼ੇਵਰ ਮਨੁੱਖੀ ਅਨੁਵਾਦ ਦੀ ਸਿਫਾਰਸ਼ ਕੀਤੀ ਜਾਂਦੀ ਹੈ। ਇਸ ਅਨੁਵਾਦ ਦੀ ਵਰਤੋਂ ਤੋਂ ਪੈਦਾ ਹੋਣ ਵਾਲੀਆਂ ਕਿਸੇ ਵੀ ਗਲਤਫਹਿਮੀਆਂ ਜਾਂ ਗਲਤ ਵਿਆਖਿਆਵਾਂ ਲਈ ਅਸੀਂ ਜ਼ਿੰਮੇਵਾਰ ਨਹੀਂ ਹਾਂ।