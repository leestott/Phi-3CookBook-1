# उल्लेखित मुख्य प्रविधिहरू

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - DirectX 12 मा आधारित हार्डवेयर-त्वरित मेसिन लर्निङको लागि कम-स्तरीय API।
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - Nvidia द्वारा विकसित समानान्तर कम्प्युटिङ प्लेटफर्म र एप्लिकेसन प्रोग्रामिङ इन्टरफेस (API) मोडेल जसले GPUs मा सामान्य-उद्देश्यीय प्रोसेसिङलाई सक्षम बनाउँछ।
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - मेसिन लर्निङ मोडेलहरूको प्रतिनिधित्व गर्न डिजाइन गरिएको खुला ढाँचा, जसले विभिन्न ML फ्रेमवर्कहरू बीच अन्तरसञ्चालन प्रदान गर्दछ।
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - मेसिन लर्निङ मोडेलहरूलाई प्रतिनिधित्व गर्न र अपडेट गर्न प्रयोग हुने ढाँचा, विशेष गरी CPU मा 4-8bit क्वान्टाइजेशनका साथ प्रभावकारी रूपमा चल्ने साना भाषा मोडेलहरूको लागि उपयोगी।

## DirectML

DirectML हार्डवेयर-त्वरित मेसिन लर्निङ सक्षम पार्ने कम-स्तरीय API हो। यो GPU एक्सेलेरेशनको उपयोग गर्न DirectX 12 मा आधारित छ र भेन्डर-एग्नोस्टिक छ, जसले विभिन्न GPU भेन्डरहरूमा काम गर्न कोड परिवर्तन आवश्यक पर्दैन। यो मुख्य रूपमा GPU मा मोडेल ट्रेनिङ र इन्फरेन्सिङ वर्कलोडहरूको लागि प्रयोग गरिन्छ।

हार्डवेयर समर्थनको सन्दर्भमा, DirectML धेरै प्रकारका GPUs सँग काम गर्न डिजाइन गरिएको छ, जसमा AMD को इंटिग्रेटेड र डिस्क्रिट GPUs, Intel को इंटिग्रेटेड GPUs, र NVIDIA को डिस्क्रिट GPUs समावेश छन्। यो Windows AI Platform को भाग हो र Windows 10 र 11 मा समर्थित छ, जसले कुनै पनि Windows उपकरणमा मोडेल ट्रेनिङ र इन्फरेन्सिङ गर्न अनुमति दिन्छ।

DirectML सँग सम्बन्धित अपडेट र अवसरहरू छन्, जस्तै 150 सम्म ONNX अपरेटरहरूको समर्थन गर्नु र ONNX runtime र WinML द्वारा प्रयोग गरिनु। यसलाई प्रमुख Integrated Hardware Vendors (IHVs) ले समर्थन गरेका छन्, जसले विभिन्न मेटाकमाण्डहरू कार्यान्वयन गरेका छन्।

## CUDA

CUDA, जसको पूर्ण रूप Compute Unified Device Architecture हो, Nvidia द्वारा सिर्जित समानान्तर कम्प्युटिङ प्लेटफर्म र एप्लिकेसन प्रोग्रामिङ इन्टरफेस (API) मोडेल हो। यसले सफ्टवेयर विकासकर्ताहरूलाई CUDA-सक्षम GPU प्रयोग गरेर सामान्य उद्देश्यीय प्रोसेसिङ गर्न अनुमति दिन्छ – जसलाई GPGPU (Graphics Processing Units मा General-Purpose कम्प्युटिङ) भनिन्छ। CUDA Nvidia को GPU एक्सेलेरेशनको प्रमुख प्रविधि हो र यो मेसिन लर्निङ, वैज्ञानिक कम्प्युटिङ, र भिडियो प्रोसेसिङ जस्ता विभिन्न क्षेत्रहरूमा व्यापक रूपमा प्रयोग गरिन्छ।

CUDA को हार्डवेयर समर्थन Nvidia का GPUs मा मात्र सीमित छ, किनभने यो Nvidia द्वारा विकसित स्वामित्व प्रविधि हो। प्रत्येक आर्किटेक्चरले CUDA टूलकिटका विशिष्ट संस्करणहरूको समर्थन गर्दछ, जसले विकासकर्ताहरूलाई CUDA एप्लिकेसनहरू निर्माण र चलाउन आवश्यक पुस्तकालय र उपकरणहरू प्रदान गर्दछ।

## ONNX

ONNX (Open Neural Network Exchange) मेसिन लर्निङ मोडेलहरूको प्रतिनिधित्व गर्न डिजाइन गरिएको खुला ढाँचा हो। यसले विस्तारयोग्य कम्प्युटेसन ग्राफ मोडेलको परिभाषा, साथै बिल्ट-इन अपरेटरहरू र मानक डाटा प्रकारहरूको परिभाषा प्रदान गर्दछ। ONNX ले विकासकर्ताहरूलाई विभिन्न ML फ्रेमवर्कहरू बीच मोडेलहरू सार्न अनुमति दिन्छ, जसले अन्तरसञ्चालन सक्षम बनाउँछ र AI एप्लिकेसनहरू सिर्जना र तैनाथ गर्न सजिलो बनाउँछ।

Phi3 mini CPU र GPU मा ONNX Runtime प्रयोग गरेर विभिन्न उपकरणहरूमा चल्न सक्छ, जसमा सर्भर प्लेटफर्महरू, Windows, Linux र Mac डेस्कटपहरू, र मोबाइल CPUs समावेश छन्। हामीले थपेका अनुकूलित कन्फिगरेसनहरू यसप्रकार छन्:

- int4 DML को लागि ONNX मोडेल: AWQ मार्फत int4 मा क्वान्टाइज गरिएको
- fp16 CUDA को लागि ONNX मोडेल
- int4 CUDA को लागि ONNX मोडेल: RTN मार्फत int4 मा क्वान्टाइज गरिएको
- int4 CPU र मोबाइलको लागि ONNX मोडेल: RTN मार्फत int4 मा क्वान्टाइज गरिएको

## Llama.cpp

Llama.cpp C++ मा लेखिएको खुला-स्रोत सफ्टवेयर लाइब्रेरी हो। यसले Llama सहित विभिन्न ठूला भाषा मोडेलहरू (LLMs) मा इन्फरेन्स गर्दछ। ggml लाइब्रेरी (एक सामान्य-उद्देश्यीय टेन्सर लाइब्रेरी) सँगै विकास गरिएको Llama.cpp ले मूल Python कार्यान्वयनको तुलनामा छिटो इन्फरेन्स र कम मेमोरी प्रयोग प्रदान गर्न लक्ष्य राख्छ। यसले हार्डवेयर अप्टिमाइजेशन, क्वान्टाइजेशनलाई समर्थन गर्छ, र सरल API र उदाहरणहरू उपलब्ध गराउँछ। यदि तपाईं कुशल LLM इन्फरेन्समा रुचि राख्नुहुन्छ भने, Llama.cpp अन्वेषण गर्न योग्य छ किनभने Phi3 Llama.cpp चलाउन सक्छ।

## GGUF

GGUF (Generic Graph Update Format) मेसिन लर्निङ मोडेलहरूको प्रतिनिधित्व गर्न र अपडेट गर्न प्रयोग हुने ढाँचा हो। यो विशेष गरी साना भाषा मोडेलहरू (SLMs) का लागि उपयोगी छ, जुन 4-8bit क्वान्टाइजेशनका साथ CPUs मा प्रभावकारी रूपमा चल्न सक्छ। GGUF तीव्र प्रोटोटाइपिङ र एज उपकरणहरूमा मोडेल चलाउन वा CI/CD पाइपलाइनहरू जस्ता ब्याच कामहरूमा उपयोगी छ।

**अस्वीकरण**:  
यो दस्तावेज मेसिन-आधारित एआई अनुवाद सेवाहरू प्रयोग गरी अनुवाद गरिएको हो। हामी शुद्धताका लागि प्रयास गर्दछौं, तर कृपया ध्यान दिनुहोस् कि स्वचालित अनुवादहरूमा त्रुटिहरू वा असत्यताहरू हुन सक्छन्। यसको मौलिक भाषामा रहेको मूल दस्तावेजलाई आधिकारिक स्रोत मानिनुपर्छ। महत्वपूर्ण जानकारीका लागि, पेशेवर मानव अनुवाद सिफारिस गरिन्छ। यस अनुवादको प्रयोगबाट उत्पन्न हुने कुनै पनि गलतफहमी वा गलत व्याख्याको लागि हामी जिम्मेवार हुनेछैनौं।