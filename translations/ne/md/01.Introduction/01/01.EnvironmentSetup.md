# Phi-3 स्थानीय रूपमा सुरु गर्नुहोस्

यो गाइडले तपाईंलाई Ollama प्रयोग गरेर Phi-3 मोडल चलाउनको लागि तपाईंको स्थानीय वातावरण सेटअप गर्न मद्दत गर्नेछ। तपाईं यो मोडल चलाउनका लागि विभिन्न तरिकाहरू प्रयोग गर्न सक्नुहुन्छ, जस्तै GitHub Codespaces, VS Code Dev Containers, वा तपाईंको स्थानीय वातावरण।

## वातावरण सेटअप

### GitHub Codespaces

तपाईं GitHub Codespaces प्रयोग गरेर यो टेम्प्लेटलाई भर्चुअल रूपमा चलाउन सक्नुहुन्छ। यो बटनले तपाईंको ब्राउजरमा वेब-आधारित VS Code इन्टेन्स खोल्नेछ:

1. टेम्प्लेट खोल्नुहोस् (यसमा केही मिनेट लाग्न सक्छ):

    [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/microsoft/phi-3cookbook)

2. टर्मिनल विन्डो खोल्नुहोस्

### VS Code Dev Containers

⚠️ यो विकल्प केवल तपाईंको Docker Desktop मा कम्तिमा 16 GB RAM छुट्याइएको भए मात्र काम गर्नेछ। यदि तपाईंको RAM 16 GB भन्दा कम छ भने, तपाईं [GitHub Codespaces विकल्प](../../../../../md/01.Introduction/01) प्रयास गर्न सक्नुहुन्छ वा [स्थानीय रूपमा सेटअप गर्नुहोस्](../../../../../md/01.Introduction/01)।

VS Code Dev Containers अर्को विकल्प हो, जसले तपाईंको स्थानीय VS Code मा [Dev Containers extension](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers) प्रयोग गरेर प्रोजेक्ट खोल्नेछ:

1. Docker Desktop सुरु गर्नुहोस् (पहिले नै इन्स्टल नभएको खण्डमा यसलाई इन्स्टल गर्नुहोस्)
2. प्रोजेक्ट खोल्नुहोस्:

    [![Open in Dev Containers](https://img.shields.io/static/v1?style=for-the-badge&label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/microsoft/phi-3cookbook)

3. जब VS Code को विन्डोमा प्रोजेक्ट फाइलहरू देखिन्छन् (यसमा केही मिनेट लाग्न सक्छ), टर्मिनल विन्डो खोल्नुहोस्।
4. [डिप्लोयमेन्ट चरणहरू](../../../../../md/01.Introduction/01) जारी राख्नुहोस्।

### स्थानीय वातावरण

1. निम्न उपकरणहरू इन्स्टल भएको सुनिश्चित गर्नुहोस्:

    * [Ollama](https://ollama.com/)
    * [Python 3.10+](https://www.python.org/downloads/)
    * [OpenAI Python SDK](https://pypi.org/project/openai/)

## मोडल परीक्षण गर्नुहोस्

1. Ollama लाई phi3:mini मोडल डाउनलोड र चलाउन भन्नुहोस्:

    ```shell
    ollama run phi3:mini
    ```

    यसले मोडल डाउनलोड गर्न केही मिनेट लाग्नेछ।

2. जब तपाईं आउटपुटमा "success" देख्नुहुन्छ, तपाईंले सो मोडललाई प्रम्प्टबाट सन्देश पठाउन सक्नुहुन्छ।

    ```shell
    >>> Write a haiku about hungry hippos
    ```

3. केही सेकेन्डपछि, तपाईंले मोडलबाट प्रतिक्रिया स्ट्रिममा आउँदै गरेको देख्नुहुनेछ।

4. भाषा मोडलहरूसँग प्रयोग गरिने विभिन्न प्रविधिहरू जान्न, Python नोटबुक [ollama.ipynb](../../../../../code/01.Introduce/ollama.ipynb) खोल्नुहोस् र प्रत्येक सेल चलाउनुहोस्। यदि तपाईंले 'phi3:mini' बाहेक कुनै मोडल प्रयोग गर्नुभएको छ भने, फाइलको माथिल्लो भागमा `MODEL_NAME` in the first cell.

5. To have a conversation with the phi3:mini model from Python, open the Python file [chat.py](../../../../../code/01.Introduce/chat.py) and run it. You can change the `MODEL_NAME` परिवर्तन गर्नुहोस्, र तपाईंले चाहनु भएमा सिस्टम सन्देश परिमार्जन गर्न वा केही उदाहरणहरू थप्न पनि सक्नुहुन्छ।

**अस्वीकरण**:  
यो दस्तावेज मेसिन-आधारित एआई अनुवाद सेवाहरू प्रयोग गरेर अनुवाद गरिएको छ। हामी शुद्धताका लागि प्रयास गर्छौं, तर कृपया ध्यान दिनुहोस् कि स्वचालित अनुवादहरूमा त्रुटिहरू वा अशुद्धताहरू हुन सक्छन्। यसको मूल भाषा भएको मूल दस्तावेजलाई आधिकारिक स्रोत मानिनुपर्छ। महत्वपूर्ण जानकारीका लागि, व्यावसायिक मानव अनुवाद सिफारिस गरिन्छ। यो अनुवादको प्रयोगबाट उत्पन्न हुने कुनै पनि गलतफहमी वा व्याख्याको लागि हामी उत्तरदायी हुने छैनौं।