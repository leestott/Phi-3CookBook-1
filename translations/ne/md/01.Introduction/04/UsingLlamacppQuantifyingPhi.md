# **llama.cpp प्रयोग गरेर Phi परिवारलाई क्वान्टाइज गर्ने**

## **llama.cpp के हो**

llama.cpp एउटा ओपन-सोर्स सफ्टवेयर लाइब्रेरी हो, जुन मुख्य रूपमा C++ मा लेखिएको छ। यसले Llama जस्ता विभिन्न ठूलो भाषा मोडेलहरू (LLMs) को इनफेरेन्स गर्न प्रयोग गरिन्छ। यसको मुख्य उद्देश्य थोरै सेटअपमै विविध हार्डवेयरमा अत्याधुनिक प्रदर्शन प्रदान गर्नु हो। साथै, यस लाइब्रेरीको लागि Python बाइन्डिङहरू पनि उपलब्ध छन्, जसले टेक्स्ट कम्प्लिशन र OpenAI सँग मिल्दो वेब सर्भरको लागि उच्च-स्तरीय API प्रदान गर्छ।

llama.cpp को मुख्य उद्देश्य भनेको न्यूनतम सेटअप र अत्याधुनिक प्रदर्शनको साथमा स्थानीय र क्लाउड दुवैमा LLM इनफेरेन्स सक्षम बनाउनु हो।

- कुनै पनि निर्भरता बिना साधारण C/C++ इम्प्लिमेन्टेसन
- Apple सिलिकनलाई प्राथमिकता - ARM NEON, Accelerate र Metal फ्रेमवर्कहरूद्वारा अनुकूलित
- x86 आर्किटेक्चरका लागि AVX, AVX2 र AVX512 समर्थन
- छिटो इनफेरेन्स र मेमोरी उपयोग घटाउन 1.5-बिट, 2-बिट, 3-बिट, 4-बिट, 5-बिट, 6-बिट, र 8-बिट इन्टिजर क्वान्टाइजेसन
- NVIDIA GPUs मा LLM चलाउनका लागि कस्टम CUDA कर्नेलहरू (AMD GPUs का लागि HIP मार्फत समर्थन)
- Vulkan र SYCL ब्याकएन्ड समर्थन
- CPU+GPU हाइब्रिड इनफेरेन्स जसले VRAM क्षमताभन्दा ठूला मोडेललाई आंशिक रूपमा छिटो बनाउँछ

## **Phi-3.5 लाई llama.cpp प्रयोग गरेर क्वान्टाइज गर्ने**

Phi-3.5-Instruct मोडेललाई llama.cpp प्रयोग गरेर क्वान्टाइज गर्न सकिन्छ, तर Phi-3.5-Vision र Phi-3.5-MoE हालसम्म समर्थन गरिएको छैन। llama.cpp ले रूपान्तरण गर्ने फर्म्याट GGUF हो, जुन अहिले सबैभन्दा बढी प्रयोगमा आउने क्वान्टाइजेसन फर्म्याट हो।

Hugging Face मा GGUF फर्म्याटका धेरै क्वान्टाइज गरिएका मोडेलहरू उपलब्ध छन्। AI Foundry, Ollama, र LlamaEdge ले llama.cpp मा निर्भर गर्छन्, त्यसैले GGUF मोडेलहरू पनि प्रायः प्रयोगमा आउँछन्।

### **GGUF के हो**

GGUF एउटा बाइनरी फर्म्याट हो, जुन मोडेलहरूलाई छिटो लोड र सेभ गर्न अनुकूलित गरिएको छ, जसले इनफेरेन्सका लागि धेरै प्रभावकारी बनाउँछ। GGUF GGML र अन्य एक्जिक्युटरहरूको लागि डिजाइन गरिएको हो। GGUF को विकास @ggerganov ले गरेका हुन्, जो llama.cpp को पनि विकासकर्ता हुन्, जुन लोकप्रिय C/C++ LLM इनफेरेन्स फ्रेमवर्क हो। PyTorch जस्ता फ्रेमवर्कहरूमा सुरुमा विकास गरिएका मोडेलहरू GGUF फर्म्याटमा रूपान्तरण गरेर ती इन्जिनहरूसँग प्रयोग गर्न सकिन्छ।

### **ONNX बनाम GGUF**

ONNX परम्परागत मेसिन लर्निङ/डीप लर्निङ फर्म्याट हो, जसलाई विभिन्न AI फ्रेमवर्कहरूले राम्रो समर्थन गर्छन् र यो एज डिभाइसहरूमा राम्रोसँग काम गर्छ। GGUF भने llama.cpp मा आधारित छ र यसलाई GenAI युगमा उत्पादन भएको भन्न सकिन्छ। यी दुईको प्रयोग मिल्दोजुल्दो छ। यदि तपाईंले एम्बेडेड हार्डवेयर र एप्लिकेसन तहमा राम्रो प्रदर्शन चाहनुहुन्छ भने, ONNX तपाईंको रोजाइ हुन सक्छ। यदि तपाईंले llama.cpp को डेरिभेटिभ फ्रेमवर्क र प्रविधि प्रयोग गर्नुभयो भने, GGUF राम्रो विकल्प हुन सक्छ।

### **Phi-3.5-Instruct लाई llama.cpp प्रयोग गरेर क्वान्टाइज गर्ने**

**1. वातावरण सेटअप**


```bash

git clone https://github.com/ggerganov/llama.cpp.git

cd llama.cpp

make -j8

```


**2. क्वान्टाइजेसन**

llama.cpp प्रयोग गरेर Phi-3.5-Instruct लाई FP16 GGUF मा रूपान्तरण गर्नुहोस्


```bash

./convert_hf_to_gguf.py <Your Phi-3.5-Instruct Location> --outfile phi-3.5-128k-mini_fp16.gguf

```

Phi-3.5 लाई INT4 मा क्वान्टाइज गर्नुहोस्


```bash

./llama.cpp/llama-quantize <Your phi-3.5-128k-mini_fp16.gguf location> ./gguf/phi-3.5-128k-mini_Q4_K_M.gguf Q4_K_M

```


**3. परीक्षण**

llama-cpp-python इन्स्टल गर्नुहोस्


```bash

pip install llama-cpp-python -U

```

***नोट*** 

यदि तपाईंले Apple सिलिकन प्रयोग गर्नुहुन्छ भने, कृपया यसरी llama-cpp-python इन्स्टल गर्नुहोस्


```bash

CMAKE_ARGS="-DLLAMA_METAL=on" pip install llama-cpp-python -U

```

परीक्षण गर्नुहोस् 


```bash

llama.cpp/llama-cli --model <Your phi-3.5-128k-mini_Q4_K_M.gguf location> --prompt "<|user|>\nCan you introduce .NET<|end|>\n<|assistant|>\n"  --gpu-layers 10

```



## **स्रोतहरू**

1. llama.cpp को बारेमा थप जान्नुस् [https://onnxruntime.ai/docs/genai/](https://onnxruntime.ai/docs/genai/)

2. GGUF को बारेमा थप जान्नुस् [https://huggingface.co/docs/hub/en/gguf](https://huggingface.co/docs/hub/en/gguf)

**अस्वीकरण**:  
यो दस्तावेज मेशिन-आधारित एआई अनुवाद सेवाहरू प्रयोग गरी अनुवाद गरिएको हो। हामी यथासम्भव शुद्धताको प्रयास गर्छौं, तर कृपया ध्यान दिनुहोस् कि स्वचालित अनुवादहरूमा त्रुटिहरू वा अशुद्धताहरू हुन सक्छन्। यसको मूल भाषामा रहेको मूल दस्तावेजलाई प्राधिकृत स्रोत मानिनुपर्छ। महत्त्वपूर्ण जानकारीका लागि, व्यावसायिक मानव अनुवाद सिफारिस गरिन्छ। यस अनुवादको प्रयोगबाट उत्पन्न हुने कुनै पनि गलतफहमी वा गलत व्याख्याका लागि हामी जिम्मेवार हुनेछैनौं।