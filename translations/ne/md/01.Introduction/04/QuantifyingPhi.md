# **फाइ परिवारको मात्रात्मककरण**

मोडेल क्वान्टाइजेशन भनेको न्यूरल नेटवर्क मोडेलका प्यारामिटरहरू (जस्तै वजन र सक्रियता मानहरू) लाई ठूलो मूल्य दायरा (सामान्यतया निरन्तर मूल्य दायरा) बाट सानो सीमित मूल्य दायरामा नक्सा गर्ने प्रक्रिया हो। यो प्रविधिले मोडेलको आकार र कम्प्युटेसनल जटिलता घटाउन सक्छ र मोबाइल उपकरणहरू वा एम्बेडेड प्रणालीजस्ता स्रोत-सीमित वातावरणमा मोडेलको सञ्चालन दक्षता सुधार गर्न सक्छ। मोडेल क्वान्टाइजेशनले प्यारामिटरहरूको शुद्धता घटाएर कम्प्रेसन हासिल गर्छ, तर यसले केही मात्रामा शुद्धताको ह्रास पनि ल्याउँछ। त्यसैले, क्वान्टाइजेशन प्रक्रियामा, मोडेलको आकार, कम्प्युटेसनल जटिलता, र शुद्धताको सन्तुलन कायम गर्नु आवश्यक हुन्छ। सामान्य क्वान्टाइजेशन विधिहरूमा फिक्स्ड-प्वाइन्ट क्वान्टाइजेशन, फ्लोटिङ-प्वाइन्ट क्वान्टाइजेशन आदि समावेश छन्। तपाईंले विशेष परिदृश्य र आवश्यकताहरू अनुसार उपयुक्त क्वान्टाइजेशन रणनीति चयन गर्न सक्नुहुन्छ।

हामी GenAI मोडेललाई एज डिभाइसहरूमा तैनात गर्न चाहन्छौं र मोबाइल उपकरणहरू, AI PC / Copilot+PC, र परम्परागत IoT उपकरणहरूजस्ता GenAI परिदृश्यहरूमा थप उपकरणहरूलाई समावेश गर्न चाहन्छौं। क्वान्टाइजेशन मोडेलमार्फत, हामी यसलाई विभिन्न उपकरणहरूको आधारमा विभिन्न एज डिभाइसहरूमा तैनात गर्न सक्छौं। हार्डवेयर निर्माताहरूले प्रदान गरेको मोडेल एक्सेलेरेशन फ्रेमवर्क र क्वान्टाइजेशन मोडेलको संयोजन गरेर, हामी अझ राम्रो SLM अनुप्रयोग परिदृश्यहरू निर्माण गर्न सक्छौं।

क्वान्टाइजेशन परिदृश्यमा, हामीसँग विभिन्न शुद्धताहरू (INT4, INT8, FP16, FP32) छन्। निम्नमा प्रायः प्रयोग गरिने क्वान्टाइजेशन शुद्धताहरूको व्याख्या छ:

### **INT4**

INT4 क्वान्टाइजेशन एक साहसी क्वान्टाइजेशन विधि हो, जसले मोडेलका वजन र सक्रियता मानहरूलाई 4-बिट पूर्णांकमा क्वान्टाइज गर्छ। सानो प्रतिनिधित्व दायरा र कम शुद्धताका कारण INT4 क्वान्टाइजेशनले सामान्यतया ठूलो मात्रामा शुद्धता ह्रास निम्त्याउँछ। तर, INT8 क्वान्टाइजेशनको तुलनामा, INT4 क्वान्टाइजेशनले मोडेलको भण्डारण आवश्यकताहरू र कम्प्युटेसनल जटिलतालाई अझै घटाउन सक्छ। ध्यान दिनुहोस् कि INT4 क्वान्टाइजेशन व्यावहारिक अनुप्रयोगहरूमा तुलनात्मक रूपमा दुर्लभ छ, किनभने अत्यन्त कम शुद्धताले मोडेलको प्रदर्शनमा महत्त्वपूर्ण गिरावट ल्याउन सक्छ। साथै, सबै हार्डवेयरले INT4 अपरेसनलाई समर्थन गर्दैन, त्यसैले क्वान्टाइजेशन विधि चयन गर्दा हार्डवेयर अनुकूलताका बारेमा विचार गर्न आवश्यक छ।

### **INT8**

INT8 क्वान्टाइजेशन भनेको मोडेलका वजन र सक्रियता मानहरूलाई फ्लोटिङ प्वाइन्ट नम्बरबाट 8-बिट पूर्णांकमा रूपान्तरण गर्ने प्रक्रिया हो। यद्यपि INT8 पूर्णांकले प्रतिनिधित्व गर्ने संख्याको दायरा सानो र कम शुद्धतामा हुन्छ, यसले भण्डारण र गणनाको आवश्यकताहरूलाई उल्लेखनीय रूपमा घटाउन सक्छ। INT8 क्वान्टाइजेशनमा, मोडेलका वजन र सक्रियता मानहरूलाई स्केलिङ र अफसेट प्रक्रियामार्फत क्वान्टाइज गरिन्छ, जसले मूल फ्लोटिङ प्वाइन्ट जानकारीलाई सकेसम्म जोगाउन प्रयास गर्दछ। इन्फरेन्सको क्रममा, यी क्वान्टाइज गरिएका मानहरूलाई गणनाका लागि फ्लोटिङ प्वाइन्ट नम्बरमा पुनः डीक्वान्टाइज गरिन्छ र त्यसपछि अर्को चरणका लागि पुनः INT8 मा क्वान्टाइज गरिन्छ। यो विधिले धेरैजसो अनुप्रयोगहरूमा पर्याप्त शुद्धता प्रदान गर्न सक्छ भने कम्प्युटेसनल दक्षता उच्च राख्छ।

### **FP16**

FP16 ढाँचा, जसलाई 16-बिट फ्लोटिङ प्वाइन्ट नम्बर (float16) पनि भनिन्छ, 32-बिट फ्लोटिङ प्वाइन्ट नम्बर (float32) को तुलनामा मेमोरी खपतलाई आधा घटाउँछ, जसले ठूलो-स्तरका डीप लर्निङ अनुप्रयोगहरूमा महत्त्वपूर्ण लाभ दिन्छ। FP16 ढाँचाले GPU मेमोरी सीमाभित्रै ठूलो मोडेलहरू लोड गर्न वा थप डेटा प्रशोधन गर्न अनुमति दिन्छ। आधुनिक GPU हार्डवेयरले FP16 अपरेसनहरूलाई निरन्तर समर्थन गरेसँगै, FP16 ढाँचाको प्रयोगले कम्प्युटिङ गति पनि सुधार गर्न सक्छ। तर, FP16 ढाँचाका आफ्नै कमीकमजोरीहरू पनि छन्, जस्तै कम शुद्धता, जसले केही अवस्थामा संख्यात्मक अस्थिरता वा शुद्धताको ह्रास निम्त्याउन सक्छ।

### **FP32**

FP32 ढाँचाले उच्च शुद्धता प्रदान गर्दछ र धेरै फराकिलो मूल्य दायरालाई सटीक रूपमा प्रतिनिधित्व गर्न सक्छ। जहाँ जटिल गणितीय अपरेसनहरू गरिन्छ वा उच्च शुद्धताका परिणामहरू आवश्यक हुन्छन्, त्यहाँ FP32 ढाँचालाई प्राथमिकता दिइन्छ। तर, उच्च शुद्धताले बढी मेमोरी प्रयोग र लामो गणना समय पनि माग गर्दछ। ठूलो-स्तरका डीप लर्निङ मोडेलहरूमा, विशेष गरी जब मोडेलका धेरै प्यारामिटरहरू र ठूलो मात्रामा डेटा हुन्छ, FP32 ढाँचाले GPU मेमोरीको अभाव वा इन्फरेन्स गतिको कमी निम्त्याउन सक्छ।

मोबाइल उपकरणहरू वा IoT उपकरणहरूमा, हामी Phi-3.x मोडेलहरूलाई INT4 मा रूपान्तरण गर्न सक्छौं, जबकि AI PC / Copilot PC ले INT8, FP16, FP32 जस्ता उच्च शुद्धता प्रयोग गर्न सक्छ।

हाल, विभिन्न हार्डवेयर निर्माताहरूले जेनरेटिभ मोडेलहरूलाई समर्थन गर्न विभिन्न फ्रेमवर्कहरू उपलब्ध गराएका छन्, जस्तै Intel को OpenVINO, Qualcomm को QNN, Apple को MLX, र Nvidia को CUDA। मोडेल क्वान्टाइजेशनको साथमा, हामी स्थानीय तैनाती पूरा गर्न सक्छौं।

प्रविधिको हिसाबले, क्वान्टाइजेशनपछि हामीसँग विभिन्न ढाँचाहरूको समर्थन छ, जस्तै PyTorch / Tensorflow ढाँचा, GGUF, र ONNX। मैले GGUF र ONNX बीच ढाँचा तुलना र अनुप्रयोग परिदृश्यहरूको विश्लेषण गरेको छु। यहाँ म ONNX क्वान्टाइजेशन ढाँचाको सिफारिस गर्दछु, जसले मोडेल फ्रेमवर्कदेखि हार्डवेयरसम्म राम्रो समर्थन प्रदान गर्दछ। यस अध्यायमा, हामी GenAI का लागि ONNX Runtime, OpenVINO, र Apple MLX मा मोडेल क्वान्टाइजेशन गर्नेमा केन्द्रित हुनेछौं (यदि तपाईंसँग राम्रो तरिका छ भने, हामीलाई PR सबमिट गरेर दिन सक्नुहुन्छ)।

**यस अध्यायमा समावेश छ:**

1. [Phi-3.5 / 4 लाई llama.cpp प्रयोग गरेर क्वान्टाइज गर्ने](./UsingLlamacppQuantifyingPhi.md)

2. [Phi-3.5 / 4 लाई Generative AI extensions for onnxruntime प्रयोग गरेर क्वान्टाइज गर्ने](./UsingORTGenAIQuantifyingPhi.md)

3. [Phi-3.5 / 4 लाई Intel OpenVINO प्रयोग गरेर क्वान्टाइज गर्ने](./UsingIntelOpenVINOQuantifyingPhi.md)

4. [Phi-3.5 / 4 लाई Apple MLX Framework प्रयोग गरेर क्वान्टाइज गर्ने](./UsingAppleMLXQuantifyingPhi.md)

**अस्वीकरण**:  
यो दस्तावेज मेसिन-आधारित एआई अनुवाद सेवाहरू प्रयोग गरेर अनुवाद गरिएको हो। हामी यथासम्भव सही अनुवाद प्रदान गर्न प्रयास गर्छौं, तर कृपया बुझ्नुहोस् कि स्वचालित अनुवादहरूमा त्रुटि वा अशुद्धि हुन सक्छ। मूल भाषामा रहेको मूल दस्तावेजलाई प्रामाणिक स्रोत मानिनुपर्छ। महत्त्वपूर्ण जानकारीका लागि, व्यावसायिक मानव अनुवाद सिफारिस गरिन्छ। यस अनुवादको प्रयोगबाट उत्पन्न हुने कुनै पनि गलतफहमी वा गलत व्याख्याका लागि हामी जिम्मेवार हुने छैनौं।