Nel contesto di Phi-3-mini, l'inferenza si riferisce al processo di utilizzo del modello per effettuare previsioni o generare output basati su dati di input. Permettimi di fornirti maggiori dettagli su Phi-3-mini e le sue capacità di inferenza.

Phi-3-mini fa parte della serie di modelli Phi-3 rilasciata da Microsoft. Questi modelli sono progettati per ridefinire ciò che è possibile fare con i Small Language Models (SLM).

Ecco alcuni punti chiave su Phi-3-mini e le sue capacità di inferenza:

## **Panoramica di Phi-3-mini:**
- Phi-3-mini ha una dimensione di parametri pari a 3,8 miliardi.
- Può essere eseguito non solo su dispositivi di calcolo tradizionali, ma anche su dispositivi edge come dispositivi mobili e dispositivi IoT.
- Il rilascio di Phi-3-mini consente a individui e aziende di implementare SLM su diversi dispositivi hardware, in particolare in ambienti con risorse limitate.
- Supporta vari formati di modello, tra cui il formato tradizionale PyTorch, la versione quantizzata del formato gguf e la versione quantizzata basata su ONNX.

## **Accesso a Phi-3-mini:**
Per accedere a Phi-3-mini, puoi utilizzare [Semantic Kernel](https://github.com/microsoft/SemanticKernelCookBook?WT.mc_id=aiml-138114-kinfeylo) in un'applicazione Copilot. Semantic Kernel è generalmente compatibile con Azure OpenAI Service, modelli open-source su Hugging Face e modelli locali.  
Puoi anche utilizzare [Ollama](https://ollama.com) o [LlamaEdge](https://llamaedge.com) per richiamare modelli quantizzati. Ollama consente agli utenti individuali di richiamare diversi modelli quantizzati, mentre LlamaEdge offre disponibilità multipiattaforma per i modelli GGUF.

## **Modelli quantizzati:**
Molti utenti preferiscono utilizzare modelli quantizzati per l'inferenza locale. Ad esempio, puoi eseguire direttamente Ollama run Phi-3 o configurarlo offline utilizzando un Modelfile. Il Modelfile specifica il percorso del file GGUF e il formato del prompt.

## **Possibilità dell'AI generativa:**
La combinazione di SLM come Phi-3-mini apre nuove possibilità per l'AI generativa. L'inferenza è solo il primo passo; questi modelli possono essere utilizzati per una varietà di compiti in scenari con risorse limitate, vincoli di latenza e costi ridotti.

## **Sbloccare l'AI generativa con Phi-3-mini: Una guida all'inferenza e al deployment**  
Scopri come utilizzare Semantic Kernel, Ollama/LlamaEdge e ONNX Runtime per accedere ai modelli Phi-3-mini e inferirli, ed esplora le possibilità dell'AI generativa in diversi scenari applicativi.

**Caratteristiche**  
Inferenza del modello Phi-3-mini in:

- [Semantic Kernel](https://github.com/Azure-Samples/Phi-3MiniSamples/tree/main/semantickernel?WT.mc_id=aiml-138114-kinfeylo)  
- [Ollama](https://github.com/Azure-Samples/Phi-3MiniSamples/tree/main/ollama?WT.mc_id=aiml-138114-kinfeylo)  
- [LlamaEdge WASM](https://github.com/Azure-Samples/Phi-3MiniSamples/tree/main/wasm?WT.mc_id=aiml-138114-kinfeylo)  
- [ONNX Runtime](https://github.com/Azure-Samples/Phi-3MiniSamples/tree/main/onnx?WT.mc_id=aiml-138114-kinfeylo)  
- [iOS](https://github.com/Azure-Samples/Phi-3MiniSamples/tree/main/ios?WT.mc_id=aiml-138114-kinfeylo)

In sintesi, Phi-3-mini consente agli sviluppatori di esplorare diversi formati di modello e sfruttare l'AI generativa in vari scenari applicativi.

**Disclaimer (Avvertenza):**  
Questo documento è stato tradotto utilizzando servizi di traduzione automatica basati sull'intelligenza artificiale. Sebbene ci impegniamo per garantire l'accuratezza, si prega di notare che le traduzioni automatiche potrebbero contenere errori o imprecisioni. Il documento originale nella sua lingua madre dovrebbe essere considerato la fonte autorevole. Per informazioni critiche, si raccomanda una traduzione professionale umana. Non siamo responsabili per eventuali fraintendimenti o interpretazioni errate derivanti dall'uso di questa traduzione.