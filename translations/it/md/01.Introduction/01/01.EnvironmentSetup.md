# Inizia con Phi-3 in locale

Questa guida ti aiuterà a configurare l'ambiente locale per eseguire il modello Phi-3 utilizzando Ollama. Puoi eseguire il modello in diversi modi, tra cui GitHub Codespaces, i Dev Containers di VS Code o il tuo ambiente locale.

## Configurazione dell'ambiente

### GitHub Codespaces

Puoi eseguire questo template virtualmente utilizzando GitHub Codespaces. Il pulsante aprirà un'istanza di VS Code basata sul web direttamente nel tuo browser:

1. Apri il template (potrebbero volerci alcuni minuti):

    [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/microsoft/phi-3cookbook)

2. Apri una finestra del terminale.

### Dev Containers di VS Code

⚠️ Questa opzione funzionerà solo se Docker Desktop ha almeno 16 GB di RAM allocati. Se hai meno di 16 GB di RAM, puoi provare l'opzione [GitHub Codespaces](../../../../../md/01.Introduction/01) o [configurarlo localmente](../../../../../md/01.Introduction/01).

Un'opzione correlata è quella dei Dev Containers di VS Code, che aprirà il progetto nel tuo VS Code locale utilizzando l'[estensione Dev Containers](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers):

1. Avvia Docker Desktop (installalo se non è già installato).
2. Apri il progetto:

    [![Open in Dev Containers](https://img.shields.io/static/v1?style=for-the-badge&label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/microsoft/phi-3cookbook)

3. Nella finestra di VS Code che si apre, una volta che i file del progetto sono visibili (potrebbero volerci alcuni minuti), apri una finestra del terminale.
4. Continua con i [passaggi di distribuzione](../../../../../md/01.Introduction/01).

### Ambiente Locale

1. Assicurati che i seguenti strumenti siano installati:

    * [Ollama](https://ollama.com/)
    * [Python 3.10+](https://www.python.org/downloads/)
    * [OpenAI Python SDK](https://pypi.org/project/openai/)

## Testa il modello

1. Chiedi a Ollama di scaricare ed eseguire il modello phi3:mini:

    ```shell
    ollama run phi3:mini
    ```

    Questo richiederà alcuni minuti per scaricare il modello.

2. Una volta che vedi "success" nell'output, puoi inviare un messaggio a quel modello dal prompt.

    ```shell
    >>> Write a haiku about hungry hippos
    ```

3. Dopo alcuni secondi, dovresti vedere una risposta arrivare in streaming dal modello.

4. Per imparare le diverse tecniche utilizzate con i modelli linguistici, apri il notebook Python [ollama.ipynb](../../../../../code/01.Introduce/ollama.ipynb) ed esegui ogni cella. Se hai utilizzato un modello diverso da 'phi3:mini', modifica il `MODEL_NAME` in the first cell.

5. To have a conversation with the phi3:mini model from Python, open the Python file [chat.py](../../../../../code/01.Introduce/chat.py) and run it. You can change the `MODEL_NAME` all'inizio del file secondo necessità. Puoi anche modificare il messaggio di sistema o aggiungere esempi few-shot se lo desideri.

**Disclaimer (Avvertenza)**:  
Questo documento è stato tradotto utilizzando servizi di traduzione automatica basati su intelligenza artificiale. Sebbene ci impegniamo per garantire l'accuratezza, si prega di tenere presente che le traduzioni automatiche possono contenere errori o imprecisioni. Il documento originale nella sua lingua nativa deve essere considerato la fonte autorevole. Per informazioni critiche, si raccomanda una traduzione professionale effettuata da un traduttore umano. Non siamo responsabili per eventuali fraintendimenti o interpretazioni errate derivanti dall'uso di questa traduzione.