# Le tecnologie chiave menzionate includono

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - un'API di basso livello per il machine learning accelerato dall'hardware, costruita sopra DirectX 12.
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - una piattaforma di calcolo parallelo e modello di interfaccia di programmazione delle applicazioni (API) sviluppato da Nvidia, che abilita l'elaborazione generale su unità di elaborazione grafica (GPU).
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - un formato aperto progettato per rappresentare modelli di machine learning che offre interoperabilità tra diversi framework ML.
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - un formato utilizzato per rappresentare e aggiornare modelli di machine learning, particolarmente utile per modelli linguistici più piccoli che possono funzionare efficacemente su CPU con quantizzazione a 4-8 bit.

## DirectML

DirectML è un'API di basso livello che consente il machine learning accelerato dall'hardware. È costruita sopra DirectX 12 per sfruttare l'accelerazione GPU ed è indipendente dal fornitore, il che significa che non richiede modifiche al codice per funzionare con diversi fornitori di GPU. È utilizzata principalmente per carichi di lavoro di addestramento e inferenza su GPU.

Per quanto riguarda il supporto hardware, DirectML è progettata per funzionare con una vasta gamma di GPU, inclusi GPU integrati e discreti AMD, GPU integrati Intel e GPU discreti NVIDIA. Fa parte della piattaforma Windows AI ed è supportata su Windows 10 e 11, consentendo l'addestramento e l'inferenza dei modelli su qualsiasi dispositivo Windows.

Ci sono stati aggiornamenti e opportunità legate a DirectML, come il supporto fino a 150 operatori ONNX ed essere utilizzata sia dal runtime ONNX che da WinML. È supportata dai principali fornitori di hardware integrato (IHV), ognuno dei quali implementa vari metacomandi.

## CUDA

CUDA, acronimo di Compute Unified Device Architecture, è una piattaforma di calcolo parallelo e modello di interfaccia di programmazione delle applicazioni (API) creata da Nvidia. Permette agli sviluppatori di software di utilizzare una GPU abilitata CUDA per l'elaborazione generale, un approccio definito GPGPU (General-Purpose computing on Graphics Processing Units). CUDA è un elemento chiave per l'accelerazione GPU di Nvidia ed è ampiamente utilizzata in vari settori, tra cui machine learning, calcolo scientifico ed elaborazione video.

Il supporto hardware per CUDA è specifico per le GPU Nvidia, in quanto si tratta di una tecnologia proprietaria sviluppata da Nvidia. Ogni architettura supporta versioni specifiche del toolkit CUDA, che fornisce le librerie e gli strumenti necessari per gli sviluppatori per costruire ed eseguire applicazioni CUDA.

## ONNX

ONNX (Open Neural Network Exchange) è un formato aperto progettato per rappresentare modelli di machine learning. Fornisce una definizione di un modello di grafo computazionale estensibile, nonché definizioni di operatori integrati e tipi di dati standard. ONNX consente agli sviluppatori di trasferire modelli tra diversi framework ML, abilitando l'interoperabilità e rendendo più semplice la creazione e la distribuzione di applicazioni AI.

Phi3 mini può funzionare con ONNX Runtime su CPU e GPU attraverso diversi dispositivi, inclusi piattaforme server, desktop Windows, Linux e Mac, e CPU mobili. Le configurazioni ottimizzate che abbiamo aggiunto sono:

- Modelli ONNX per int4 DML: quantizzati a int4 tramite AWQ
- Modello ONNX per fp16 CUDA
- Modello ONNX per int4 CUDA: quantizzato a int4 tramite RTN
- Modello ONNX per int4 CPU e Mobile: quantizzato a int4 tramite RTN

## Llama.cpp

Llama.cpp è una libreria software open-source scritta in C++. Esegue inferenze su vari modelli linguistici di grandi dimensioni (LLM), incluso Llama. Sviluppata insieme alla libreria ggml (una libreria di tensori generica), llama.cpp mira a fornire inferenze più rapide e un utilizzo della memoria ridotto rispetto all'implementazione originale in Python. Supporta l'ottimizzazione hardware, la quantizzazione e offre una semplice API con esempi. Se sei interessato a inferenze LLM efficienti, vale la pena esplorare llama.cpp, poiché Phi3 può eseguire Llama.cpp.

## GGUF

GGUF (Generic Graph Update Format) è un formato utilizzato per rappresentare e aggiornare modelli di machine learning. È particolarmente utile per modelli linguistici più piccoli (SLM) che possono funzionare efficacemente su CPU con quantizzazione a 4-8 bit. GGUF è vantaggioso per il rapido prototipazione e l'esecuzione di modelli su dispositivi edge o in lavori batch come pipeline CI/CD.

**Disclaimer (Avvertenza)**:  
Questo documento è stato tradotto utilizzando servizi di traduzione automatica basati sull'intelligenza artificiale. Sebbene ci impegniamo per garantire l'accuratezza, si prega di tenere presente che le traduzioni automatiche possono contenere errori o imprecisioni. Il documento originale nella sua lingua madre dovrebbe essere considerato la fonte autorevole. Per informazioni critiche, si raccomanda una traduzione professionale umana. Non siamo responsabili per eventuali fraintendimenti o interpretazioni errate derivanti dall'uso di questa traduzione.