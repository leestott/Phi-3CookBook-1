# **Chatta con Phi-4-mini ONNX**

***ONNX*** è un formato aperto creato per rappresentare modelli di machine learning. ONNX definisce un set comune di operatori - i mattoni fondamentali dei modelli di machine learning e deep learning - e un formato file comune per consentire agli sviluppatori di AI di utilizzare i modelli con una varietà di framework, strumenti, runtime e compilatori.

Speriamo di poter distribuire modelli di intelligenza artificiale generativa su dispositivi edge e utilizzarli in ambienti con potenza di calcolo limitata o offline. Ora possiamo raggiungere questo obiettivo convertendo il modello in modo quantizzato. Possiamo convertire il modello quantizzato nei formati GGUF o ONNX.

Microsoft Olive può aiutarti a convertire SLM nel formato ONNX quantizzato. Il metodo per effettuare la conversione del modello è molto semplice.

**Installa Microsoft Olive SDK**

```bash

pip install olive-ai

pip install transformers

```

**Converti per Supporto CPU ONNX**

```bash

olive auto-opt --model_name_or_path Your Phi-4-mini location --output_path Your onnx ouput location --device cpu --provider CPUExecutionProvider --precision int4 --use_model_builder --log_level 1

```

***Nota*** questo esempio utilizza la CPU.

### **Esegui inferenza con il modello Phi-4-mini ONNX usando ONNX Runtime GenAI**

- **Installa ONNX Runtime GenAI**

```bash

pip install --pre onnxruntime-genai

```

- **Codice Python**

*Questa è la versione 0.5.2 di ONNX Runtime GenAI*

```python

import onnxruntime_genai as og
import numpy as np
import os


model_folder = "Your Phi-4-mini-onnx-cpu-int4 location"


model = og.Model(model_folder)


tokenizer = og.Tokenizer(model)
tokenizer_stream = tokenizer.create_stream()


search_options = {}
search_options['max_length'] = 2048
search_options['past_present_share_buffer'] = False


chat_template = "<|user|>\n{input}</s>\n<|assistant|>"


text = """Can you introduce yourself"""


prompt = f'{chat_template.format(input=text)}'


input_tokens = tokenizer.encode(prompt)


params = og.GeneratorParams(model)


params.set_search_options(**search_options)
params.input_ids = input_tokens


generator = og.Generator(model, params)


while not generator.is_done():
      generator.compute_logits()
      generator.generate_next_token()

      new_token = generator.get_next_tokens()[0]
      print(tokenizer_stream.decode(new_token), end='', flush=True)

```

*Questa è la versione 0.6.0 di ONNX Runtime GenAI*

```python

import onnxruntime_genai as og
import numpy as np
import os
import time
import psutil

model_folder = "Your Phi-4-mini-onnx model path"

model = og.Model(model_folder)

tokenizer = og.Tokenizer(model)
tokenizer_stream = tokenizer.create_stream()

search_options = {}
search_options['max_length'] = 1024
search_options['past_present_share_buffer'] = False

chat_template = "<|user|>{input}<|assistant|>"

text = """can you introduce yourself"""

prompt = f'{chat_template.format(input=text)}'

input_tokens = tokenizer.encode(prompt)

params = og.GeneratorParams(model)

params.set_search_options(**search_options)

generator = og.Generator(model, params)

generator.append_tokens(input_tokens)

while not generator.is_done():
      generator.generate_next_token()

      new_token = generator.get_next_tokens()[0]
      token_text = tokenizer.decode(new_token)
      # print(tokenizer_stream.decode(new_token), end='', flush=True)
      if token_count == 0:
        first_token_time = time.time()
        first_response_latency = first_token_time - start_time
        print(f"firstly token delpay: {first_response_latency:.4f} s")

      print(token_text, end='', flush=True)
      token_count += 1

```

**Disclaimer (Avvertenza):**  
Questo documento è stato tradotto utilizzando servizi di traduzione automatizzati basati sull'intelligenza artificiale. Sebbene ci impegniamo per garantire l'accuratezza, si prega di notare che le traduzioni automatizzate potrebbero contenere errori o imprecisioni. Il documento originale nella sua lingua nativa dovrebbe essere considerato la fonte autorevole. Per informazioni critiche, si raccomanda una traduzione professionale eseguita da un traduttore umano. Non siamo responsabili per eventuali fraintendimenti o interpretazioni errate derivanti dall'uso di questa traduzione.