# Teknologi utama yang disebutkan meliputi

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - API tingkat rendah untuk pembelajaran mesin yang dipercepat perangkat keras, dibangun di atas DirectX 12.
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - platform komputasi paralel dan model antarmuka pemrograman aplikasi (API) yang dikembangkan oleh Nvidia, memungkinkan pemrosesan umum pada unit pemrosesan grafis (GPU).
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - format terbuka yang dirancang untuk merepresentasikan model pembelajaran mesin dan menyediakan interoperabilitas antar kerangka kerja ML yang berbeda.
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - format yang digunakan untuk merepresentasikan dan memperbarui model pembelajaran mesin, terutama berguna untuk model bahasa kecil yang dapat berjalan efektif di CPU dengan kuantisasi 4-8bit.

## DirectML

DirectML adalah API tingkat rendah yang memungkinkan pembelajaran mesin dipercepat perangkat keras. API ini dibangun di atas DirectX 12 untuk memanfaatkan akselerasi GPU dan bersifat vendor-agnostik, yang berarti tidak memerlukan perubahan kode untuk bekerja di berbagai vendor GPU. DirectML terutama digunakan untuk pelatihan model dan beban kerja inferensi pada GPU.

Dalam hal dukungan perangkat keras, DirectML dirancang untuk bekerja dengan berbagai GPU, termasuk GPU terintegrasi dan diskret AMD, GPU terintegrasi Intel, dan GPU diskret NVIDIA. DirectML adalah bagian dari Windows AI Platform dan didukung di Windows 10 & 11, memungkinkan pelatihan model dan inferensi pada perangkat Windows mana pun.

Ada pembaruan dan peluang terkait DirectML, seperti mendukung hingga 150 operator ONNX dan digunakan oleh runtime ONNX serta WinML. DirectML didukung oleh vendor perangkat keras terintegrasi utama (IHVs), yang masing-masing mengimplementasikan berbagai metakomando.

## CUDA

CUDA, singkatan dari Compute Unified Device Architecture, adalah platform komputasi paralel dan model antarmuka pemrograman aplikasi (API) yang dibuat oleh Nvidia. CUDA memungkinkan pengembang perangkat lunak menggunakan unit pemrosesan grafis (GPU) yang mendukung CUDA untuk pemrosesan tujuan umum â€“ pendekatan yang disebut GPGPU (General-Purpose computing on Graphics Processing Units). CUDA merupakan komponen kunci akselerasi GPU Nvidia dan banyak digunakan di berbagai bidang, termasuk pembelajaran mesin, komputasi ilmiah, dan pemrosesan video.

Dukungan perangkat keras untuk CUDA khusus untuk GPU Nvidia, karena ini adalah teknologi kepemilikan yang dikembangkan oleh Nvidia. Setiap arsitektur mendukung versi tertentu dari toolkit CUDA, yang menyediakan pustaka dan alat yang diperlukan bagi pengembang untuk membangun dan menjalankan aplikasi CUDA.

## ONNX

ONNX (Open Neural Network Exchange) adalah format terbuka yang dirancang untuk merepresentasikan model pembelajaran mesin. ONNX menyediakan definisi model grafik komputasi yang dapat diperluas, serta definisi operator bawaan dan tipe data standar. ONNX memungkinkan pengembang memindahkan model antar kerangka kerja ML yang berbeda, memberikan interoperabilitas dan mempermudah pembuatan serta penerapan aplikasi AI.

Phi3 mini dapat dijalankan dengan ONNX Runtime pada CPU dan GPU di berbagai perangkat, termasuk platform server, desktop Windows, Linux, dan Mac, serta CPU mobile. Konfigurasi yang telah dioptimalkan meliputi:

- Model ONNX untuk int4 DML: Dikuantisasi ke int4 melalui AWQ
- Model ONNX untuk fp16 CUDA
- Model ONNX untuk int4 CUDA: Dikuantisasi ke int4 melalui RTN
- Model ONNX untuk int4 CPU dan Mobile: Dikuantisasi ke int4 melalui RTN

## Llama.cpp

Llama.cpp adalah pustaka perangkat lunak sumber terbuka yang ditulis dalam C++. Pustaka ini menjalankan inferensi pada berbagai Model Bahasa Besar (LLM), termasuk Llama. Dikembangkan bersama pustaka ggml (pustaka tensor serbaguna), llama.cpp bertujuan untuk menyediakan inferensi yang lebih cepat dan penggunaan memori yang lebih rendah dibandingkan implementasi Python aslinya. Llama.cpp mendukung optimasi perangkat keras, kuantisasi, serta menawarkan API sederhana dan contoh penggunaan. Jika Anda tertarik dengan inferensi LLM yang efisien, llama.cpp patut dijelajahi karena Phi3 dapat menjalankan Llama.cpp.

## GGUF

GGUF (Generic Graph Update Format) adalah format yang digunakan untuk merepresentasikan dan memperbarui model pembelajaran mesin. Format ini sangat berguna untuk model bahasa kecil (SLM) yang dapat berjalan efektif di CPU dengan kuantisasi 4-8bit. GGUF bermanfaat untuk pembuatan prototipe cepat dan menjalankan model di perangkat edge atau dalam pekerjaan batch seperti pipeline CI/CD.

**Penafian**:  
Dokumen ini telah diterjemahkan menggunakan layanan terjemahan berbasis AI. Meskipun kami berusaha untuk memberikan hasil yang akurat, harap diperhatikan bahwa terjemahan otomatis dapat mengandung kesalahan atau ketidakakuratan. Dokumen asli dalam bahasa aslinya harus dianggap sebagai sumber yang berwenang. Untuk informasi yang bersifat kritis, disarankan menggunakan jasa terjemahan manusia profesional. Kami tidak bertanggung jawab atas kesalahpahaman atau penafsiran yang salah yang timbul dari penggunaan terjemahan ini.