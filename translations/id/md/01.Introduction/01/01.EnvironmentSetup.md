# Memulai dengan Phi-3 secara Lokal

Panduan ini akan membantu Anda mengatur lingkungan lokal untuk menjalankan model Phi-3 menggunakan Ollama. Anda dapat menjalankan model ini dengan beberapa cara, termasuk menggunakan GitHub Codespaces, VS Code Dev Containers, atau lingkungan lokal Anda.

## Pengaturan Lingkungan

### GitHub Codespaces

Anda dapat menjalankan template ini secara virtual dengan menggunakan GitHub Codespaces. Tombol berikut akan membuka instance VS Code berbasis web di browser Anda:

1. Buka template (ini mungkin memerlukan beberapa menit):

    [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/microsoft/phi-3cookbook)

2. Buka jendela terminal

### VS Code Dev Containers

⚠️ Opsi ini hanya akan berfungsi jika Docker Desktop Anda dialokasikan setidaknya 16 GB RAM. Jika Anda memiliki kurang dari 16 GB RAM, Anda dapat mencoba opsi [GitHub Codespaces](../../../../../md/01.Introduction/01) atau [mengaturnya secara lokal](../../../../../md/01.Introduction/01).

Opsi terkait adalah VS Code Dev Containers, yang akan membuka proyek di VS Code lokal Anda menggunakan [Dev Containers extension](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers):

1. Jalankan Docker Desktop (instal jika belum terinstal)
2. Buka proyek:

    [![Open in Dev Containers](https://img.shields.io/static/v1?style=for-the-badge&label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/microsoft/phi-3cookbook)

3. Di jendela VS Code yang terbuka, setelah file proyek muncul (ini mungkin memerlukan beberapa menit), buka jendela terminal.
4. Lanjutkan dengan [langkah-langkah penerapan](../../../../../md/01.Introduction/01)

### Lingkungan Lokal

1. Pastikan alat berikut sudah diinstal:

    * [Ollama](https://ollama.com/)
    * [Python 3.10+](https://www.python.org/downloads/)
    * [OpenAI Python SDK](https://pypi.org/project/openai/)

## Menguji Model

1. Minta Ollama untuk mengunduh dan menjalankan model phi3:mini:

    ```shell
    ollama run phi3:mini
    ```

    Proses ini akan memerlukan beberapa menit untuk mengunduh model.

2. Setelah Anda melihat "success" pada output, Anda dapat mengirim pesan ke model tersebut dari prompt.

    ```shell
    >>> Write a haiku about hungry hippos
    ```

3. Setelah beberapa detik, Anda akan melihat respons yang di-stream dari model.

4. Untuk mempelajari berbagai teknik yang digunakan dengan model bahasa, buka notebook Python [ollama.ipynb](../../../../../code/01.Introduce/ollama.ipynb) dan jalankan setiap sel. Jika Anda menggunakan model selain 'phi3:mini', ubah `MODEL_NAME` in the first cell.

5. To have a conversation with the phi3:mini model from Python, open the Python file [chat.py](../../../../../code/01.Introduce/chat.py) and run it. You can change the `MODEL_NAME` di bagian atas file sesuai kebutuhan, dan Anda juga dapat memodifikasi pesan sistem atau menambahkan contoh few-shot jika diinginkan.

**Penafian**:  
Dokumen ini telah diterjemahkan menggunakan layanan terjemahan berbasis AI. Meskipun kami berusaha untuk memberikan hasil yang akurat, harap disadari bahwa terjemahan otomatis dapat mengandung kesalahan atau ketidakakuratan. Dokumen asli dalam bahasa aslinya harus dianggap sebagai sumber yang berwenang. Untuk informasi yang bersifat kritis, disarankan menggunakan jasa terjemahan manusia profesional. Kami tidak bertanggung jawab atas kesalahpahaman atau interpretasi yang keliru yang timbul dari penggunaan terjemahan ini.