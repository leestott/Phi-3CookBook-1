# Viktiga teknologier som nämns inkluderar

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - ett lågnivå-API för hårdvaruaccelererad maskininlärning byggt ovanpå DirectX 12.
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - en plattform för parallellberäkning och applikationsprogrammeringsgränssnitt (API) utvecklad av Nvidia, som möjliggör allmän databehandling på grafikkort (GPUs).
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - ett öppet format designat för att representera maskininlärningsmodeller och ge interoperabilitet mellan olika ML-ramverk.
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - ett format som används för att representera och uppdatera maskininlärningsmodeller, särskilt användbart för mindre språkmodeller som kan köras effektivt på CPU:er med 4-8-bitars kvantisering.

## DirectML

DirectML är ett lågnivå-API som möjliggör hårdvaruaccelererad maskininlärning. Det är byggt ovanpå DirectX 12 för att utnyttja GPU-acceleration och är leverantörsneutralt, vilket innebär att det inte krävs kodändringar för att fungera med olika GPU-leverantörer. Det används främst för modellträning och inferensuppgifter på GPU:er.

När det gäller hårdvarustöd är DirectML designat för att fungera med ett brett utbud av GPU:er, inklusive AMD:s integrerade och diskreta GPU:er, Intels integrerade GPU:er och NVIDIAs diskreta GPU:er. Det är en del av Windows AI-plattformen och stöds på Windows 10 och 11, vilket möjliggör modellträning och inferens på alla Windows-enheter.

Det har gjorts uppdateringar och skett framsteg relaterade till DirectML, såsom stöd för upp till 150 ONNX-operatörer och användning av både ONNX Runtime och WinML. Det stöds av stora integrerade hårdvaruleverantörer (IHVs), som implementerar olika metakommandon.

## CUDA

CUDA, som står för Compute Unified Device Architecture, är en plattform för parallellberäkning och ett applikationsprogrammeringsgränssnitt (API) utvecklat av Nvidia. Det gör det möjligt för mjukvaruutvecklare att använda ett CUDA-kompatibelt grafikkort (GPU) för allmän databehandling – en metod som kallas GPGPU (General-Purpose computing on Graphics Processing Units). CUDA är en nyckelteknologi för Nvidias GPU-acceleration och används i stor utsträckning inom olika områden, inklusive maskininlärning, vetenskapliga beräkningar och videobearbetning.

Hårdvarustödet för CUDA är specifikt för Nvidias GPU:er, eftersom det är en proprietär teknologi utvecklad av Nvidia. Varje arkitektur stöder specifika versioner av CUDA-verktygssatsen, som tillhandahåller nödvändiga bibliotek och verktyg för utvecklare att bygga och köra CUDA-applikationer.

## ONNX

ONNX (Open Neural Network Exchange) är ett öppet format designat för att representera maskininlärningsmodeller. Det tillhandahåller en definition av en utbyggbar beräkningsgrafmodell samt definitioner av inbyggda operatorer och standarddatatyper. ONNX gör det möjligt för utvecklare att flytta modeller mellan olika ML-ramverk, vilket möjliggör interoperabilitet och gör det enklare att skapa och distribuera AI-applikationer.

Phi3 mini kan köras med ONNX Runtime på CPU och GPU över olika enheter, inklusive serverplattformar, Windows, Linux och Mac-datorer samt mobila CPU:er.
De optimerade konfigurationer vi har lagt till är:

- ONNX-modeller för int4 DML: Kvantiserade till int4 via AWQ
- ONNX-modell för fp16 CUDA
- ONNX-modell för int4 CUDA: Kvantiserade till int4 via RTN
- ONNX-modell för int4 CPU och mobiler: Kvantiserade till int4 via RTN

## Llama.cpp

Llama.cpp är ett öppen källkods-bibliotek skrivet i C++. Det utför inferens på olika stora språkmodeller (LLMs), inklusive Llama. Utvecklat tillsammans med ggml-biblioteket (ett allmänt tensorbibliotek) syftar llama.cpp till att erbjuda snabbare inferens och lägre minnesanvändning jämfört med den ursprungliga Python-implementationen. Det stöder hårdvaruoptimering, kvantisering och erbjuder ett enkelt API och exempel. Om du är intresserad av effektiv inferens för LLM är llama.cpp värt att utforska, då Phi3 kan köra Llama.cpp.

## GGUF

GGUF (Generic Graph Update Format) är ett format som används för att representera och uppdatera maskininlärningsmodeller. Det är särskilt användbart för mindre språkmodeller (SLMs) som kan köras effektivt på CPU:er med 4-8-bitars kvantisering. GGUF är fördelaktigt för snabb prototypframtagning och för att köra modeller på edge-enheter eller i batchjobb som CI/CD-pipelines.

**Ansvarsfriskrivning**:  
Detta dokument har översatts med hjälp av maskinbaserade AI-översättningstjänster. Även om vi strävar efter noggrannhet, vänligen notera att automatiserade översättningar kan innehålla fel eller felaktigheter. Det ursprungliga dokumentet på sitt originalspråk bör betraktas som den auktoritativa källan. För kritisk information rekommenderas professionell human översättning. Vi ansvarar inte för eventuella missförstånd eller feltolkningar som uppstår vid användning av denna översättning.