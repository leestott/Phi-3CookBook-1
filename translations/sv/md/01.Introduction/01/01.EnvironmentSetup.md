# Kom igång med Phi-3 lokalt

Den här guiden hjälper dig att konfigurera din lokala miljö för att köra Phi-3-modellen med Ollama. Du kan köra modellen på flera olika sätt, inklusive GitHub Codespaces, VS Code Dev Containers eller din lokala miljö.

## Miljöinställning

### GitHub Codespaces

Du kan köra den här mallen virtuellt genom att använda GitHub Codespaces. Knappen öppnar ett webbaserat VS Code-fönster i din webbläsare:

1. Öppna mallen (detta kan ta några minuter):

    [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/microsoft/phi-3cookbook)

2. Öppna ett terminalfönster

### VS Code Dev Containers

⚠️ Det här alternativet fungerar endast om ditt Docker Desktop har minst 16 GB RAM tilldelat. Om du har mindre än 16 GB RAM kan du prova [GitHub Codespaces-alternativet](../../../../../md/01.Introduction/01) eller [ställa in det lokalt](../../../../../md/01.Introduction/01).

Ett relaterat alternativ är VS Code Dev Containers, som öppnar projektet i din lokala VS Code med hjälp av [Dev Containers-tillägget](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers):

1. Starta Docker Desktop (installera det om det inte redan är installerat)
2. Öppna projektet:

    [![Open in Dev Containers](https://img.shields.io/static/v1?style=for-the-badge&label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/microsoft/phi-3cookbook)

3. I VS Code-fönstret som öppnas, när projektfilerna visas (detta kan ta några minuter), öppna ett terminalfönster.
4. Fortsätt med [distributionsstegen](../../../../../md/01.Introduction/01)

### Lokal miljö

1. Kontrollera att följande verktyg är installerade:

    * [Ollama](https://ollama.com/)
    * [Python 3.10+](https://www.python.org/downloads/)
    * [OpenAI Python SDK](https://pypi.org/project/openai/)

## Testa modellen

1. Be Ollama att ladda ner och köra phi3:mini-modellen:

    ```shell
    ollama run phi3:mini
    ```

    Det kommer att ta några minuter att ladda ner modellen.

2. När du ser "success" i utdata kan du skicka ett meddelande till modellen från prompten.

    ```shell
    >>> Write a haiku about hungry hippos
    ```

3. Efter några sekunder bör du se ett svar strömma in från modellen.

4. För att lära dig om olika tekniker som används med språkmodeller, öppna Python-notebooken [ollama.ipynb](../../../../../code/01.Introduce/ollama.ipynb) och kör varje cell. Om du använde en annan modell än 'phi3:mini', ändra `MODEL_NAME` in the first cell.

5. To have a conversation with the phi3:mini model from Python, open the Python file [chat.py](../../../../../code/01.Introduce/chat.py) and run it. You can change the `MODEL_NAME` högst upp i filen vid behov, och du kan även modifiera systemmeddelandet eller lägga till få-skott-exempel om så önskas.

**Ansvarsfriskrivning**:  
Detta dokument har översatts med hjälp av AI-baserade maskinöversättningstjänster. Även om vi strävar efter noggrannhet, vänligen notera att automatiserade översättningar kan innehålla fel eller brister. Det ursprungliga dokumentet på dess ursprungliga språk bör betraktas som den auktoritativa källan. För kritisk information rekommenderas professionell human översättning. Vi ansvarar inte för eventuella missförstånd eller feltolkningar som uppstår vid användning av denna översättning.