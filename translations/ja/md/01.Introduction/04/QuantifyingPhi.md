# **Phiファミリーの量子化**

モデルの量子化とは、ニューラルネットワークモデルのパラメータ（例えば重みや活性化値）を、大きな値の範囲（通常は連続値の範囲）から小さな有限の値の範囲にマッピングするプロセスを指します。この技術は、モデルのサイズと計算の複雑さを削減し、モバイルデバイスや組み込みシステムなどのリソースが制約された環境での動作効率を向上させることができます。モデルの量子化は、パラメータの精度を低下させることで圧縮を実現しますが、同時に一定の精度損失を伴います。そのため、量子化プロセスでは、モデルサイズ、計算の複雑さ、精度のバランスを取る必要があります。一般的な量子化手法には、固定小数点量子化、浮動小数点量子化などがあります。具体的なシナリオやニーズに応じて適切な量子化戦略を選択することが可能です。

私たちは、GenAIモデルをエッジデバイスに展開し、モバイルデバイス、AI PC / Copilot+PC、従来のIoTデバイスなど、より多くのデバイスがGenAIシナリオに参加できるようにしたいと考えています。量子化モデルを通じて、さまざまなデバイスに基づいて異なるエッジデバイスに展開することができます。ハードウェアメーカーが提供するモデル加速フレームワークや量子化モデルと組み合わせることで、より優れたSLMアプリケーションシナリオを構築することが可能です。

量子化のシナリオでは、さまざまな精度（INT4、INT8、FP16、FP32）があります。以下に、一般的な量子化精度について説明します。

### **INT4**

INT4量子化は、モデルの重みや活性化値を4ビットの整数に量子化するという、非常に大胆な量子化手法です。INT4量子化は、表現範囲が狭く精度が低いため、精度損失が大きくなる傾向があります。しかし、INT8量子化と比較して、INT4量子化はモデルのストレージ要件や計算の複雑さをさらに削減することができます。ただし、INT4量子化は実際のアプリケーションでは比較的稀であり、精度が低すぎるとモデルの性能が大幅に低下する可能性があります。また、すべてのハードウェアがINT4操作をサポートしているわけではないため、量子化手法を選択する際にはハードウェアの互換性を考慮する必要があります。

### **INT8**

INT8量子化は、モデルの重みや活性化値を浮動小数点数から8ビットの整数に変換するプロセスです。INT8整数が表す数値範囲は狭く精度も低いですが、ストレージと計算の要件を大幅に削減できます。INT8量子化では、モデルの重みや活性化値がスケーリングやオフセットを含む量子化プロセスを経て、元の浮動小数点情報をできるだけ保持します。推論中、これらの量子化された値は計算のために浮動小数点数にデ量子化され、次のステップのために再度INT8に量子化されます。この方法は、高い計算効率を維持しながら、ほとんどのアプリケーションで十分な精度を提供できます。

### **FP16**

FP16フォーマット、つまり16ビットの浮動小数点数（float16）は、32ビット浮動小数点数（float32）と比較してメモリ使用量を半分に削減し、大規模な深層学習アプリケーションで大きな利点があります。FP16フォーマットを使用することで、同じGPUメモリ制限内でより大きなモデルをロードしたり、より多くのデータを処理したりすることができます。また、現代のGPUハードウェアがFP16操作をサポートし続けているため、FP16フォーマットの使用は計算速度の向上をもたらす可能性があります。ただし、FP16フォーマットには固有の欠点もあり、精度が低いため、一部のケースでは数値の不安定性や精度の損失が発生する可能性があります。

### **FP32**

FP32フォーマットは高い精度を提供し、幅広い値を正確に表現できます。複雑な数学的操作が必要な場合や高精度な結果が求められるシナリオでは、FP32フォーマットが好まれます。ただし、高精度はより多くのメモリ使用量と計算時間を意味します。特にモデルパラメータが多く、データ量が膨大な大規模な深層学習モデルでは、FP32フォーマットがGPUメモリ不足や推論速度の低下を引き起こす可能性があります。

モバイルデバイスやIoTデバイスでは、Phi-3.xモデルをINT4に変換することが可能です。一方、AI PC / Copilot PCでは、INT8、FP16、FP32などの高精度を使用できます。

現在、異なるハードウェアメーカーは、IntelのOpenVINO、QualcommのQNN、AppleのMLX、NvidiaのCUDAなど、生成モデルをサポートするさまざまなフレームワークを提供しています。これらとモデル量子化を組み合わせることで、ローカルデプロイメントを完了することができます。

技術的には、量子化後にはさまざまなフォーマットがサポートされており、PyTorch / Tensorflowフォーマット、GGUF、ONNXなどがあります。GGUFとONNXのフォーマット比較とアプリケーションシナリオを行いましたが、ここではONNX量子化フォーマットを推奨します。ONNXは、モデルフレームワークからハードウェアまでのサポートが優れています。本章では、ONNX Runtime for GenAI、OpenVINO、Apple MLXを用いてモデル量子化を行う方法に焦点を当てます（もしより良い方法があれば、PRを提出して教えてください）。

**本章の内容**

1. [Phi-3.5 / 4をllama.cppで量子化する](./UsingLlamacppQuantifyingPhi.md)

2. [Phi-3.5 / 4をonnxruntimeのGenerative AI拡張機能で量子化する](./UsingORTGenAIQuantifyingPhi.md)

3. [Phi-3.5 / 4をIntel OpenVINOで量子化する](./UsingIntelOpenVINOQuantifyingPhi.md)

4. [Phi-3.5 / 4をApple MLX Frameworkで量子化する](./UsingAppleMLXQuantifyingPhi.md)

**免責事項**:  
この文書は、機械翻訳AIサービスを使用して翻訳されています。正確性を追求していますが、自動翻訳には誤りや不正確な部分が含まれる場合があります。元の言語で記載された原文が正式な情報源として優先されるべきです。重要な情報については、専門の人間による翻訳をお勧めします。この翻訳の使用により生じる誤解や誤認について、当社は一切の責任を負いません。