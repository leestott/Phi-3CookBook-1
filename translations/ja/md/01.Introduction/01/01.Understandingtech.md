# 記載されている主な技術

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - DirectX 12上に構築された、ハードウェアアクセラレーションによる機械学習のための低レベルAPI。
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - Nvidiaによって開発された並列コンピューティングプラットフォームおよびAPIモデルで、GPUでの汎用処理を可能にする。
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - 機械学習モデルの相互運用性を提供するために設計されたオープンフォーマット。
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - 特に4-8bit量子化でCPU上で効果的に動作する小型の言語モデルに役立つ、機械学習モデルの表現と更新に使用されるフォーマット。

## DirectML

DirectMLは、ハードウェアアクセラレーションによる機械学習を可能にする低レベルAPIです。DirectX 12上に構築されており、GPUアクセラレーションを活用しながら、特定のGPUベンダーに依存しない設計になっています。そのため、異なるGPUベンダー間でコード変更なしで動作します。主にモデルのトレーニングや推論ワークロードに使用されます。

ハードウェアのサポートに関しては、DirectMLは幅広いGPUで動作するように設計されています。これには、AMDの統合型およびディスクリートGPU、Intelの統合型GPU、NVIDIAのディスクリートGPUが含まれます。Windows AI Platformの一部であり、Windows 10および11でサポートされているため、あらゆるWindowsデバイスでモデルのトレーニングや推論が可能です。

DirectMLに関連する更新や機会として、150以上のONNXオペレーターのサポートや、ONNXランタイムおよびWinMLでの利用が挙げられます。また、主要な統合ハードウェアベンダー（IHV）によってバックアップされており、それぞれがさまざまなメタコマンドを実装しています。

## CUDA

CUDA（Compute Unified Device Architecture）は、Nvidiaによって作成された並列コンピューティングプラットフォームおよびAPIモデルです。CUDA対応のGPUを使用して汎用処理を実行できるようにするもので、このアプローチはGPGPU（Graphics Processing Units上での汎用計算）と呼ばれます。CUDAはNvidiaのGPUアクセラレーションを支える重要な技術であり、機械学習、科学計算、ビデオ処理などさまざまな分野で広く使用されています。

CUDAのハードウェアサポートはNvidiaのGPUに特化しており、Nvidiaによって開発された独自技術です。各アーキテクチャは、CUDAアプリケーションを構築および実行するために必要なライブラリやツールを提供するCUDAツールキットの特定バージョンをサポートしています。

## ONNX

ONNX（Open Neural Network Exchange）は、機械学習モデルを表現するために設計されたオープンフォーマットです。拡張可能な計算グラフモデルの定義、組み込みオペレーターの定義、標準データ型の定義を提供します。ONNXは、異なる機械学習フレームワーク間でモデルを移行可能にし、相互運用性を実現します。これにより、AIアプリケーションの作成やデプロイが容易になります。

Phi3 miniは、ONNXランタイムを使用してCPUおよびGPU上で動作し、サーバープラットフォーム、Windows、Linux、Macデスクトップ、モバイルCPUなど幅広いデバイスに対応しています。
最適化された設定として以下を追加しました：

- int4 DML用ONNXモデル：AWQを使用してint4に量子化
- fp16 CUDA用ONNXモデル
- int4 CUDA用ONNXモデル：RTNを使用してint4に量子化
- int4 CPUおよびモバイル用ONNXモデル：RTNを使用してint4に量子化

## Llama.cpp

Llama.cppは、C++で記述されたオープンソースのソフトウェアライブラリです。Llamaを含むさまざまな大規模言語モデル（LLM）の推論を実行します。汎用テンソルライブラリであるggmlライブラリとともに開発され、元のPython実装と比較して高速な推論と低メモリ使用量を目指しています。ハードウェア最適化や量子化をサポートし、シンプルなAPIとサンプルも提供しています。効率的なLLM推論に関心がある場合は、Phi3がLlama.cppを実行できるため、Llama.cppを試してみる価値があります。

## GGUF

GGUF（Generic Graph Update Format）は、機械学習モデルの表現と更新に使用されるフォーマットです。特に、4-8bit量子化でCPU上で効果的に動作する小型の言語モデル（SLM）に適しています。GGUFは、迅速なプロトタイピングや、エッジデバイスやCI/CDパイプラインのようなバッチジョブでのモデル実行に役立ちます。

**免責事項**:  
この文書は、機械翻訳AIサービスを使用して翻訳されています。正確性を追求しておりますが、自動翻訳には誤りや不正確さが含まれる場合があります。本来の言語で書かれた原文を、信頼できる正式な情報源としてお考えください。重要な情報については、専門の人間による翻訳をお勧めします。この翻訳の使用に起因する誤解や誤解釈について、当社は一切の責任を負いません。