# Bahsedilen Temel Teknolojiler

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - DirectX 12 üzerine inşa edilmiş, donanım hızlandırmalı makine öğrenimi için düşük seviyeli bir API.
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - Nvidia tarafından geliştirilen, grafik işlem birimlerinde (GPU) genel amaçlı işlemeyi mümkün kılan bir paralel hesaplama platformu ve uygulama programlama arayüzü (API) modeli.
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - Makine öğrenimi modellerini temsil etmek için tasarlanmış, farklı ML çerçeveleri arasında birlikte çalışabilirlik sağlayan açık bir format.
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - Özellikle 4-8 bit kuantizasyonla CPU'larda etkili bir şekilde çalışabilen daha küçük dil modelleri için kullanılan bir format.

## DirectML

DirectML, donanım hızlandırmalı makine öğrenimini mümkün kılan düşük seviyeli bir API'dir. GPU hızlandırmasını kullanmak için DirectX 12 üzerine inşa edilmiştir ve satıcı bağımsızdır; bu da farklı GPU satıcılarında çalışması için kod değişiklikleri gerektirmez. Genellikle GPU'larda model eğitimi ve çıkarım iş yükleri için kullanılır.

Donanım desteği açısından DirectML, AMD entegre ve ayrık GPU'lar, Intel entegre GPU'lar ve NVIDIA ayrık GPU'lar dahil olmak üzere geniş bir GPU yelpazesiyle çalışacak şekilde tasarlanmıştır. Windows AI Platformunun bir parçasıdır ve Windows 10 ve 11'de desteklenir, böylece herhangi bir Windows cihazında model eğitimi ve çıkarımı yapılabilir.

DirectML ile ilgili olarak, 150'ye kadar ONNX operatörünü desteklemek ve hem ONNX çalışma zamanı hem de WinML tarafından kullanılmak gibi güncellemeler ve fırsatlar olmuştur. Büyük Entegre Donanım Satıcıları (IHV'ler) tarafından desteklenmektedir ve her biri çeşitli metakomutları uygulamaktadır.

## CUDA

CUDA, Compute Unified Device Architecture'ın kısaltmasıdır ve Nvidia tarafından oluşturulmuş bir paralel hesaplama platformu ve uygulama programlama arayüzü (API) modelidir. Yazılım geliştiricilerin, genel amaçlı işlem (GPGPU - Graphics Processing Units üzerinde Genel Amaçlı Hesaplama) için CUDA uyumlu bir grafik işlem birimi (GPU) kullanmasına olanak tanır. CUDA, Nvidia'nın GPU hızlandırmasının temel bir unsuru olup, makine öğrenimi, bilimsel hesaplama ve video işleme gibi çeşitli alanlarda yaygın olarak kullanılmaktadır.

CUDA'nın donanım desteği, Nvidia'nın GPU'larına özgüdür, çünkü bu teknoloji Nvidia tarafından geliştirilmiş bir özel teknolojidir. Her mimari, CUDA uygulamaları oluşturmak ve çalıştırmak için gerekli kütüphaneleri ve araçları sağlayan belirli CUDA araç seti sürümlerini destekler.

## ONNX

ONNX (Open Neural Network Exchange), makine öğrenimi modellerini temsil etmek için tasarlanmış açık bir formattır. Genişletilebilir bir hesaplama grafiği modelinin tanımını, yerleşik operatörlerin ve standart veri türlerinin tanımlarını sağlar. ONNX, geliştiricilerin modelleri farklı ML çerçeveleri arasında taşımasına olanak tanır, böylece birlikte çalışabilirliği sağlar ve AI uygulamalarını oluşturmayı ve dağıtmayı kolaylaştırır.

Phi3 mini, ONNX Runtime ile CPU ve GPU üzerinde sunucu platformları, Windows, Linux ve Mac masaüstleri ve mobil CPU'lar dahil olmak üzere çeşitli cihazlarda çalışabilir. Eklediğimiz optimize edilmiş yapılandırmalar şunlardır:

- int4 DML için ONNX modelleri: AWQ ile int4'e kuantize edilmiş
- fp16 CUDA için ONNX modeli
- int4 CUDA için ONNX modeli: RTN ile int4'e kuantize edilmiş
- int4 CPU ve Mobil için ONNX modeli: RTN ile int4'e kuantize edilmiş

## Llama.cpp

Llama.cpp, C++ ile yazılmış açık kaynaklı bir yazılım kütüphanesidir. Llama dahil olmak üzere çeşitli Büyük Dil Modelleri (LLM'ler) üzerinde çıkarım yapar. ggml kütüphanesi (genel amaçlı bir tensör kütüphanesi) ile birlikte geliştirilmiştir. Llama.cpp, orijinal Python uygulamasına kıyasla daha hızlı çıkarım ve daha düşük bellek kullanımı sağlamayı amaçlar. Donanım optimizasyonunu, kuantizasyonu destekler ve basit bir API ile örnekler sunar. Eğer verimli LLM çıkarımıyla ilgileniyorsanız, Phi3'ün Llama.cpp çalıştırabildiği göz önünde bulundurulduğunda Llama.cpp keşfetmeye değer.

## GGUF

GGUF (Generic Graph Update Format), makine öğrenimi modellerini temsil etmek ve güncellemek için kullanılan bir formattır. Özellikle, 4-8 bit kuantizasyonla CPU'larda etkili bir şekilde çalışabilen daha küçük dil modelleri (SLM'ler) için faydalıdır. GGUF, hızlı prototipleme ve uç cihazlarda veya CI/CD gibi toplu işlerde modelleri çalıştırmak için avantaj sağlar.

**Feragatname**:  
Bu belge, yapay zeka tabanlı makine çeviri hizmetleri kullanılarak çevrilmiştir. Doğruluk için çaba göstersek de, otomatik çevirilerin hata veya yanlışlıklar içerebileceğini lütfen unutmayın. Orijinal belgenin kendi dilindeki hali yetkili kaynak olarak kabul edilmelidir. Kritik bilgiler için profesyonel insan çevirisi önerilir. Bu çevirinin kullanımından kaynaklanan herhangi bir yanlış anlama veya yanlış yorumlamadan sorumlu değiliz.