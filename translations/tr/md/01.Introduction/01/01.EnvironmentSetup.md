# Phi-3 ile Yerel Ortamda Başlayın

Bu rehber, Ollama kullanarak Phi-3 modelini çalıştırmak için yerel ortamınızı nasıl kuracağınızı gösterecek. Modeli çalıştırmanın birkaç farklı yolu vardır, örneğin GitHub Codespaces, VS Code Dev Containers veya yerel ortamınızı kullanabilirsiniz.

## Ortam Kurulumu

### GitHub Codespaces

Phi-3 şablonunu GitHub Codespaces kullanarak sanal olarak çalıştırabilirsiniz. Aşağıdaki buton, tarayıcınızda web tabanlı bir VS Code oturumu açacaktır:

1. Şablonu açın (bu işlem birkaç dakika sürebilir):

    [![GitHub Codespaces'te Aç](https://github.com/codespaces/badge.svg)](https://codespaces.new/microsoft/phi-3cookbook)

2. Bir terminal penceresi açın.

### VS Code Dev Containers

⚠️ Bu seçenek yalnızca Docker Desktop’ınıza en az 16 GB RAM ayrılmışsa çalışacaktır. 16 GB RAM’den az bir sisteminiz varsa, [GitHub Codespaces seçeneğini](../../../../../md/01.Introduction/01) deneyebilir veya [yerel ortamınızı kurabilirsiniz](../../../../../md/01.Introduction/01).

İlgili bir seçenek de VS Code Dev Containers’dır. Bu yöntem, projeyi [Dev Containers eklentisi](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers) kullanarak yerel VS Code’unuzda açar:

1. Docker Desktop'ı başlatın (henüz yüklü değilse yükleyin).
2. Projeyi açın:

    [![Dev Containers'ta Aç](https://img.shields.io/static/v1?style=for-the-badge&label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/microsoft/phi-3cookbook)

3. Açılan VS Code penceresinde, proje dosyaları göründüğünde (bu işlem birkaç dakika sürebilir), bir terminal penceresi açın.
4. [Dağıtım adımları](../../../../../md/01.Introduction/01) ile devam edin.

### Yerel Ortam

1. Aşağıdaki araçların kurulu olduğundan emin olun:

    * [Ollama](https://ollama.com/)
    * [Python 3.10+](https://www.python.org/downloads/)
    * [OpenAI Python SDK](https://pypi.org/project/openai/)

## Modeli Test Etme

1. Ollama’dan phi3:mini modelini indirmesini ve çalıştırmasını isteyin:

    ```shell
    ollama run phi3:mini
    ```

    Bu işlem modelin indirilmesi için birkaç dakika sürecektir.

2. Çıktıda "success" gördüğünüzde, modelden bir istem aracılığıyla mesaj gönderebilirsiniz.

    ```shell
    >>> Write a haiku about hungry hippos
    ```

3. Birkaç saniye içinde modelden bir yanıt akışı görmelisiniz.

4. Dil modelleriyle kullanılan farklı teknikleri öğrenmek için [ollama.ipynb](../../../../../code/01.Introduce/ollama.ipynb) adlı Python not defterini açın ve her bir hücreyi çalıştırın. 'phi3:mini' dışında bir model kullandıysanız, dosyanın en üstündeki `MODEL_NAME` in the first cell.

5. To have a conversation with the phi3:mini model from Python, open the Python file [chat.py](../../../../../code/01.Introduce/chat.py) and run it. You can change the `MODEL_NAME` kısmını gerektiği gibi değiştirin. Ayrıca, isterseniz sistem mesajını düzenleyebilir veya birkaç örnek ekleyebilirsiniz.

**Feragatname**:  
Bu belge, yapay zeka tabanlı makine çeviri hizmetleri kullanılarak çevrilmiştir. Doğruluk için çaba göstersek de, otomatik çevirilerin hata veya yanlışlıklar içerebileceğini lütfen unutmayın. Belgenin orijinal dilindeki hali, bağlayıcı kaynak olarak kabul edilmelidir. Kritik bilgiler için profesyonel bir insan çevirisi önerilir. Bu çevirinin kullanımından kaynaklanan yanlış anlamalar veya yanlış yorumlamalardan sorumlu değiliz.