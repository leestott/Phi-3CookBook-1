# উল্লেখযোগ্য প্রযুক্তি অন্তর্ভুক্ত করে

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - DirectX 12-এর উপর ভিত্তি করে তৈরি হার্ডওয়্যার-অ্যাক্সিলারেটেড মেশিন লার্নিংয়ের জন্য একটি লো-লেভেল API।
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - Nvidia দ্বারা তৈরি একটি প্যারালাল কম্পিউটিং প্ল্যাটফর্ম এবং অ্যাপ্লিকেশন প্রোগ্রামিং ইন্টারফেস (API) মডেল, যা গ্রাফিক্স প্রসেসিং ইউনিট (GPU)-তে জেনারেল-পারপাস প্রসেসিং সক্রিয় করে।
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - মেশিন লার্নিং মডেল উপস্থাপনের জন্য একটি ওপেন ফরম্যাট, যা বিভিন্ন ML ফ্রেমওয়ার্কের মধ্যে ইন্টারঅপারেবিলিটি প্রদান করে।
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - মেশিন লার্নিং মডেল উপস্থাপন এবং আপডেট করার জন্য ব্যবহৃত একটি ফরম্যাট, বিশেষত ছোট ভাষার মডেলের জন্য যা CPU-তে 4-8bit কোয়ান্টাইজেশনের সাথে কার্যকরভাবে চলতে পারে।

## DirectML

DirectML একটি লো-লেভেল API, যা হার্ডওয়্যার-অ্যাক্সিলারেটেড মেশিন লার্নিং সক্রিয় করে। এটি DirectX 12-এর উপর ভিত্তি করে তৈরি, যা GPU অ্যাক্সিলারেশন ব্যবহার করে এবং ভেন্ডর-অ্যাগনস্টিক, অর্থাৎ এটি বিভিন্ন GPU ভেন্ডরের মধ্যে কাজ করতে কোনো কোড পরিবর্তনের প্রয়োজন হয় না। এটি প্রধানত GPU-তে মডেল ট্রেনিং এবং ইনফারেন্সিং ওয়ার্কলোডের জন্য ব্যবহৃত হয়।

হার্ডওয়্যার সমর্থনের ক্ষেত্রে, DirectML বিভিন্ন ধরনের GPU-র সাথে কাজ করার জন্য ডিজাইন করা হয়েছে, যার মধ্যে রয়েছে AMD ইন্টিগ্রেটেড এবং ডিসক্রিট GPU, Intel ইন্টিগ্রেটেড GPU এবং NVIDIA ডিসক্রিট GPU। এটি Windows AI প্ল্যাটফর্মের অংশ এবং Windows 10 ও 11-এ সমর্থিত, যা যেকোনো Windows ডিভাইসে মডেল ট্রেনিং এবং ইনফারেন্সিং সম্ভব করে।

DirectML সম্পর্কিত আপডেট এবং সুযোগ রয়েছে, যেমন ১৫০টি ONNX অপারেটর সমর্থন করা এবং এটি ONNX runtime এবং WinML উভয়ের মাধ্যমেই ব্যবহৃত হয়। এটি প্রধান ইন্টিগ্রেটেড হার্ডওয়্যার ভেন্ডর (IHV)-দের দ্বারা সমর্থিত, যারা বিভিন্ন মেটাকমান্ড বাস্তবায়ন করে।

## CUDA

CUDA, যার পূর্ণরূপ Compute Unified Device Architecture, Nvidia দ্বারা তৈরি একটি প্যারালাল কম্পিউটিং প্ল্যাটফর্ম এবং অ্যাপ্লিকেশন প্রোগ্রামিং ইন্টারফেস (API) মডেল। এটি সফটওয়্যার ডেভেলপারদের CUDA-সক্ষম গ্রাফিক্স প্রসেসিং ইউনিট (GPU) ব্যবহার করে জেনারেল-পারপাস প্রসেসিং করার সুযোগ দেয় – যাকে GPGPU (General-Purpose computing on Graphics Processing Units) বলা হয়। CUDA Nvidia-র GPU অ্যাক্সিলারেশনের একটি মূল চালক এবং এটি মেশিন লার্নিং, বৈজ্ঞানিক গণনা এবং ভিডিও প্রসেসিং সহ বিভিন্ন ক্ষেত্রে ব্যাপকভাবে ব্যবহৃত হয়।

CUDA-এর হার্ডওয়্যার সমর্থন শুধুমাত্র Nvidia-র GPU-এর জন্য নির্দিষ্ট, কারণ এটি Nvidia দ্বারা উন্নত একটি প্রোপ্রাইটারি প্রযুক্তি। প্রতিটি আর্কিটেকচার CUDA টুলকিটের নির্দিষ্ট সংস্করণ সমর্থন করে, যা ডেভেলপারদের CUDA অ্যাপ্লিকেশন তৈরি এবং চালানোর জন্য প্রয়োজনীয় লাইব্রেরি এবং টুল সরবরাহ করে।

## ONNX

ONNX (Open Neural Network Exchange) একটি ওপেন ফরম্যাট, যা মেশিন লার্নিং মডেল উপস্থাপন করার জন্য ডিজাইন করা হয়েছে। এটি একটি এক্সটেনসিবল কম্পিউটেশন গ্রাফ মডেলের সংজ্ঞা, বিল্ট-ইন অপারেটর এবং স্ট্যান্ডার্ড ডেটা টাইপের সংজ্ঞা প্রদান করে। ONNX ডেভেলপারদের বিভিন্ন ML ফ্রেমওয়ার্কের মধ্যে মডেল স্থানান্তর করতে সক্ষম করে, যা ইন্টারঅপারেবিলিটি সক্রিয় করে এবং AI অ্যাপ্লিকেশন তৈরি ও স্থাপন সহজ করে তোলে।

Phi3 mini ONNX Runtime-এর মাধ্যমে CPU এবং GPU-তে মডেল চালাতে পারে, যার মধ্যে রয়েছে সার্ভার প্ল্যাটফর্ম, Windows, Linux এবং Mac ডেস্কটপ, এবং মোবাইল CPU। আমরা যে অপ্টিমাইজড কনফিগারেশনগুলো যোগ করেছি সেগুলো হলো:

- int4 DML-এর জন্য ONNX মডেল: AWQ-এর মাধ্যমে int4-এ কোয়ান্টাইজড
- fp16 CUDA-এর জন্য ONNX মডেল
- int4 CUDA-এর জন্য ONNX মডেল: RTN-এর মাধ্যমে int4-এ কোয়ান্টাইজড
- int4 CPU এবং মোবাইলের জন্য ONNX মডেল: RTN-এর মাধ্যমে int4-এ কোয়ান্টাইজড

## Llama.cpp

Llama.cpp একটি ওপেন-সোর্স সফটওয়্যার লাইব্রেরি, যা C++-এ লেখা হয়েছে। এটি Llama সহ বিভিন্ন Large Language Models (LLMs)-এ ইনফারেন্স সম্পাদন করে। ggml লাইব্রেরি (একটি জেনারেল-পারপাস টেনসর লাইব্রেরি)-এর সাথে বিকাশিত, Llama.cpp মূল Python ইমপ্লিমেন্টেশনের তুলনায় দ্রুত ইনফারেন্স এবং কম মেমোরি ব্যবহারের লক্ষ্য রাখে। এটি হার্ডওয়্যার অপ্টিমাইজেশন, কোয়ান্টাইজেশন সমর্থন করে এবং একটি সহজ API এবং উদাহরণ প্রদান করে। যদি আপনি দক্ষ LLM ইনফারেন্সে আগ্রহী হন, তবে Llama.cpp অন্বেষণ করা মূল্যবান, কারণ Phi3 Llama.cpp চালাতে পারে।

## GGUF

GGUF (Generic Graph Update Format) একটি ফরম্যাট, যা মেশিন লার্নিং মডেল উপস্থাপন এবং আপডেট করার জন্য ব্যবহৃত হয়। এটি বিশেষত ছোট ভাষার মডেলের (SLMs) জন্য উপযোগী, যা CPU-তে 4-8bit কোয়ান্টাইজেশনের সাথে কার্যকরভাবে চালানো যায়। GGUF দ্রুত প্রোটোটাইপিং এবং এজ ডিভাইস বা CI/CD পাইপলাইনের মতো ব্যাচ কাজের জন্য উপকারী।

**অস্বীকৃতি**:  
এই নথিটি মেশিন-ভিত্তিক এআই অনুবাদ পরিষেবাগুলির মাধ্যমে অনুবাদ করা হয়েছে। আমরা যথাসম্ভব সঠিক অনুবাদের চেষ্টা করি, তবে অনুগ্রহ করে মনে রাখবেন যে স্বয়ংক্রিয় অনুবাদে ত্রুটি বা অসঙ্গতি থাকতে পারে। নথিটির মূল ভাষায় রচিত সংস্করণটিকে প্রামাণিক উৎস হিসেবে বিবেচনা করা উচিত। গুরুত্বপূর্ণ তথ্যের জন্য পেশাদার মানব অনুবাদের পরামর্শ দেওয়া হচ্ছে। এই অনুবাদ ব্যবহারের ফলে কোনো ভুল বোঝাবুঝি বা ভুল ব্যাখ্যার জন্য আমরা দায়ী থাকব না।