# Phi-3 স্থানীয়ভাবে শুরু করুন

এই গাইডটি আপনাকে Ollama ব্যবহার করে Phi-3 মডেল চালানোর জন্য আপনার স্থানীয় পরিবেশ সেটআপ করতে সাহায্য করবে। আপনি বিভিন্ন উপায়ে মডেলটি চালাতে পারেন, যেমন GitHub Codespaces, VS Code Dev Containers, বা আপনার স্থানীয় পরিবেশে।

## পরিবেশ সেটআপ

### GitHub Codespaces

আপনি GitHub Codespaces ব্যবহার করে এই টেমপ্লেটটি ভার্চুয়ালি চালাতে পারেন। এই বোতামটি আপনার ব্রাউজারে একটি ওয়েব-ভিত্তিক VS Code ইনস্ট্যান্স খুলবে:

1. টেমপ্লেটটি খুলুন (এটি খুলতে কয়েক মিনিট সময় লাগতে পারে):

    [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/microsoft/phi-3cookbook)

2. একটি টার্মিনাল উইন্ডো খুলুন

### VS Code Dev Containers

⚠️ এই অপশনটি শুধুমাত্র কাজ করবে যদি আপনার Docker Desktop-এ কমপক্ষে ১৬ জিবি RAM বরাদ্দ থাকে। যদি আপনার কম RAM থাকে, তবে আপনি [GitHub Codespaces অপশন](../../../../../md/01.Introduction/01) চেষ্টা করতে পারেন অথবা [স্থানীয়ভাবে সেটআপ করুন](../../../../../md/01.Introduction/01)।

আরেকটি সম্পর্কিত অপশন হলো VS Code Dev Containers, যা প্রকল্পটি আপনার স্থানীয় VS Code-এ [Dev Containers এক্সটেনশন](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers) ব্যবহার করে খুলবে:

1. Docker Desktop চালু করুন (ইনস্টল করা না থাকলে এটি ইনস্টল করুন)
2. প্রকল্পটি খুলুন:

    [![Open in Dev Containers](https://img.shields.io/static/v1?style=for-the-badge&label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/microsoft/phi-3cookbook)

3. যখন প্রকল্প ফাইলগুলো VS Code উইন্ডোতে দেখাতে শুরু করবে (এটি কয়েক মিনিট সময় নিতে পারে), একটি টার্মিনাল উইন্ডো খুলুন।
4. [ডিপ্লয়মেন্ট ধাপগুলো](../../../../../md/01.Introduction/01) অনুসরণ করুন।

### স্থানীয় পরিবেশ

1. নিশ্চিত করুন যে নিচের টুলগুলো ইনস্টল করা আছে:

    * [Ollama](https://ollama.com/)
    * [Python 3.10+](https://www.python.org/downloads/)
    * [OpenAI Python SDK](https://pypi.org/project/openai/)

## মডেল পরীক্ষা করুন

1. Ollama-কে phi3:mini মডেলটি ডাউনলোড এবং চালাতে বলুন:

    ```shell
    ollama run phi3:mini
    ```

    এটি মডেলটি ডাউনলোড করতে কয়েক মিনিট সময় নেবে।

2. আউটপুটে "success" দেখার পর, আপনি প্রম্পট থেকে মডেলটিকে একটি বার্তা পাঠাতে পারবেন।

    ```shell
    >>> Write a haiku about hungry hippos
    ```

3. কয়েক সেকেন্ড পরে, আপনি মডেল থেকে একটি প্রতিক্রিয়া স্ট্রিম দেখতে পাবেন।

4. ভাষা মডেলের সাথে ব্যবহৃত বিভিন্ন কৌশল সম্পর্কে জানতে, Python নোটবুক [ollama.ipynb](../../../../../code/01.Introduce/ollama.ipynb) খুলুন এবং প্রতিটি সেল চালান। যদি আপনি 'phi3:mini' ছাড়া অন্য কোনো মডেল ব্যবহার করেন, তাহলে ফাইলের উপরের অংশে `MODEL_NAME` in the first cell.

5. To have a conversation with the phi3:mini model from Python, open the Python file [chat.py](../../../../../code/01.Introduce/chat.py) and run it. You can change the `MODEL_NAME` পরিবর্তন করুন এবং প্রয়োজনে সিস্টেম বার্তা বা কয়েকটি উদাহরণ যোগ করতে পারেন।

**অস্বীকৃতি**:  
এই নথিটি মেশিন-ভিত্তিক এআই অনুবাদ পরিষেবাগুলি ব্যবহার করে অনুবাদ করা হয়েছে। আমরা যথাসম্ভব সঠিক অনুবাদের জন্য চেষ্টা করি, তবে দয়া করে মনে রাখবেন যে স্বয়ংক্রিয় অনুবাদে ত্রুটি বা অসঙ্গতি থাকতে পারে। এর মূল ভাষায় থাকা নথিটিকেই প্রামাণিক উৎস হিসেবে বিবেচনা করা উচিত। গুরুত্বপূর্ণ তথ্যের জন্য পেশাদার মানব অনুবাদ সুপারিশ করা হয়। এই অনুবাদ ব্যবহার থেকে উদ্ভূত যে কোনো ভুল বোঝাবুঝি বা ভুল ব্যাখ্যার জন্য আমরা দায়ী থাকব না।