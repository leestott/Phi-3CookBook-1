# **llama.cpp ব্যবহার করে Phi Family কে কোয়ান্টাইজ করা**

## **llama.cpp কী**

llama.cpp একটি ওপেন-সোর্স সফটওয়্যার লাইব্রেরি, যা মূলত C++ ভাষায় লেখা। এটি বিভিন্ন বড় ভাষা মডেল (LLMs), যেমন Llama, এর উপর ইনফারেন্স সম্পাদন করে। এর মূল লক্ষ্য হলো বিভিন্ন হার্ডওয়্যারে সর্বোচ্চ কর্মক্ষমতা নিশ্চিত করা, এবং সেটআপ প্রক্রিয়া সহজ রাখা। এছাড়াও, এই লাইব্রেরির জন্য Python bindings পাওয়া যায়, যা একটি উচ্চ-স্তরের API এবং OpenAI এর সাথে সামঞ্জস্যপূর্ণ একটি ওয়েব সার্ভার প্রদান করে।

llama.cpp এর প্রধান লক্ষ্য হলো ন্যূনতম সেটআপ এবং সর্বোচ্চ কর্মক্ষমতার সাথে বিভিন্ন ধরনের হার্ডওয়্যারে LLM ইনফারেন্স সম্ভব করা - স্থানীয়ভাবে এবং ক্লাউডে।

- কোনো নির্ভরতা ছাড়াই সরল C/C++ ইমপ্লিমেন্টেশন
- Apple silicon এর জন্য প্রথম শ্রেণীর সমর্থন - ARM NEON, Accelerate এবং Metal frameworks এর মাধ্যমে অপ্টিমাইজড
- x86 আর্কিটেকচারের জন্য AVX, AVX2 এবং AVX512 সমর্থন
- দ্রুত ইনফারেন্স এবং কম মেমরি ব্যবহারের জন্য ১.৫-বিট, ২-বিট, ৩-বিট, ৪-বিট, ৫-বিট, ৬-বিট এবং ৮-বিট ইন্টিজার কোয়ান্টাইজেশন
- NVIDIA GPU-তে LLM চালানোর জন্য কাস্টম CUDA kernels (AMD GPU এর জন্য HIP এর মাধ্যমে সমর্থন)
- Vulkan এবং SYCL ব্যাকএন্ড সমর্থন
- CPU+GPU হাইব্রিড ইনফারেন্স, যা VRAM ক্ষমতার চেয়ে বড় মডেল আংশিকভাবে দ্রুততর করতে সক্ষম

## **llama.cpp দিয়ে Phi-3.5 কোয়ান্টাইজ করা**

Phi-3.5-Instruct মডেলটি llama.cpp ব্যবহার করে কোয়ান্টাইজ করা যেতে পারে, কিন্তু Phi-3.5-Vision এবং Phi-3.5-MoE এখনও সমর্থিত নয়। llama.cpp দ্বারা রূপান্তরিত ফরম্যাটটি হলো gguf, যা সবচেয়ে ব্যাপকভাবে ব্যবহৃত কোয়ান্টাইজেশন ফরম্যাট।

Hugging Face-এ প্রচুর সংখ্যক কোয়ান্টাইজড GGUF ফরম্যাট মডেল পাওয়া যায়। AI Foundry, Ollama, এবং LlamaEdge llama.cpp এর উপর নির্ভরশীল, তাই GGUF মডেলগুলিও প্রায়ই ব্যবহৃত হয়।

### **GGUF কী**

GGUF একটি বাইনারি ফরম্যাট, যা মডেল দ্রুত লোড এবং সেভ করার জন্য অপ্টিমাইজড। এটি ইনফারেন্সের জন্য অত্যন্ত কার্যকর। GGUF GGML এবং অন্যান্য এক্সিকিউটরের সাথে ব্যবহারের জন্য ডিজাইন করা হয়েছে। GGUF তৈরি করেছেন @ggerganov, যিনি llama.cpp এর ডেভেলপার। llama.cpp একটি জনপ্রিয় C/C++ LLM ইনফারেন্স ফ্রেমওয়ার্ক। PyTorch এর মতো ফ্রেমওয়ার্কে তৈরি মডেলগুলো GGUF ফরম্যাটে রূপান্তরিত করে এই ইঞ্জিনগুলোর সাথে ব্যবহার করা যায়।

### **ONNX বনাম GGUF**

ONNX একটি প্রচলিত মেশিন লার্নিং/ডিপ লার্নিং ফরম্যাট, যা বিভিন্ন AI Framework-এ ভালোভাবে সমর্থিত এবং এজ ডিভাইসে ভালো ব্যবহারিক সুবিধা প্রদান করে। অন্যদিকে, GGUF llama.cpp এর উপর ভিত্তি করে তৈরি এবং বলা যায় এটি GenAI যুগে উৎপন্ন হয়েছে। এদের ব্যবহার প্রায় একই। যদি আপনি এমবেডেড হার্ডওয়্যার এবং অ্যাপ্লিকেশন স্তরে ভালো কর্মক্ষমতা চান, তাহলে ONNX আপনার পছন্দ হতে পারে। আর যদি আপনি llama.cpp এর ডেরিভেটিভ ফ্রেমওয়ার্ক এবং প্রযুক্তি ব্যবহার করেন, তাহলে GGUF আপনার জন্য ভালো হতে পারে।

### **llama.cpp দিয়ে Phi-3.5-Instruct কোয়ান্টাইজ করা**

**১. পরিবেশ কনফিগারেশন**


```bash

git clone https://github.com/ggerganov/llama.cpp.git

cd llama.cpp

make -j8

```


**২. কোয়ান্টাইজেশন**

llama.cpp ব্যবহার করে Phi-3.5-Instruct কে FP16 GGUF এ রূপান্তর করা


```bash

./convert_hf_to_gguf.py <Your Phi-3.5-Instruct Location> --outfile phi-3.5-128k-mini_fp16.gguf

```

Phi-3.5 কে INT4 এ কোয়ান্টাইজ করা


```bash

./llama.cpp/llama-quantize <Your phi-3.5-128k-mini_fp16.gguf location> ./gguf/phi-3.5-128k-mini_Q4_K_M.gguf Q4_K_M

```


**৩. পরীক্ষা করা**

llama-cpp-python ইনস্টল করা


```bash

pip install llama-cpp-python -U

```

***নোট*** 

যদি আপনি Apple Silicon ব্যবহার করেন, তাহলে এইভাবে llama-cpp-python ইনস্টল করুন


```bash

CMAKE_ARGS="-DLLAMA_METAL=on" pip install llama-cpp-python -U

```

পরীক্ষা করা 


```bash

llama.cpp/llama-cli --model <Your phi-3.5-128k-mini_Q4_K_M.gguf location> --prompt "<|user|>\nCan you introduce .NET<|end|>\n<|assistant|>\n"  --gpu-layers 10

```



## **সম্পদসমূহ**

১. llama.cpp সম্পর্কে আরও জানুন [https://onnxruntime.ai/docs/genai/](https://onnxruntime.ai/docs/genai/)

২. GGUF সম্পর্কে আরও জানুন [https://huggingface.co/docs/hub/en/gguf](https://huggingface.co/docs/hub/en/gguf)

**অস্বীকৃতি**:  
এই নথিটি মেশিন-ভিত্তিক এআই অনুবাদ পরিষেবার মাধ্যমে অনুবাদ করা হয়েছে। আমরা যথাসাধ্য নির্ভুলতার জন্য চেষ্টা করি, তবে দয়া করে মনে রাখবেন যে স্বয়ংক্রিয় অনুবাদে ত্রুটি বা অসঙ্গতি থাকতে পারে। এর মূল ভাষায় থাকা নথিটিকেই প্রামাণিক উৎস হিসাবে বিবেচনা করা উচিত। গুরুত্বপূর্ণ তথ্যের জন্য পেশাদার মানব অনুবাদের পরামর্শ দেওয়া হয়। এই অনুবাদ ব্যবহারের ফলে উদ্ভূত কোনো ভুল বোঝাবুঝি বা ভুল ব্যাখ্যার জন্য আমরা দায়ী নই।