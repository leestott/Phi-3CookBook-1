# **ফাই ফ্যামিলি কোয়ান্টিফিকেশন**

মডেল কোয়ান্টাইজেশন বলতে নিউরাল নেটওয়ার্ক মডেলের প্যারামিটার (যেমন ওয়েট এবং অ্যাক্টিভেশন ভ্যালু) একটি বড় মানের পরিসীমা (সাধারণত একটি ক্রমাগত মানের পরিসীমা) থেকে একটি ছোট সীমিত মানের পরিসীমায় ম্যাপ করার প্রক্রিয়াকে বোঝায়। এই প্রযুক্তি মডেলের আকার এবং গণনার জটিলতা কমাতে পারে এবং মোবাইল ডিভাইস বা এমবেডেড সিস্টেমের মতো সীমিত রিসোর্সের পরিবেশে মডেলের কার্যকারিতা উন্নত করতে পারে। মডেল কোয়ান্টাইজেশন প্যারামিটারের প্রিসিশন কমিয়ে কম্প্রেশন অর্জন করে, তবে এটি নির্ভুলতার কিছু ক্ষতিও ঘটায়। তাই, কোয়ান্টাইজেশনের প্রক্রিয়ায় মডেলের আকার, গণনার জটিলতা এবং নির্ভুলতার মধ্যে ভারসাম্য বজায় রাখা প্রয়োজন। সাধারণ কোয়ান্টাইজেশন পদ্ধতিগুলির মধ্যে ফিক্সড-পয়েন্ট কোয়ান্টাইজেশন, ফ্লোটিং-পয়েন্ট কোয়ান্টাইজেশন ইত্যাদি অন্তর্ভুক্ত। নির্দিষ্ট পরিস্থিতি এবং প্রয়োজন অনুযায়ী আপনি উপযুক্ত কোয়ান্টাইজেশন কৌশল নির্বাচন করতে পারেন।

আমরা চাই জেনারেটিভ এআই মডেল এজ ডিভাইসে মোতায়েন করতে এবং আরও বেশি ডিভাইসকে জেনারেটিভ এআই পরিস্থিতিতে প্রবেশের সুযোগ দিতে, যেমন মোবাইল ডিভাইস, এআই পিসি/কোপাইলট+পিসি এবং প্রচলিত আইওটি ডিভাইস। কোয়ান্টাইজেশন মডেলের মাধ্যমে, আমরা বিভিন্ন ডিভাইসের ভিত্তিতে এটি বিভিন্ন এজ ডিভাইসে মোতায়েন করতে পারি। হার্ডওয়্যার নির্মাতাদের সরবরাহকৃত মডেল অ্যাক্সিলারেশন ফ্রেমওয়ার্ক এবং কোয়ান্টাইজেশন মডেলের সঙ্গে মিলিত হয়ে, আমরা আরও উন্নত এসএলএম অ্যাপ্লিকেশন পরিস্থিতি তৈরি করতে পারি।

কোয়ান্টাইজেশন পরিস্থিতিতে, আমাদের বিভিন্ন প্রিসিশন (INT4, INT8, FP16, FP32) রয়েছে। নিচে সাধারণত ব্যবহৃত কোয়ান্টাইজেশন প্রিসিশনের একটি ব্যাখ্যা দেওয়া হলো:

### **INT4**

INT4 কোয়ান্টাইজেশন একটি সাহসী কোয়ান্টাইজেশন পদ্ধতি, যা মডেলের ওয়েট এবং অ্যাক্টিভেশন ভ্যালুগুলোকে ৪-বিট পূর্ণসংখ্যায় কোয়ান্টাইজ করে। ছোট উপস্থাপনা পরিসীমা এবং নিম্ন প্রিসিশনের কারণে INT4 কোয়ান্টাইজেশন সাধারণত একটি বড় নির্ভুলতার ক্ষতি ঘটায়। তবে, INT8 কোয়ান্টাইজেশনের তুলনায়, INT4 কোয়ান্টাইজেশন মডেলের স্টোরেজ প্রয়োজনীয়তা এবং গণনার জটিলতা আরও কমিয়ে দিতে পারে। উল্লেখ্য, INT4 কোয়ান্টাইজেশন বাস্তব অ্যাপ্লিকেশনে তুলনামূলকভাবে বিরল, কারণ অত্যন্ত কম নির্ভুলতা মডেলের পারফরম্যান্সে উল্লেখযোগ্য অবনতি ঘটাতে পারে। তদ্ব্যতীত, সব হার্ডওয়্যার INT4 অপারেশন সমর্থন করে না, তাই কোয়ান্টাইজেশন পদ্ধতি বেছে নেওয়ার সময় হার্ডওয়্যার সামঞ্জস্য বিবেচনা করা প্রয়োজন।

### **INT8**

INT8 কোয়ান্টাইজেশন হল মডেলের ওয়েট এবং অ্যাক্টিভেশন ভ্যালুগুলোকে ফ্লোটিং পয়েন্ট সংখ্যাগুলো থেকে ৮-বিট পূর্ণসংখ্যায় রূপান্তর করার প্রক্রিয়া। যদিও INT8 পূর্ণসংখ্যাগুলোর উপস্থাপিত সংখ্যার পরিসীমা ছোট এবং কম নির্ভুল, এটি স্টোরেজ এবং গণনার প্রয়োজনীয়তাকে উল্লেখযোগ্যভাবে কমাতে পারে। INT8 কোয়ান্টাইজেশনে, মডেলের ওয়েট এবং অ্যাক্টিভেশন ভ্যালুগুলো একটি কোয়ান্টাইজেশন প্রক্রিয়ার মধ্য দিয়ে যায়, যার মধ্যে স্কেলিং এবং অফসেট অন্তর্ভুক্ত থাকে, যাতে মূল ফ্লোটিং পয়েন্ট তথ্য যতটা সম্ভব সংরক্ষণ করা যায়। ইনফারেন্স চলাকালীন, এই কোয়ান্টাইজড মানগুলোকে গণনার জন্য পুনরায় ফ্লোটিং পয়েন্ট সংখ্যায় ডিকোয়ান্টাইজ করা হবে এবং তারপরে পরবর্তী ধাপের জন্য পুনরায় INT8-এ কোয়ান্টাইজ করা হবে। এই পদ্ধতি বেশিরভাগ অ্যাপ্লিকেশনে যথেষ্ট নির্ভুলতা প্রদান করতে পারে এবং একই সঙ্গে উচ্চতর গণনাকাজের দক্ষতা বজায় রাখতে পারে।

### **FP16**

FP16 ফরম্যাট, অর্থাৎ ১৬-বিট ফ্লোটিং পয়েন্ট সংখ্যা (float16), ৩২-বিট ফ্লোটিং পয়েন্ট সংখ্যা (float32)-এর তুলনায় মেমোরি ব্যবহারের পরিমাণ অর্ধেক কমিয়ে দেয়, যা বৃহৎ পরিসরের ডিপ লার্নিং অ্যাপ্লিকেশনগুলিতে উল্লেখযোগ্য সুবিধা প্রদান করে। FP16 ফরম্যাট একই GPU মেমোরি সীমাবদ্ধতার মধ্যে বড় মডেল লোড করা বা আরও বেশি ডেটা প্রক্রিয়াকরণের সুযোগ দেয়। আধুনিক GPU হার্ডওয়্যার FP16 অপারেশন সমর্থন করতে থাকায়, FP16 ফরম্যাট ব্যবহারের ফলে গণনার গতিতেও উন্নতি ঘটতে পারে। তবে, FP16 ফরম্যাটের নিজস্ব কিছু সীমাবদ্ধতা রয়েছে, যেমন নিম্ন প্রিসিশন, যা কিছু ক্ষেত্রে গাণিতিক অস্থিরতা বা নির্ভুলতার ক্ষতি ঘটাতে পারে।

### **FP32**

FP32 ফরম্যাট উচ্চতর নির্ভুলতা প্রদান করে এবং একটি বিস্তৃত মানের পরিসীমা সঠিকভাবে উপস্থাপন করতে পারে। যেখানে জটিল গাণিতিক অপারেশন সম্পাদন করা হয় বা উচ্চ-নির্ভুলতার ফলাফল প্রয়োজন হয়, সেখানে FP32 ফরম্যাট পছন্দনীয়। তবে, উচ্চ নির্ভুলতা মানেই বেশি মেমোরি ব্যবহার এবং দীর্ঘতর গণনার সময়। বৃহৎ পরিসরের ডিপ লার্নিং মডেলগুলির জন্য, বিশেষ করে যখন মডেলের অনেক প্যারামিটার এবং বিশাল পরিমাণ ডেটা থাকে, FP32 ফরম্যাট GPU মেমোরির অভাব বা ইনফারেন্সের গতি হ্রাস ঘটাতে পারে।

মোবাইল ডিভাইস বা আইওটি ডিভাইসগুলিতে, আমরা ফাই-৩.x মডেলগুলোকে INT4-এ রূপান্তর করতে পারি, অন্যদিকে AI PC / কোপাইলট পিসিতে INT8, FP16, FP32-এর মতো উচ্চতর প্রিসিশন ব্যবহার করা যেতে পারে।

বর্তমানে, বিভিন্ন হার্ডওয়্যার নির্মাতাদের জেনারেটিভ মডেল সমর্থনের জন্য বিভিন্ন ফ্রেমওয়ার্ক রয়েছে, যেমন ইন্টেলের OpenVINO, কোয়ালকমের QNN, অ্যাপলের MLX এবং এনভিডিয়ার CUDA ইত্যাদি, যা মডেল কোয়ান্টাইজেশনের সাথে মিলিত হয়ে স্থানীয় মোতায়েন সম্পন্ন করে।

প্রযুক্তিগত দিক থেকে, কোয়ান্টাইজেশনের পরে আমাদের বিভিন্ন ফরম্যাট সমর্থন রয়েছে, যেমন PyTorch / TensorFlow ফরম্যাট, GGUF এবং ONNX। আমি GGUF এবং ONNX-এর মধ্যে একটি ফরম্যাট তুলনা এবং অ্যাপ্লিকেশন পরিস্থিতি বিশ্লেষণ করেছি। এখানে আমি ONNX কোয়ান্টাইজেশন ফরম্যাটের সুপারিশ করছি, যা মডেল ফ্রেমওয়ার্ক থেকে হার্ডওয়্যার পর্যন্ত ভালো সমর্থন প্রদান করে। এই অধ্যায়ে, আমরা GenAI-এর জন্য ONNX Runtime, OpenVINO এবং Apple MLX ব্যবহার করে মডেল কোয়ান্টাইজেশন করব (যদি আপনার আরও ভালো কোনো উপায় থাকে, তবে আপনি PR জমা দিয়ে আমাদের জানাতে পারেন)।

**এই অধ্যায়ে অন্তর্ভুক্ত**

1. [ফাই-৩.৫ / ৪ কোয়ান্টাইজ করা llama.cpp ব্যবহার করে](./UsingLlamacppQuantifyingPhi.md)

2. [ফাই-৩.৫ / ৪ কোয়ান্টাইজ করা Generative AI extensions for onnxruntime ব্যবহার করে](./UsingORTGenAIQuantifyingPhi.md)

3. [ফাই-৩.৫ / ৪ কোয়ান্টাইজ করা Intel OpenVINO ব্যবহার করে](./UsingIntelOpenVINOQuantifyingPhi.md)

4. [ফাই-৩.৫ / ৪ কোয়ান্টাইজ করা Apple MLX Framework ব্যবহার করে](./UsingAppleMLXQuantifyingPhi.md)

**অস্বীকৃতি**:  
এই নথি মেশিন-ভিত্তিক কৃত্রিম বুদ্ধিমত্তা অনুবাদ পরিষেবা ব্যবহার করে অনূদিত হয়েছে। আমরা যথাসাধ্য সঠিক অনুবাদের জন্য চেষ্টা করি, তবে অনুগ্রহ করে মনে রাখবেন যে স্বয়ংক্রিয় অনুবাদে ত্রুটি বা অসঙ্গতি থাকতে পারে। মূল ভাষায় থাকা নথিটিকেই প্রামাণিক উৎস হিসেবে বিবেচনা করা উচিত। গুরুত্বপূর্ণ তথ্যের জন্য পেশাদার মানব অনুবাদ সুপারিশ করা হয়। এই অনুবাদ ব্যবহার থেকে উদ্ভূত কোনো ভুল বোঝাবুঝি বা ব্যাখ্যার জন্য আমরা দায়ী নই।