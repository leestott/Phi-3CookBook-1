# Začněte s Phi-3 lokálně

Tento průvodce vám pomůže nastavit vaše lokální prostředí pro spuštění modelu Phi-3 pomocí Ollama. Model můžete spustit několika různými způsoby, včetně GitHub Codespaces, VS Code Dev Containers nebo vašeho lokálního prostředí.

## Nastavení prostředí

### GitHub Codespaces

Tuto šablonu můžete spustit virtuálně pomocí GitHub Codespaces. Tlačítko otevře webovou instanci VS Code ve vašem prohlížeči:

1. Otevřete šablonu (může to trvat několik minut):

    [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/microsoft/phi-3cookbook)

2. Otevřete okno terminálu.

### VS Code Dev Containers

⚠️ Tato možnost bude fungovat pouze v případě, že váš Docker Desktop má přiděleno alespoň 16 GB RAM. Pokud máte méně než 16 GB RAM, můžete zkusit možnost [GitHub Codespaces](../../../../../md/01.Introduction/01) nebo [nastavení lokálně](../../../../../md/01.Introduction/01).

Další možností jsou VS Code Dev Containers, které otevřou projekt ve vašem lokálním VS Code pomocí [rozšíření Dev Containers](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers):

1. Spusťte Docker Desktop (nainstalujte jej, pokud ještě není nainstalován).
2. Otevřete projekt:

    [![Open in Dev Containers](https://img.shields.io/static/v1?style=for-the-badge&label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/microsoft/phi-3cookbook)

3. V okně VS Code, které se otevře, jakmile se zobrazí soubory projektu (může to trvat několik minut), otevřete okno terminálu.
4. Pokračujte podle [kroků nasazení](../../../../../md/01.Introduction/01).

### Lokální prostředí

1. Ujistěte se, že máte nainstalovány následující nástroje:

    * [Ollama](https://ollama.com/)
    * [Python 3.10+](https://www.python.org/downloads/)
    * [OpenAI Python SDK](https://pypi.org/project/openai/)

## Otestujte model

1. Požádejte Ollama, aby stáhl a spustil model phi3:mini:

    ```shell
    ollama run phi3:mini
    ```

    To bude trvat několik minut, než se model stáhne.

2. Jakmile v výstupu uvidíte "success", můžete tomuto modelu poslat zprávu z příkazového řádku.

    ```shell
    >>> Write a haiku about hungry hippos
    ```

3. Po několika sekundách byste měli vidět proud odpovědí od modelu.

4. Pokud se chcete dozvědět o různých technikách používaných s jazykovými modely, otevřete Python notebook [ollama.ipynb](../../../../../code/01.Introduce/ollama.ipynb) a spusťte jednotlivé buňky. Pokud jste použili jiný model než 'phi3:mini', změňte `MODEL_NAME` in the first cell.

5. To have a conversation with the phi3:mini model from Python, open the Python file [chat.py](../../../../../code/01.Introduce/chat.py) and run it. You can change the `MODEL_NAME` na začátku souboru podle potřeby. Můžete také upravit systémovou zprávu nebo přidat příklady few-shot, pokud si přejete.

**Upozornění**:  
Tento dokument byl přeložen pomocí strojových AI překladatelských služeb. I když se snažíme o přesnost, mějte prosím na paměti, že automatické překlady mohou obsahovat chyby nebo nepřesnosti. Původní dokument v jeho původním jazyce by měl být považován za závazný zdroj. Pro důležité informace se doporučuje profesionální lidský překlad. Nenese žádnou odpovědnost za nedorozumění nebo nesprávné interpretace vyplývající z použití tohoto překladu.