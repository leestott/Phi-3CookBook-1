# Klíčové technologie zahrnují

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - nízkoúrovňové API pro hardwarově akcelerované strojové učení postavené na DirectX 12.
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - platforma pro paralelní výpočty a model aplikačního programovacího rozhraní (API) vyvinutý společností Nvidia, umožňující obecné zpracování na grafických procesorech (GPU).
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - otevřený formát navržený pro reprezentaci modelů strojového učení, který zajišťuje interoperabilitu mezi různými ML frameworky.
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - formát používaný k reprezentaci a aktualizaci modelů strojového učení, obzvláště užitečný pro menší jazykové modely, které mohou efektivně běžet na CPU s kvantizací na 4–8 bitů.

## DirectML

DirectML je nízkoúrovňové API, které umožňuje hardwarově akcelerované strojové učení. Je postaveno na DirectX 12, aby využilo akceleraci GPU, a je nezávislé na dodavateli, což znamená, že nevyžaduje změny kódu pro fungování na různých GPU. Primárně se používá pro trénování modelů a inferenci na GPU.

Co se týče hardwarové podpory, DirectML je navrženo tak, aby fungovalo s širokou škálou GPU, včetně integrovaných a diskrétních GPU od AMD, integrovaných GPU od Intelu a diskrétních GPU od Nvidie. Je součástí platformy Windows AI a je podporováno na Windows 10 a 11, což umožňuje trénování a inferenci modelů na jakémkoli zařízení s Windows.

Byly provedeny aktualizace a otevřeny příležitosti související s DirectML, jako je podpora až 150 ONNX operátorů a využití v ONNX runtime i WinML. Podporu zajišťují hlavní výrobci hardwaru (IHV), kteří implementují různé metapříkazy.

## CUDA

CUDA, což znamená Compute Unified Device Architecture, je platforma pro paralelní výpočty a model aplikačního programovacího rozhraní (API) vytvořený společností Nvidia. Umožňuje softwarovým vývojářům používat CUDA-kompatibilní grafický procesor (GPU) pro obecné zpracování – přístup známý jako GPGPU (General-Purpose computing on Graphics Processing Units). CUDA je klíčovým nástrojem pro akceleraci GPU od Nvidie a je široce využíváno v různých oblastech, včetně strojového učení, vědeckých výpočtů a zpracování videa.

Hardwarová podpora CUDA je specifická pro GPU od Nvidie, protože se jedná o proprietární technologii vyvinutou touto společností. Každá architektura podporuje specifické verze nástroje CUDA toolkit, který poskytuje potřebné knihovny a nástroje pro vývojáře k tvorbě a spouštění aplikací CUDA.

## ONNX

ONNX (Open Neural Network Exchange) je otevřený formát navržený pro reprezentaci modelů strojového učení. Poskytuje definici rozšiřitelného modelu výpočetního grafu, stejně jako definice vestavěných operátorů a standardních datových typů. ONNX umožňuje vývojářům přesouvat modely mezi různými ML frameworky, čímž zajišťuje interoperabilitu a usnadňuje tvorbu a nasazení AI aplikací.

Phi3 mini může běžet s ONNX Runtime na CPU i GPU napříč zařízeními, včetně serverových platforem, desktopů s Windows, Linuxem a Macem, a mobilních CPU. Optimalizované konfigurace, které jsme přidali, jsou:

- ONNX modely pro int4 DML: Kvantizováno na int4 pomocí AWQ
- ONNX model pro fp16 CUDA
- ONNX model pro int4 CUDA: Kvantizováno na int4 pomocí RTN
- ONNX model pro int4 CPU a mobilní zařízení: Kvantizováno na int4 pomocí RTN

## Llama.cpp

Llama.cpp je open-source softwarová knihovna napsaná v C++. Provádí inferenci na různých velkých jazykových modelech (LLM), včetně modelu Llama. Vyvíjena společně s knihovnou ggml (obecná knihovna pro práci s tensory), Llama.cpp si klade za cíl poskytovat rychlejší inferenci a nižší paměťové nároky ve srovnání s původní implementací v Pythonu. Podporuje hardwarovou optimalizaci, kvantizaci a nabízí jednoduché API a příklady. Pokud vás zajímá efektivní inference LLM, stojí Llama.cpp za prozkoumání, protože Phi3 může běžet s Llama.cpp.

## GGUF

GGUF (Generic Graph Update Format) je formát používaný k reprezentaci a aktualizaci modelů strojového učení. Je obzvláště užitečný pro menší jazykové modely (SLM), které mohou efektivně běžet na CPU s kvantizací na 4–8 bitů. GGUF je přínosný pro rychlé prototypování a běh modelů na edge zařízeních nebo v dávkových úlohách, jako jsou CI/CD pipeline.

**Upozornění**:  
Tento dokument byl přeložen pomocí strojových AI překladových služeb. Ačkoli se snažíme o přesnost, mějte prosím na paměti, že automatické překlady mohou obsahovat chyby nebo nepřesnosti. Původní dokument v jeho původním jazyce by měl být považován za autoritativní zdroj. Pro důležité informace se doporučuje profesionální lidský překlad. Nezodpovídáme za žádná nedorozumění nebo nesprávné interpretace vyplývající z použití tohoto překladu.