# Erste Schritte mit Phi-3 lokal

Diese Anleitung hilft dir, deine lokale Umgebung einzurichten, um das Phi-3-Modell mit Ollama auszuführen. Du kannst das Modell auf verschiedene Arten nutzen, einschließlich GitHub Codespaces, VS Code Dev Containers oder deiner lokalen Umgebung.

## Einrichtung der Umgebung

### GitHub Codespaces

Du kannst diese Vorlage virtuell mit GitHub Codespaces ausführen. Der Button öffnet eine webbasierte VS Code-Instanz in deinem Browser:

1. Öffne die Vorlage (dies kann einige Minuten dauern):

    [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/microsoft/phi-3cookbook)

2. Öffne ein Terminalfenster.

### VS Code Dev Containers

⚠️ Diese Option funktioniert nur, wenn deinem Docker Desktop mindestens 16 GB RAM zugewiesen sind. Falls du weniger als 16 GB RAM hast, kannst du die [GitHub Codespaces-Option](../../../../../md/01.Introduction/01) ausprobieren oder die [lokale Einrichtung](../../../../../md/01.Introduction/01) wählen.

Eine verwandte Option sind VS Code Dev Containers, die das Projekt in deinem lokalen VS Code mithilfe der [Dev Containers-Erweiterung](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers) öffnen:

1. Starte Docker Desktop (installiere es, falls es noch nicht installiert ist).
2. Öffne das Projekt:

    [![Open in Dev Containers](https://img.shields.io/static/v1?style=for-the-badge&label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/microsoft/phi-3cookbook)

3. Sobald die Projektdateien im geöffneten VS Code-Fenster angezeigt werden (dies kann einige Minuten dauern), öffne ein Terminalfenster.
4. Fahre mit den [Bereitstellungsschritten](../../../../../md/01.Introduction/01) fort.

### Lokale Umgebung

1. Stelle sicher, dass die folgenden Tools installiert sind:

    * [Ollama](https://ollama.com/)
    * [Python 3.10+](https://www.python.org/downloads/)
    * [OpenAI Python SDK](https://pypi.org/project/openai/)

## Test des Modells

1. Fordere Ollama auf, das Modell phi3:mini herunterzuladen und auszuführen:

    ```shell
    ollama run phi3:mini
    ```

    Das Herunterladen des Modells dauert einige Minuten.

2. Sobald im Output "success" angezeigt wird, kannst du über die Eingabeaufforderung eine Nachricht an das Modell senden.

    ```shell
    >>> Write a haiku about hungry hippos
    ```

3. Nach einigen Sekunden solltest du eine Antwort des Modells erhalten.

4. Um mehr über verschiedene Techniken im Umgang mit Sprachmodellen zu lernen, öffne das Python-Notebook [ollama.ipynb](../../../../../code/01.Introduce/ollama.ipynb) und führe jede Zelle aus. Falls du ein anderes Modell als 'phi3:mini' verwendet hast, passe den `MODEL_NAME` in the first cell.

5. To have a conversation with the phi3:mini model from Python, open the Python file [chat.py](../../../../../code/01.Introduce/chat.py) and run it. You can change the `MODEL_NAME` am Anfang der Datei entsprechend an. Du kannst auch die Systemnachricht ändern oder Beispiele für Few-Shot-Learning hinzufügen, falls gewünscht.

**Haftungsausschluss**:  
Dieses Dokument wurde mithilfe maschineller KI-Übersetzungsdienste übersetzt. Obwohl wir uns um Genauigkeit bemühen, weisen wir darauf hin, dass automatisierte Übersetzungen Fehler oder Ungenauigkeiten enthalten können. Das Originaldokument in seiner ursprünglichen Sprache sollte als maßgebliche Quelle betrachtet werden. Für kritische Informationen wird eine professionelle menschliche Übersetzung empfohlen. Wir übernehmen keine Haftung für Missverständnisse oder Fehlinterpretationen, die aus der Nutzung dieser Übersetzung entstehen.