# Schlüsseltechnologien umfassen

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) – eine Low-Level-API für hardwarebeschleunigtes maschinelles Lernen, die auf DirectX 12 aufbaut.
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) – eine Plattform für paralleles Rechnen und ein API-Modell, entwickelt von Nvidia, das allgemeine Berechnungen auf Grafikprozessoren (GPUs) ermöglicht.
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) – ein offenes Format zur Darstellung von Modellen des maschinellen Lernens, das Interoperabilität zwischen verschiedenen ML-Frameworks ermöglicht.
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) – ein Format zur Darstellung und Aktualisierung von Modellen des maschinellen Lernens, besonders nützlich für kleinere Sprachmodelle, die effektiv auf CPUs mit 4-8-Bit-Quantisierung laufen können.

## DirectML

DirectML ist eine Low-Level-API, die hardwarebeschleunigtes maschinelles Lernen ermöglicht. Sie basiert auf DirectX 12, um GPU-Beschleunigung zu nutzen, und ist herstellerunabhängig, was bedeutet, dass keine Codeänderungen erforderlich sind, um auf verschiedenen GPU-Anbietern zu arbeiten. Sie wird hauptsächlich für Modelltraining und Inferenz-Workloads auf GPUs verwendet.

Hinsichtlich der Hardwareunterstützung ist DirectML so konzipiert, dass es mit einer Vielzahl von GPUs funktioniert, einschließlich AMD-integrierter und diskreter GPUs, Intel-integrierter GPUs und NVIDIA-diskreter GPUs. Es ist Teil der Windows AI-Plattform und wird unter Windows 10 und 11 unterstützt, wodurch Modelltraining und Inferenz auf jedem Windows-Gerät möglich sind.

Es gab Updates und Chancen im Zusammenhang mit DirectML, wie die Unterstützung von bis zu 150 ONNX-Operatoren und die Nutzung sowohl durch die ONNX-Runtime als auch durch WinML. Es wird von großen Hardwareherstellern (IHVs) unterstützt, die jeweils verschiedene Metakommandos implementieren.

## CUDA

CUDA, was für Compute Unified Device Architecture steht, ist eine Plattform für paralleles Rechnen und ein API-Modell, das von Nvidia entwickelt wurde. Es ermöglicht Softwareentwicklern, eine CUDA-fähige GPU für allgemeine Berechnungen zu nutzen – ein Ansatz, der als GPGPU (General-Purpose Computing on Graphics Processing Units) bezeichnet wird. CUDA ist ein Schlüsselbestandteil von Nvidias GPU-Beschleunigung und wird in verschiedenen Bereichen wie maschinellem Lernen, wissenschaftlichem Rechnen und Videobearbeitung weit verbreitet eingesetzt.

Die Hardwareunterstützung für CUDA ist spezifisch für Nvidias GPUs, da es sich um eine proprietäre Technologie handelt, die von Nvidia entwickelt wurde. Jede Architektur unterstützt spezifische Versionen des CUDA-Toolkits, das die notwendigen Bibliotheken und Werkzeuge für Entwickler bereitstellt, um CUDA-Anwendungen zu erstellen und auszuführen.

## ONNX

ONNX (Open Neural Network Exchange) ist ein offenes Format zur Darstellung von Modellen des maschinellen Lernens. Es bietet eine Definition eines erweiterbaren Rechengraphenmodells sowie Definitionen von eingebauten Operatoren und Standarddatentypen. ONNX ermöglicht es Entwicklern, Modelle zwischen verschiedenen ML-Frameworks zu verschieben, was Interoperabilität ermöglicht und die Erstellung und Bereitstellung von KI-Anwendungen erleichtert.

Phi3 mini kann mit der ONNX-Runtime auf CPU und GPU über verschiedene Geräte hinweg laufen, einschließlich Serverplattformen, Windows-, Linux- und Mac-Desktops sowie mobilen CPUs. Die optimierten Konfigurationen, die wir hinzugefügt haben, sind:

- ONNX-Modelle für int4 DML: Quantisiert auf int4 über AWQ
- ONNX-Modell für fp16 CUDA
- ONNX-Modell für int4 CUDA: Quantisiert auf int4 über RTN
- ONNX-Modell für int4 CPU und Mobile: Quantisiert auf int4 über RTN

## Llama.cpp

Llama.cpp ist eine Open-Source-Softwarebibliothek, die in C++ geschrieben ist. Sie führt Inferenz auf verschiedenen großen Sprachmodellen (LLMs) durch, einschließlich Llama. Entwickelt zusammen mit der ggml-Bibliothek (einer universellen Tensorbibliothek), zielt llama.cpp darauf ab, schnellere Inferenz und einen geringeren Speicherverbrauch im Vergleich zur ursprünglichen Python-Implementierung zu bieten. Es unterstützt Hardwareoptimierung, Quantisierung und bietet eine einfache API sowie Beispiele. Wenn Sie an effizienter LLM-Inferenz interessiert sind, lohnt es sich, llama.cpp zu erkunden, da Phi3 Llama.cpp ausführen kann.

## GGUF

GGUF (Generic Graph Update Format) ist ein Format zur Darstellung und Aktualisierung von Modellen des maschinellen Lernens. Es ist besonders nützlich für kleinere Sprachmodelle (SLMs), die effektiv auf CPUs mit 4-8-Bit-Quantisierung laufen können. GGUF ist vorteilhaft für schnelles Prototyping und das Ausführen von Modellen auf Edge-Geräten oder in Batch-Jobs wie CI/CD-Pipelines.

**Haftungsausschluss**:  
Dieses Dokument wurde mit KI-gestützten maschinellen Übersetzungsdiensten übersetzt. Obwohl wir uns um Genauigkeit bemühen, weisen wir darauf hin, dass automatisierte Übersetzungen Fehler oder Ungenauigkeiten enthalten können. Das Originaldokument in seiner ursprünglichen Sprache sollte als maßgebliche Quelle betrachtet werden. Für kritische Informationen wird eine professionelle menschliche Übersetzung empfohlen. Wir übernehmen keine Haftung für Missverständnisse oder Fehlinterpretationen, die sich aus der Nutzung dieser Übersetzung ergeben.