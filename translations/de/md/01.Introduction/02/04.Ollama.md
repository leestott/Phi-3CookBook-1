## Phi-Familie in Ollama

[Ollama](https://ollama.com) ermöglicht es mehr Menschen, Open-Source-LLM oder SLM direkt über einfache Skripte einzusetzen. Außerdem können APIs erstellt werden, um lokale Copilot-Anwendungsszenarien zu unterstützen.

## **1. Installation**

Ollama unterstützt Windows, macOS und Linux. Du kannst Ollama über diesen Link installieren ([https://ollama.com/download](https://ollama.com/download)). Nach erfolgreicher Installation kannst du das Ollama-Skript direkt verwenden, um Phi-3 über ein Terminalfenster aufzurufen. Eine Übersicht über alle [verfügbaren Bibliotheken in Ollama](https://ollama.com/library) findest du hier. Wenn du dieses Repository in einem Codespace öffnest, ist Ollama bereits installiert.

```bash

ollama run phi4

```

> [!NOTE]
> Das Modell wird beim ersten Ausführen heruntergeladen. Natürlich kannst du auch direkt das heruntergeladene Phi-4-Modell angeben. Wir nehmen WSL als Beispiel, um den Befehl auszuführen. Nach dem erfolgreichen Download des Modells kannst du direkt im Terminal interagieren.

![run](../../../../../translated_images/ollama_run.b0be611de61f3bb3b42e22205cedf6714b0335ba9288e71d985bf9024f3c20f5.de.png)

## **2. Die phi-4-API von Ollama aufrufen**

Wenn du die von Ollama generierte Phi-4-API aufrufen möchtest, kannst du diesen Befehl im Terminal verwenden, um den Ollama-Server zu starten.

```bash

ollama serve

```

> [!NOTE]
> Wenn du macOS oder Linux verwendest, beachte bitte, dass du möglicherweise den folgenden Fehler erhältst: **"Error: listen tcp 127.0.0.1:11434: bind: address already in use"**. Dieser Fehler tritt häufig auf, wenn der Server bereits läuft. Du kannst den Fehler entweder ignorieren oder Ollama stoppen und neu starten:

**macOS**

```bash

brew services restart ollama

```

**Linux**

```bash

sudo systemctl stop ollama

```

Ollama unterstützt zwei APIs: generate und chat. Du kannst die von Ollama bereitgestellte Modell-API nach Bedarf aufrufen, indem du Anfragen an den lokalen Dienst sendest, der auf Port 11434 läuft.

**Chat**

```bash

curl http://127.0.0.1:11434/api/chat -d '{
  "model": "phi3",
  "messages": [
    {
      "role": "system",
      "content": "Your are a python developer."
    },
    {
      "role": "user",
      "content": "Help me generate a bubble algorithm"
    }
  ],
  "stream": false
  
}'

This is the result in Postman

![Screenshot of JSON results for generate request](../../../../../translated_images/ollama_gen.bd58ab69d4004826e8cd31e17a3c59840df127b0a30ac9bb38325ac58c74caa5.de.png)

## Additional Resources

Check the list of available models in Ollama in [their library](https://ollama.com/library).

Pull your model from the Ollama server using this command

```bash
ollama pull phi4
```

Run the model using this command

```bash
ollama run phi4
```

***Note:*** Visit this link [https://github.com/ollama/ollama/blob/main/docs/api.md](https://github.com/ollama/ollama/blob/main/docs/api.md) to learn more

## Calling Ollama from Python

You can use `requests` or `urllib3` to make requests to the local server endpoints used above. However, a popular way to use Ollama in Python is via the [openai](https://pypi.org/project/openai/) SDK, since Ollama provides OpenAI-compatible server endpoints as well.

Here is an example for phi3-mini:

```python
import openai

client = openai.OpenAI(
    base_url="http://localhost:11434/v1",
    api_key="nokeyneeded",
)

response = client.chat.completions.create(
    model="phi4",
    temperature=0.7,
    n=1,
    messages=[
        {"role": "system", "content": "Du bist ein hilfreicher Assistent."},
        {"role": "user", "content": "Schreibe ein Haiku über eine hungrige Katze"},
    ],
)

print("Antwort:")
print(response.choices[0].message.content)
```

## Calling Ollama from JavaScript 

```javascript
// Beispiel: Eine Datei mit Phi-4 zusammenfassen
script({
    model: "ollama:phi4",
    title: "Zusammenfassung mit Phi-4",
    system: ["system"],
})

// Beispiel für eine Zusammenfassung
const file = def("FILE", env.files)
$`Fasse ${file} in einem einzigen Absatz zusammen.`
```

## Calling Ollama from C#

Create a new C# Console application and add the following NuGet package:

```bash
dotnet add package Microsoft.SemanticKernel --version 1.34.0
```

Then replace this code in the `Program.cs` file

```csharp
using Microsoft.SemanticKernel;
using Microsoft.SemanticKernel.ChatCompletion;

// Chat-Completion-Service mit dem lokalen Ollama-Server-Endpunkt hinzufügen
#pragma warning disable SKEXP0001, SKEXP0003, SKEXP0010, SKEXP0011, SKEXP0050, SKEXP0052
builder.AddOpenAIChatCompletion(
    modelId: "phi4",
    endpoint: new Uri("http://localhost:11434/"),
    apiKey: "non required");

// Einfache Eingabeaufforderung an den Chat-Service senden
string prompt = "Erzähle einen Witz über Kätzchen";
var response = await kernel.InvokePromptAsync(prompt);
Console.WriteLine(response.GetValue<string>());
```

Run the app with the command:

```bash
dotnet run

**Haftungsausschluss**:  
Dieses Dokument wurde mit KI-basierten maschinellen Übersetzungsdiensten übersetzt. Obwohl wir uns um Genauigkeit bemühen, beachten Sie bitte, dass automatisierte Übersetzungen Fehler oder Ungenauigkeiten enthalten können. Das Originaldokument in seiner ursprünglichen Sprache sollte als maßgebliche Quelle betrachtet werden. Für kritische Informationen wird eine professionelle menschliche Übersetzung empfohlen. Wir übernehmen keine Haftung für Missverständnisse oder Fehlinterpretationen, die sich aus der Nutzung dieser Übersetzung ergeben.