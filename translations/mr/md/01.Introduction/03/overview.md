फाय-3-मिनी संदर्भात, इनफरन्स म्हणजे इनपुट डेटाच्या आधारे मॉडेलचा वापर करून अंदाज लावणे किंवा आउटपुट तयार करणे. फाय-3-मिनी आणि त्याच्या इनफरन्स क्षमतांबद्दल अधिक माहिती देतो.

फाय-3-मिनी हे मायक्रोसॉफ्टने रिलीज केलेल्या फाय-3 मॉडेल्सच्या मालिकेचा भाग आहे. हे मॉडेल्स छोटे भाषा मॉडेल्स (SLMs) कशासाठी सक्षम असतात याची व्याख्या पुन्हा परिभाषित करण्यासाठी डिझाइन केले गेले आहेत.

फाय-3-मिनी आणि त्याच्या इनफरन्स क्षमतांबद्दल काही महत्त्वाचे मुद्दे:

## **फाय-3-मिनीचा आढावा:**
- फाय-3-मिनीचे पॅरामीटर साईज 3.8 बिलियन आहे.
- हे पारंपरिक संगणकीय उपकरणांवरच नाही तर मोबाईल डिव्हाइस आणि IoT डिव्हाइससारख्या एज डिव्हाइसवरही चालवता येते.
- फाय-3-मिनीच्या रिलीजमुळे व्यक्ती आणि उद्योगांना वेगवेगळ्या हार्डवेअर उपकरणांवर, विशेषतः संसाधन-निर्बंधित वातावरणात, SLMs तैनात करण्याची परवानगी मिळते.
- यात विविध मॉडेल फॉरमॅट्सचा समावेश आहे, जसे की पारंपरिक PyTorch फॉरमॅट, gguf फॉरमॅटचे क्वांटाइज्ड व्हर्जन, आणि ONNX-आधारित क्वांटाइज्ड व्हर्जन.

## **फाय-3-मिनीला प्रवेश:**
फाय-3-मिनीला अॅक्सेस करण्यासाठी तुम्ही [Semantic Kernel](https://github.com/microsoft/SemanticKernelCookBook?WT.mc_id=aiml-138114-kinfeylo) चा वापर कोपायलट अॅप्लिकेशनमध्ये करू शकता. सेमॅंटिक कर्नल सामान्यतः Azure OpenAI Service, Hugging Face वरील ओपन-सोर्स मॉडेल्स, आणि लोकल मॉडेल्ससह सुसंगत आहे.  
तुम्ही [Ollama](https://ollama.com) किंवा [LlamaEdge](https://llamaedge.com) देखील क्वांटाइज्ड मॉडेल्ससाठी वापरू शकता. Ollama वैयक्तिक वापरकर्त्यांना वेगवेगळ्या क्वांटाइज्ड मॉडेल्स कॉल करण्याची परवानगी देते, तर LlamaEdge GGUF मॉडेल्ससाठी क्रॉस-प्लॅटफॉर्म उपलब्धता प्रदान करते.

## **क्वांटाइज्ड मॉडेल्स:**
बर्‍याच वापरकर्त्यांना लोकल इनफरन्ससाठी क्वांटाइज्ड मॉडेल्स वापरणे प्राधान्याचे वाटते. उदाहरणार्थ, तुम्ही थेट Ollama चालवून फाय-3 मॉडेल वापरू शकता किंवा ऑफलाइन मोडमध्ये Modelfile वापरून ते कॉन्फिगर करू शकता. Modelfile GGUF फाईल पथ आणि प्रॉम्प्ट फॉरमॅट निर्दिष्ट करते.

## **जनरेटिव्ह AI ची शक्यता:**
फाय-3-मिनीसारख्या SLMs एकत्र करून जनरेटिव्ह AI साठी नवीन शक्यता उघडतात. इनफरन्स ही फक्त पहिली पायरी आहे; हे मॉडेल्स संसाधन-निर्बंधित, लेटन्सी-बाउंड आणि खर्च-निर्बंधित परिस्थितींमध्ये विविध कार्यांसाठी वापरले जाऊ शकतात.

## **फाय-3-मिनीसह जनरेटिव्ह AI अनलॉक करणे: इनफरन्स आणि डिप्लॉयमेंटसाठी मार्गदर्शक**
सेमॅंटिक कर्नल, Ollama/LlamaEdge, आणि ONNX Runtime चा वापर करून फाय-3-मिनी मॉडेल्समध्ये प्रवेश करण्यासाठी आणि इनफरन्स करण्यासाठी शिकून घ्या आणि विविध अॅप्लिकेशन परिदृश्यांमध्ये जनरेटिव्ह AI च्या शक्यता शोधा.

**वैशिष्ट्ये**
फाय-3-मिनी मॉडेलचे इनफरन्स येथे करता येते:

- [Semantic Kernel](https://github.com/Azure-Samples/Phi-3MiniSamples/tree/main/semantickernel?WT.mc_id=aiml-138114-kinfeylo)
- [Ollama](https://github.com/Azure-Samples/Phi-3MiniSamples/tree/main/ollama?WT.mc_id=aiml-138114-kinfeylo)
- [LlamaEdge WASM](https://github.com/Azure-Samples/Phi-3MiniSamples/tree/main/wasm?WT.mc_id=aiml-138114-kinfeylo)
- [ONNX Runtime](https://github.com/Azure-Samples/Phi-3MiniSamples/tree/main/onnx?WT.mc_id=aiml-138114-kinfeylo)
- [iOS](https://github.com/Azure-Samples/Phi-3MiniSamples/tree/main/ios?WT.mc_id=aiml-138114-kinfeylo)

सारांश, फाय-3-मिनी डेव्हलपर्सना विविध मॉडेल फॉरमॅट्स एक्सप्लोर करण्याची आणि विविध अॅप्लिकेशन परिदृश्यांमध्ये जनरेटिव्ह AI चा लाभ घेण्याची परवानगी देते.

**अस्वीकरण**:  
हा दस्तऐवज मशीन-आधारित AI भाषांतर सेवा वापरून अनुवादित करण्यात आला आहे. आम्ही अचूकतेसाठी प्रयत्नशील असलो तरी, कृपया लक्षात घ्या की स्वयंचलित भाषांतरांमध्ये त्रुटी किंवा अचूकतेचा अभाव असू शकतो. मूळ भाषेतील दस्तऐवज हा प्राधिकृत स्रोत मानावा. महत्त्वाच्या माहितीसाठी, व्यावसायिक मानवी भाषांतराचा सल्ला घेणे शिफारसीय आहे. या भाषांतराचा वापर करून निर्माण होणाऱ्या कोणत्याही गैरसमजुतींसाठी किंवा चुकीच्या अर्थांसाठी आम्ही जबाबदार राहणार नाही.