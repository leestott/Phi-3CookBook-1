# **फाय फॅमिलीचे मोजमाप**

मॉडेल क्वांटायझेशन म्हणजे न्यूरल नेटवर्क मॉडेलमधील पॅरामिटर्स (जसे की वजन आणि अॅक्टिव्हेशन मूल्ये) मोठ्या मूल्य श्रेणीमधून (सामान्यतः सतत मूल्य श्रेणी) कमी मर्यादित मूल्य श्रेणीमध्ये मॅप करण्याची प्रक्रिया. या तंत्रज्ञानामुळे मॉडेलचा आकार आणि संगणन जटिलता कमी होते आणि मोबाइल डिव्हाइस किंवा एम्बेडेड सिस्टिमसारख्या संसाधन-आधारित वातावरणात मॉडेलची कार्यक्षमता सुधारते. मॉडेल क्वांटायझेशन पॅरामिटर्सची अचूकता कमी करून कॉम्प्रेशन साध्य करते, परंतु त्यामुळे काही प्रमाणात अचूकतेचा तोटा होतो. त्यामुळे, क्वांटायझेशन प्रक्रियेत मॉडेलचा आकार, संगणन जटिलता आणि अचूकतेचा समतोल राखणे आवश्यक असते. सामान्य क्वांटायझेशन पद्धतींमध्ये फिक्स्ड-पॉइंट क्वांटायझेशन, फ्लोटिंग-पॉइंट क्वांटायझेशन इत्यादींचा समावेश होतो. विशिष्ट परिस्थितीनुसार आणि गरजेनुसार योग्य क्वांटायझेशन धोरण निवडता येते.

आम्हाला GenAI मॉडेल एज डिव्हाइसवर तैनात करायचे आहे आणि जास्तीत जास्त डिव्हाइस GenAI परिस्थितींमध्ये आणायचे आहेत, जसे की मोबाइल डिव्हाइस, AI PC/Copilot+PC, आणि पारंपरिक IoT डिव्हाइस. क्वांटायझेशन मॉडेलच्या मदतीने, आम्ही वेगवेगळ्या डिव्हाइसवर आधारित वेगवेगळ्या एज डिव्हाइसवर ते तैनात करू शकतो. हार्डवेअर उत्पादकांनी पुरवलेल्या मॉडेल एक्सलरेशन फ्रेमवर्क आणि क्वांटायझेशन मॉडेलच्या संयोजनाने, आम्ही चांगल्या SLM अनुप्रयोग परिस्थिती तयार करू शकतो.

क्वांटायझेशनच्या परिस्थितीत, आमच्याकडे विविध अचूकता स्तर आहेत (INT4, INT8, FP16, FP32). खाली सामान्यतः वापरल्या जाणाऱ्या क्वांटायझेशन अचूकतेचे स्पष्टीकरण दिले आहे:

### **INT4**

INT4 क्वांटायझेशन ही एक धाडसी पद्धत आहे, ज्यामध्ये मॉडेलच्या वजन आणि अॅक्टिव्हेशन मूल्यांना 4-बिट पूर्णांकांमध्ये क्वांटायझ केले जाते. लहान प्रतिनिधित्व श्रेणी आणि कमी अचूकतेमुळे INT4 क्वांटायझेशनमध्ये सामान्यतः अचूकतेचा मोठा तोटा होतो. मात्र, INT8 क्वांटायझेशनच्या तुलनेत, INT4 क्वांटायझेशन मॉडेलच्या स्टोरेजची आवश्यकता आणि संगणन जटिलता आणखी कमी करू शकते. असे लक्षात ठेवणे महत्त्वाचे आहे की व्यावहारिक अनुप्रयोगांमध्ये INT4 क्वांटायझेशन तुलनेने दुर्मिळ आहे, कारण खूप कमी अचूकतेमुळे मॉडेलच्या कार्यक्षमतेत लक्षणीय घट होऊ शकते. याशिवाय, सर्व हार्डवेअर INT4 ऑपरेशन्सला सपोर्ट करत नाहीत, त्यामुळे क्वांटायझेशन पद्धत निवडताना हार्डवेअर सुसंगततेचा विचार करणे आवश्यक आहे.

### **INT8**

INT8 क्वांटायझेशन ही प्रक्रिया आहे, ज्यामध्ये मॉडेलचे वजन आणि अॅक्टिव्हेशन फ्लोटिंग पॉइंट नंबरमधून 8-बिट पूर्णांकांमध्ये रूपांतरित केले जाते. जरी INT8 पूर्णांकांनी प्रतिनिधित्व केलेली संख्यात्मक श्रेणी लहान आणि कमी अचूक असते, तरी ती स्टोरेज आणि गणनेच्या गरजा लक्षणीयरीत्या कमी करू शकते. INT8 क्वांटायझेशनमध्ये, मॉडेलच्या वजन आणि अॅक्टिव्हेशन मूल्यांवर क्वांटायझेशन प्रक्रिया केली जाते, ज्यामध्ये स्केलिंग आणि ऑफसेटचा समावेश होतो, ज्यामुळे मूळ फ्लोटिंग पॉइंट माहिती शक्य तितकी जतन केली जाते. अंदाजावेळी, या क्वांटायझ केलेल्या मूल्यांना गणनेसाठी पुन्हा फ्लोटिंग पॉइंट नंबरमध्ये डी-क्वांटायझ केले जाईल आणि नंतर पुढील चरणासाठी पुन्हा INT8 मध्ये क्वांटायझ केले जाईल. ही पद्धत उच्च संगणन कार्यक्षमतेसह बहुतेक अनुप्रयोगांमध्ये पुरेशी अचूकता प्रदान करू शकते.

### **FP16**

FP16 फॉरमॅट म्हणजेच 16-बिट फ्लोटिंग पॉइंट नंबर (float16), 32-बिट फ्लोटिंग पॉइंट नंबर (float32) च्या तुलनेत मेमरीचा उपयोग निम्म्यावर आणते, ज्यामुळे मोठ्या प्रमाणावर डीप लर्निंग अनुप्रयोगांमध्ये महत्त्वपूर्ण फायदे होतात. FP16 फॉरमॅटमुळे GPU मेमरी मर्यादेत मोठी मॉडेल्स लोड करता येतात किंवा अधिक डेटा प्रक्रिया करता येतो. आधुनिक GPU हार्डवेअरमध्ये FP16 ऑपरेशन्सला सपोर्ट असल्याने, FP16 फॉरमॅटचा वापर गणनेच्या वेगातही सुधारणा करू शकतो. मात्र, FP16 फॉरमॅटचे अंतर्निहित तोटेही आहेत, जसे की कमी अचूकता, ज्यामुळे काही प्रकरणांमध्ये संख्यात्मक अस्थिरता किंवा अचूकतेचा तोटा होऊ शकतो.

### **FP32**

FP32 फॉरमॅट उच्च अचूकता प्रदान करते आणि विस्तृत मूल्य श्रेणी अचूकपणे दर्शवू शकते. जिथे जटिल गणितीय ऑपरेशन्स केली जातात किंवा उच्च-अचूक परिणाम आवश्यक असतात, अशा परिस्थितीत FP32 फॉरमॅटला प्राधान्य दिले जाते. मात्र, उच्च अचूकतेमुळे अधिक मेमरीचा वापर आणि जास्त गणना वेळ लागतो. मोठ्या प्रमाणावर डीप लर्निंग मॉडेल्ससाठी, विशेषतः जेव्हा मॉडेल पॅरामिटर्स खूप असतात आणि डेटाचे प्रमाण मोठे असते, तेव्हा FP32 फॉरमॅटमुळे GPU मेमरी अपुरी होऊ शकते किंवा अंदाजाच्या वेगात घट होऊ शकते.

मोबाइल डिव्हाइस किंवा IoT डिव्हाइसवर, आम्ही Phi-3.x मॉडेल्सला INT4 मध्ये रूपांतरित करू शकतो, तर AI PC / Copilot PC सारख्या डिव्हाइससाठी उच्च अचूकता जसे की INT8, FP16, FP32 वापरू शकतो.

सध्या, विविध हार्डवेअर उत्पादकांकडे जेनरेटिव्ह मॉडेल्सला सपोर्ट करण्यासाठी वेगवेगळे फ्रेमवर्क आहेत, जसे की Intel चे OpenVINO, Qualcomm चे QNN, Apple चे MLX, आणि Nvidia चे CUDA. मॉडेल क्वांटायझेशनच्या मदतीने स्थानिक तैनाती पूर्ण केली जाऊ शकते.

तांत्रिक दृष्टिकोनातून, क्वांटायझेशननंतर आमच्याकडे विविध फॉरमॅट्सचे समर्थन आहे, जसे की PyTorch / Tensorflow फॉरमॅट, GGUF, आणि ONNX. GGUF आणि ONNX यांच्यातील फॉरमॅट तुलना आणि अनुप्रयोग परिस्थिती मी तपासली आहे. येथे मी ONNX क्वांटायझेशन फॉरमॅटची शिफारस करतो, कारण मॉडेल फ्रेमवर्कपासून हार्डवेअरपर्यंत त्याला चांगले समर्थन आहे. या अध्यायात, आपण GenAI साठी ONNX Runtime, OpenVINO, आणि Apple MLX वापरून मॉडेल क्वांटायझेशनवर लक्ष केंद्रित करू (जर तुमच्याकडे चांगली पद्धत असेल, तर कृपया PR सबमिट करून आम्हाला कळवा).

**या अध्यायात समाविष्ट आहे**

1. [Phi-3.5 / 4 चे क्वांटायझेशन llama.cpp वापरून](./UsingLlamacppQuantifyingPhi.md)

2. [Phi-3.5 / 4 चे क्वांटायझेशन Generative AI extensions for onnxruntime वापरून](./UsingORTGenAIQuantifyingPhi.md)

3. [Phi-3.5 / 4 चे क्वांटायझेशन Intel OpenVINO वापरून](./UsingIntelOpenVINOQuantifyingPhi.md)

4. [Phi-3.5 / 4 चे क्वांटायझेशन Apple MLX Framework वापरून](./UsingAppleMLXQuantifyingPhi.md)

**अस्वीकरण**:  
हे दस्तऐवज मशीन-आधारित AI अनुवाद सेवांचा वापर करून भाषांतरित करण्यात आले आहे. आम्ही अचूकतेसाठी प्रयत्नशील असलो तरी, कृपया लक्षात घ्या की स्वयंचलित भाषांतरांमध्ये चुका किंवा अचूकतेचा अभाव असू शकतो. मूळ भाषेतील मूळ दस्तऐवज हा अधिकृत स्रोत मानला जावा. महत्त्वाच्या माहितीसाठी, व्यावसायिक मानवी भाषांतराची शिफारस केली जाते. या भाषांतराचा वापर केल्यामुळे उद्भवलेल्या कोणत्याही गैरसमज किंवा चुकीच्या अर्थासाठी आम्ही जबाबदार राहणार नाही.