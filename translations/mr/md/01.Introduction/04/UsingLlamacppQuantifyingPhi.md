# **llama.cpp वापरून Phi कुटुंबाचे क्वांटायझेशन**

## **llama.cpp म्हणजे काय?**

llama.cpp ही एक ओपन-सोर्स सॉफ्टवेअर लायब्ररी आहे जी प्रामुख्याने C++ मध्ये लिहिलेली आहे. ती विविध मोठ्या भाषा मॉडेल्स (LLMs) जसे की Llama यांच्यावर इनफरन्स करते. तिचा मुख्य उद्देश कमी सेटअपसह विविध हार्डवेअरवर अत्याधुनिक कामगिरी प्रदान करणे आहे. याशिवाय, या लायब्ररीसाठी Python बाइंडिंग्स देखील उपलब्ध आहेत, ज्या टेक्स्ट पूर्ण करण्यासाठी उच्च-स्तरीय API आणि OpenAI सुसंगत वेब सर्व्हर प्रदान करतात.

llama.cpp चा मुख्य उद्देश म्हणजे कमी सेटअपसह आणि अत्याधुनिक कामगिरीसह स्थानिक आणि क्लाउडवर विविध हार्डवेअरवर LLM इनफरन्स सक्षम करणे.

- कोणत्याही डिपेंडन्सीशिवाय साधी C/C++ अंमलबजावणी
- Apple silicon ला प्राथमिक महत्त्व - ARM NEON, Accelerate आणि Metal फ्रेमवर्कद्वारे ऑप्टिमाइझ
- x86 आर्किटेक्चरसाठी AVX, AVX2 आणि AVX512 सपोर्ट
- जलद इनफरन्स आणि कमी मेमरी वापरासाठी 1.5-बिट, 2-बिट, 3-बिट, 4-बिट, 5-बिट, 6-बिट, आणि 8-बिट पूर्णांक क्वांटायझेशन
- NVIDIA GPUs वर LLM चालवण्यासाठी कस्टम CUDA कर्नल्स (AMD GPUs साठी HIP द्वारे सपोर्ट)
- Vulkan आणि SYCL बॅकएंड सपोर्ट
- CPU+GPU हायब्रिड इनफरन्स मोठ्या मॉडेल्ससाठी, जे एकूण VRAM क्षमतेपेक्षा जास्त आहेत

## **Phi-3.5 चे llama.cpp वापरून क्वांटायझेशन**

Phi-3.5-Instruct मॉडेलचे क्वांटायझेशन llama.cpp वापरून केले जाऊ शकते, परंतु Phi-3.5-Vision आणि Phi-3.5-MoE सध्या समर्थित नाहीत. llama.cpp ने रूपांतरित केलेला फॉरमॅट gguf आहे, जो क्वांटायझेशनसाठी सर्वाधिक वापरला जाणारा फॉरमॅट आहे.

Hugging Face वर मोठ्या प्रमाणात क्वांटायझ्ड GGUF फॉरमॅट मॉडेल्स उपलब्ध आहेत. AI Foundry, Ollama, आणि LlamaEdge हे llama.cpp वर अवलंबून आहेत, त्यामुळे GGUF मॉडेल्सही मोठ्या प्रमाणावर वापरले जातात.

### **GGUF म्हणजे काय?**

GGUF हा एक बायनरी फॉरमॅट आहे जो मॉडेल्स लवकर लोड आणि सेव्ह करण्यासाठी ऑप्टिमाइझ केलेला आहे, ज्यामुळे तो इनफरन्ससाठी अत्यंत कार्यक्षम ठरतो. GGUF GGML आणि इतर एक्झिक्युटर्ससाठी डिझाइन केलेला आहे. GGUF @ggerganov यांनी विकसित केला आहे, जे llama.cpp या लोकप्रिय C/C++ LLM इनफरन्स फ्रेमवर्कचे डेव्हलपर आहेत. सुरुवातीला PyTorch सारख्या फ्रेमवर्कमध्ये विकसित केलेली मॉडेल्स GGUF फॉरमॅटमध्ये रूपांतरित करून वापरली जाऊ शकतात.

### **ONNX vs GGUF**

ONNX हा पारंपरिक मशीन लर्निंग/डीप लर्निंग फॉरमॅट आहे, जो विविध AI फ्रेमवर्क्समध्ये चांगल्या प्रकारे समर्थित आहे आणि एज डिव्हाइसवर चांगल्या प्रकारे वापरला जातो. GGUF बद्दल सांगायचे तर, तो llama.cpp वर आधारित आहे आणि जनरेटिव्ह AI च्या युगात विकसित झालेला आहे. दोघांचे उपयोगसंदर्भ समान आहेत. जर तुम्हाला एम्बेडेड हार्डवेअर आणि अॅप्लिकेशन लेयर्समध्ये चांगली कामगिरी हवी असेल, तर ONNX हा तुमचा पर्याय असू शकतो. जर तुम्ही llama.cpp च्या डेरिव्हेटिव्ह फ्रेमवर्क आणि तंत्रज्ञानाचा वापर करत असाल, तर GGUF अधिक चांगला ठरू शकतो.

### **Phi-3.5-Instruct चे llama.cpp वापरून क्वांटायझेशन**

**1. वातावरण कॉन्फिगरेशन**


```bash

git clone https://github.com/ggerganov/llama.cpp.git

cd llama.cpp

make -j8

```


**2. क्वांटायझेशन**

llama.cpp वापरून Phi-3.5-Instruct ला FP16 GGUF मध्ये रूपांतरित करा


```bash

./convert_hf_to_gguf.py <Your Phi-3.5-Instruct Location> --outfile phi-3.5-128k-mini_fp16.gguf

```

Phi-3.5 चे INT4 मध्ये क्वांटायझेशन


```bash

./llama.cpp/llama-quantize <Your phi-3.5-128k-mini_fp16.gguf location> ./gguf/phi-3.5-128k-mini_Q4_K_M.gguf Q4_K_M

```


**3. चाचणी**

llama-cpp-python इंस्टॉल करा


```bash

pip install llama-cpp-python -U

```

***टीप*** 

जर तुम्ही Apple Silicon वापरत असाल, तर कृपया llama-cpp-python अशा प्रकारे इंस्टॉल करा


```bash

CMAKE_ARGS="-DLLAMA_METAL=on" pip install llama-cpp-python -U

```

चाचणी 


```bash

llama.cpp/llama-cli --model <Your phi-3.5-128k-mini_Q4_K_M.gguf location> --prompt "<|user|>\nCan you introduce .NET<|end|>\n<|assistant|>\n"  --gpu-layers 10

```



## **संसाधने**

1. llama.cpp बद्दल अधिक जाणून घ्या [https://onnxruntime.ai/docs/genai/](https://onnxruntime.ai/docs/genai/)

2. GGUF बद्दल अधिक जाणून घ्या [https://huggingface.co/docs/hub/en/gguf](https://huggingface.co/docs/hub/en/gguf)

**अस्वीकरण**:  
हे दस्तऐवज मशीन-आधारित AI अनुवाद सेवांचा वापर करून भाषांतरित केले गेले आहे. आम्ही अचूकतेसाठी प्रयत्नशील असलो तरी, कृपया लक्षात घ्या की स्वयंचलित भाषांतरांमध्ये त्रुटी किंवा अचूकतेचा अभाव असू शकतो. मूळ भाषेतील मूळ दस्तऐवज प्राधिकृत स्रोत मानला पाहिजे. महत्त्वाच्या माहितीसाठी व्यावसायिक मानवी भाषांतराची शिफारस केली जाते. या भाषांतराच्या वापरामुळे उद्भवलेल्या कोणत्याही गैरसमजुती किंवा चुकीच्या अर्थासाठी आम्ही जबाबदार नाही.