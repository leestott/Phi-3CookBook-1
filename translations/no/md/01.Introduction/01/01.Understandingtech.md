# Nøkkel-teknologier som nevnes inkluderer

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - et lavnivå-API for maskinvareakselerert maskinlæring bygget på toppen av DirectX 12.
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - en plattform for parallell databehandling og applikasjonsprogrammeringsgrensesnitt (API) utviklet av Nvidia, som muliggjør generell prosessering på grafikkprosessorer (GPUs).
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - et åpent format designet for å representere maskinlæringsmodeller som gir interoperabilitet mellom ulike ML-rammeverk.
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - et format brukt til å representere og oppdatere maskinlæringsmodeller, spesielt nyttig for mindre språkmodeller som kan kjøre effektivt på CPU-er med 4-8bit kvantisering.

## DirectML

DirectML er et lavnivå-API som muliggjør maskinvareakselerert maskinlæring. Det er bygget på toppen av DirectX 12 for å utnytte GPU-akselerasjon og er leverandøruavhengig, noe som betyr at det ikke krever kodeendringer for å fungere på tvers av ulike GPU-leverandører. Det brukes hovedsakelig til modelltrening og inferensarbeidsmengder på GPU-er.

Når det gjelder maskinvarestøtte, er DirectML designet for å fungere med et bredt spekter av GPU-er, inkludert AMD integrerte og dedikerte GPU-er, Intel integrerte GPU-er og NVIDIA dedikerte GPU-er. Det er en del av Windows AI-plattformen og støttes på Windows 10 og 11, noe som gjør det mulig å trene og kjøre modeller på alle Windows-enheter.

Det har vært oppdateringer og muligheter relatert til DirectML, som støtte for opptil 150 ONNX-operatører og bruk i både ONNX-runtime og WinML. Det støttes av store maskinvareleverandører (IHVs), som hver implementerer ulike metakommander.

## CUDA

CUDA, som står for Compute Unified Device Architecture, er en plattform for parallell databehandling og et applikasjonsprogrammeringsgrensesnitt (API) utviklet av Nvidia. Det lar programvareutviklere bruke en CUDA-kompatibel grafikkprosessor (GPU) til generell prosessering – en tilnærming kalt GPGPU (General-Purpose computing on Graphics Processing Units). CUDA er en nøkkelkomponent i Nvidias GPU-akselerasjon og brukes mye innen ulike felt, inkludert maskinlæring, vitenskapelig databehandling og videobehandling.

Maskinvarestøtten for CUDA er spesifikk for Nvidias GPU-er, siden det er en proprietær teknologi utviklet av Nvidia. Hver arkitektur støtter spesifikke versjoner av CUDA-verktøysettet, som gir de nødvendige bibliotekene og verktøyene for utviklere til å bygge og kjøre CUDA-applikasjoner.

## ONNX

ONNX (Open Neural Network Exchange) er et åpent format designet for å representere maskinlæringsmodeller. Det gir en definisjon av en utvidbar beregningsgrafmodell, samt definisjoner av innebygde operatører og standard datatyper. ONNX lar utviklere flytte modeller mellom ulike ML-rammeverk, noe som muliggjør interoperabilitet og gjør det enklere å lage og distribuere AI-applikasjoner.

Phi3 mini kan kjøre med ONNX Runtime på CPU og GPU på tvers av enheter, inkludert serverplattformer, Windows, Linux og Mac-desktopper, samt mobile CPU-er. De optimaliserte konfigurasjonene vi har lagt til er:

- ONNX-modeller for int4 DML: Kvantisert til int4 via AWQ
- ONNX-modell for fp16 CUDA
- ONNX-modell for int4 CUDA: Kvantisert til int4 via RTN
- ONNX-modell for int4 CPU og mobil: Kvantisert til int4 via RTN

## Llama.cpp

Llama.cpp er et åpen kildekode-bibliotek skrevet i C++. Det utfører inferens på ulike store språkmodeller (LLMs), inkludert Llama. Utviklet sammen med ggml-biblioteket (et generelt tensorbibliotek), har llama.cpp som mål å gi raskere inferens og lavere minnebruk sammenlignet med den originale Python-implementasjonen. Det støtter maskinvareoptimalisering, kvantisering og tilbyr et enkelt API og eksempler. Hvis du er interessert i effektiv LLM-inferens, er llama.cpp verdt å utforske, da Phi3 kan kjøre Llama.cpp.

## GGUF

GGUF (Generic Graph Update Format) er et format brukt til å representere og oppdatere maskinlæringsmodeller. Det er spesielt nyttig for mindre språkmodeller (SLMs) som kan kjøre effektivt på CPU-er med 4-8bit kvantisering. GGUF er fordelaktig for rask prototyping og kjøring av modeller på edge-enheter eller i batch-jobber som CI/CD-pipelines.

**Ansvarsfraskrivelse**:  
Dette dokumentet er oversatt ved hjelp av maskinbaserte AI-oversettelsestjenester. Selv om vi bestreber oss på nøyaktighet, vennligst vær oppmerksom på at automatiske oversettelser kan inneholde feil eller unøyaktigheter. Det originale dokumentet på sitt opprinnelige språk bør anses som den autoritative kilden. For kritisk informasjon anbefales profesjonell menneskelig oversettelse. Vi er ikke ansvarlige for misforståelser eller feiltolkninger som oppstår ved bruk av denne oversettelsen.