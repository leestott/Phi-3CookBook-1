# Kom i gang med Phi-3 lokalt

Denne veiledningen hjelper deg med å sette opp ditt lokale miljø for å kjøre Phi-3-modellen ved hjelp av Ollama. Du kan kjøre modellen på flere måter, inkludert bruk av GitHub Codespaces, VS Code Dev Containers, eller ditt lokale miljø.

## Miljøoppsett

### GitHub Codespaces

Du kan kjøre denne malen virtuelt ved å bruke GitHub Codespaces. Knappen åpner en nettbasert VS Code-instans i nettleseren din:

1. Åpne malen (dette kan ta flere minutter):

    [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/microsoft/phi-3cookbook)

2. Åpne et terminalvindu.

### VS Code Dev Containers

⚠️ Dette alternativet fungerer kun hvis Docker Desktop er tildelt minst 16 GB RAM. Hvis du har mindre enn 16 GB RAM, kan du prøve [GitHub Codespaces-alternativet](../../../../../md/01.Introduction/01) eller [sette det opp lokalt](../../../../../md/01.Introduction/01).

Et annet alternativ er VS Code Dev Containers, som åpner prosjektet i din lokale VS Code ved hjelp av [Dev Containers-utvidelsen](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers):

1. Start Docker Desktop (installer det hvis det ikke allerede er installert).
2. Åpne prosjektet:

    [![Open in Dev Containers](https://img.shields.io/static/v1?style=for-the-badge&label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/microsoft/phi-3cookbook)

3. I VS Code-vinduet som åpnes, når prosjektfilene vises (dette kan ta flere minutter), åpne et terminalvindu.
4. Fortsett med [distribusjonstrinnene](../../../../../md/01.Introduction/01).

### Lokalt miljø

1. Sørg for at følgende verktøy er installert:

    * [Ollama](https://ollama.com/)
    * [Python 3.10+](https://www.python.org/downloads/)
    * [OpenAI Python SDK](https://pypi.org/project/openai/)

## Test modellen

1. Be Ollama om å laste ned og kjøre phi3:mini-modellen:

    ```shell
    ollama run phi3:mini
    ```

    Dette vil ta noen minutter for å laste ned modellen.

2. Når du ser "success" i utdataene, kan du sende en melding til modellen fra prompten.

    ```shell
    >>> Write a haiku about hungry hippos
    ```

3. Etter noen sekunder skal du se et svar strømme inn fra modellen.

4. For å lære om ulike teknikker brukt med språkmodeller, åpne Python-notatboken [ollama.ipynb](../../../../../code/01.Introduce/ollama.ipynb) og kjør hver celle. Hvis du brukte en annen modell enn 'phi3:mini', endre `MODEL_NAME` in the first cell.

5. To have a conversation with the phi3:mini model from Python, open the Python file [chat.py](../../../../../code/01.Introduce/chat.py) and run it. You can change the `MODEL_NAME` øverst i filen etter behov. Du kan også endre systemmeldingen eller legge til få-skuddseksempler hvis ønskelig.

**Ansvarsfraskrivelse**:  
Dette dokumentet er oversatt ved hjelp av maskinbaserte AI-oversettingstjenester. Selv om vi tilstreber nøyaktighet, vær oppmerksom på at automatiserte oversettelser kan inneholde feil eller unøyaktigheter. Det originale dokumentet på sitt opprinnelige språk bør betraktes som den autoritative kilden. For kritisk informasjon anbefales profesjonell menneskelig oversettelse. Vi er ikke ansvarlige for eventuelle misforståelser eller feiltolkninger som oppstår ved bruk av denne oversettelsen.