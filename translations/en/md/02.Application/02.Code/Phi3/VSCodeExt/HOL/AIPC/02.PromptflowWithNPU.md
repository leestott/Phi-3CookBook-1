# **Lab 2 - Run Prompt flow with Phi-3-mini in AIPC**

## **What is Prompt flow**

Prompt flow is a set of development tools designed to simplify the entire development cycle of AI applications based on large language models (LLMs), from ideation, prototyping, testing, evaluation, to production deployment and monitoring. It makes prompt engineering much more straightforward and allows you to build LLM applications with production-grade quality.

With Prompt flow, you can:

- Create workflows that combine LLMs, prompts, Python code, and other tools into an executable flow.

- Debug and refine your workflows, especially interactions with LLMs, effortlessly.

- Evaluate your workflows, measure quality and performance metrics with larger datasets.

- Integrate testing and evaluation into your CI/CD system to ensure the quality of your workflow.

- Deploy your workflows to your chosen serving platform or integrate them into your application’s codebase with ease.

- (Optional but highly recommended) Collaborate with your team by using the cloud version of Prompt flow in Azure AI.

## **What is AIPC**

An AI PC (AIPC) is equipped with a CPU, a GPU, and an NPU, each providing specific AI acceleration capabilities. An NPU, or neural processing unit, is a specialized accelerator designed to handle artificial intelligence (AI) and machine learning (ML) tasks directly on your PC, rather than relying on cloud processing. While GPUs and CPUs can also handle these tasks, the NPU excels at low-power AI computations. The AI PC represents a major evolution in how computers function. It doesn’t solve a problem that didn’t exist before but instead offers a significant enhancement for everyday PC use.

How does it work? Unlike generative AI and the massive LLMs trained on vast amounts of public data, the AI on your PC operates on a much more accessible level. The concept is easier to grasp, and since it’s trained on your data without requiring cloud access, the benefits are immediately tangible to a wider audience.

In the short term, AI PCs will feature personal assistants and smaller AI models running directly on your PC, leveraging your data to provide personalized, private, and more secure AI enhancements for everyday tasks—like taking meeting notes, organizing a fantasy football league, automating photo and video editing, or planning a family reunion itinerary based on everyone’s travel schedules.

## **Creating generation code flows on AIPC**

***Note***: If you haven’t completed the environment installation, please refer to [Lab 0 - Installations](./01.Installations.md).

1. Open the Prompt flow Extension in Visual Studio Code and create an empty flow project.

![create](../../../../../../../../../translated_images/pf_create.d6172d8277a78a7fa82cd6ff727ed44e037fa78b662f1f62d5963f36d712d229.en.png)

2. Add Input and Output parameters, and add Python Code as a new flow.

![flow](../../../../../../../../../translated_images/pf_flow.d5646a323fb7f444c0b98b4521057a592325c583e7ba18bc31500bc0415e9ef3.en.png)

You can use this structure (flow.dag.yaml) as a reference to build your flow:

```yaml

inputs:
  question:
    type: string
    default: how to write Bubble Algorithm
outputs:
  answer:
    type: string
    reference: ${Chat_With_Phi3.output}
nodes:
- name: Chat_With_Phi3
  type: python
  source:
    type: code
    path: Chat_With_Phi3.py
  inputs:
    question: ${inputs.question}


```

3. Add the code in ***Chat_With_Phi3.py***.

```python


from promptflow.core import tool

# import torch
from transformers import AutoTokenizer, pipeline,TextStreamer
import intel_npu_acceleration_library as npu_lib

import warnings

import asyncio
import platform

class Phi3CodeAgent:
    
    model = None
    tokenizer = None
    text_streamer = None
    
    model_id = "microsoft/Phi-3-mini-4k-instruct"

    @staticmethod
    def init_phi3():
        
        if Phi3CodeAgent.model is None or Phi3CodeAgent.tokenizer is None or Phi3CodeAgent.text_streamer is None:
            Phi3CodeAgent.model = npu_lib.NPUModelForCausalLM.from_pretrained(
                                    Phi3CodeAgent.model_id,
                                    torch_dtype="auto",
                                    dtype=npu_lib.int4,
                                    trust_remote_code=True
                                )
            Phi3CodeAgent.tokenizer = AutoTokenizer.from_pretrained(Phi3CodeAgent.model_id)
            Phi3CodeAgent.text_streamer = TextStreamer(Phi3CodeAgent.tokenizer, skip_prompt=True)

    

    @staticmethod
    def chat_with_phi3(prompt):
        
        Phi3CodeAgent.init_phi3()

        messages = "<|system|>You are a AI Python coding assistant. Please help me to generate code in Python.The answer only genertated Python code, but any comments and instructions do not need to be generated<|end|><|user|>" + prompt +"<|end|><|assistant|>"



        generation_args = {
            "max_new_tokens": 1024,
            "return_full_text": False,
            "temperature": 0.3,
            "do_sample": False,
            "streamer": Phi3CodeAgent.text_streamer,
        }

        pipe = pipeline(
            "text-generation",
            model=Phi3CodeAgent.model,
            tokenizer=Phi3CodeAgent.tokenizer,
            # **generation_args
        )

        result = ''

        with warnings.catch_warnings():
            warnings.simplefilter("ignore")
            response = pipe(messages, **generation_args)
            result =response[0]['generated_text']
            return result


@tool
def my_python_tool(question: str) -> str:
    if platform.system() == 'Windows':
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    return Phi3CodeAgent.chat_with_phi3(question)


```

4. Test the flow by using Debug or Run to ensure the generation code works correctly.

![RUN](../../../../../../../../../translated_images/pf_run.d918637dc00f61e9bdeec37d4cc9646f77d270ac9203bcce13569f3157202b6e.en.png)

5. Run the flow as a development API in the terminal.

```

pf flow serve --source ./ --port 8080 --host localhost   

```

You can test it using Postman or Thunder Client.

### **Notes**

1. The first run may take a long time. It’s recommended to download the Phi-3 model using the Hugging Face CLI.

2. Due to the limited computing power of the Intel NPU, it’s recommended to use Phi-3-mini-4k-instruct.

3. We use Intel NPU Acceleration for INT4 quantization. However, if you restart the service, you need to delete the cache and nc_workshop folders.

## **Resources**

1. Learn about Prompt flow: [https://microsoft.github.io/promptflow/](https://microsoft.github.io/promptflow/)

2. Learn about Intel NPU Acceleration: [https://github.com/intel/intel-npu-acceleration-library](https://github.com/intel/intel-npu-acceleration-library)

3. Sample code: Download [Local NPU Agent Sample Code](../../../../../../../../../code/07.Lab/01/AIPC)

**Disclaimer**:  
This document has been translated using machine-based AI translation services. While we strive for accuracy, please note that automated translations may contain errors or inaccuracies. The original document in its native language should be regarded as the authoritative source. For critical information, professional human translation is recommended. We are not responsible for any misunderstandings or misinterpretations resulting from the use of this translation.