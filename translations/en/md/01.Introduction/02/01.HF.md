# **Using Phi Family in Hugging Face**

[Hugging Face](https://huggingface.co/) is a widely popular AI community offering extensive data and open-source model resources. Various companies, such as Microsoft, Meta, Mistral, Apple, and Google, release open-source LLMs and SLMs through Hugging Face.

Microsoft's Phi Family is now available on Hugging Face. Developers can download the appropriate Phi Family models based on their use cases and business needs. In addition to deploying Phi Pytorch models on Hugging Face, quantized models in GGUF and ONNX formats have also been released, providing end users with more options.

## **Downloading Models from Hugging Face**

You can download the Phi Family models using the following links:

-  **Phi-1 / 1.5** https://huggingface.co/collections/microsoft/phi-1-6626e29134744e94e222d572

-  **Phi-3 / 3.5** https://huggingface.co/collections/microsoft/phi-3-6626e15e9585a200d2d761e3

-  **Phi-4** https://huggingface.co/collections/microsoft/phi-4-677e9380e514feb5577a40e4

There are multiple ways to download the models, such as using the ***Hugging Face CLI SDK*** or ***git clone***.

### **Downloading Phi Family Models Using Hugging Face CLI**

- Install Hugging Face CLI

```bash

pip install -U "huggingface_hub[cli]"

```

- Log in using the huggingface-cli

Log in to Hugging Face using your [User Access Token](https://huggingface.co/docs/hub/security-tokens) from the [Settings page](https://huggingface.co/settings/tokens).

```bash

huggingface-cli login --token $HF_TOKEN --add-to-git-credential

```

- Download the model

You can download the model and save it to the cache:

```bash

huggingface-cli download microsoft/phi-4

```

Alternatively, you can specify a custom location to save the model:

```bash

huggingface-cli download microsoft/phi-4 --local-dir $YOUR_PATH

```

### **Downloading Phi Family Models Using Git Clone**

You can also download the models using ***git clone***:

```bash

git lfs install

git clone https://huggingface.co/microsoft/phi-4

```

## **Examples - Running Inference with Microsoft Phi-4**

- **Installing the transformers library**

```bash

pip install transformers -U

```

- **Executing the code in VSCode**

```python

import transformers

pipeline = transformers.pipeline(
    "text-generation",
    model="microsoft/phi-4",
    model_kwargs={"torch_dtype": "auto"},
    device_map="auto",
)

messages = [
    {"role": "user", "content": "I have $20,000 in my savings account, where I receive a 4% profit per year and payments twice a year. Can you please tell me how long it will take for me to become a millionaire? Also, can you please explain the math step by step as if you were explaining it to an uneducated person?"},
]

outputs = pipeline(messages, max_new_tokens=2048)
print(outputs[0]["generated_text"][-1])

```

**Disclaimer**:  
This document has been translated using machine-based AI translation services. While we strive for accuracy, please note that automated translations may contain errors or inaccuracies. The original document in its native language should be regarded as the authoritative source. For critical information, professional human translation is recommended. We are not responsible for any misunderstandings or misinterpretations resulting from the use of this translation.