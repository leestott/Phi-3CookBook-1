# Get started with Phi-3 locally

This guide will help you set up your local environment to run the Phi-3 model using Ollama. You can run the model in several ways, including using GitHub Codespaces, VS Code Dev Containers, or your local environment.

## Environment setup

### GitHub Codespaces

You can run this template virtually by using GitHub Codespaces. Clicking the button will open a web-based VS Code instance in your browser:

1. Open the template (this might take a few minutes):

    [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/microsoft/phi-3cookbook)

2. Open a terminal window.

### VS Code Dev Containers

⚠️ This option will only work if your Docker Desktop is allocated at least 16 GB of RAM. If you have less than 16 GB of RAM, consider using the [GitHub Codespaces option](../../../../../md/01.Introduction/01) or [setting it up locally](../../../../../md/01.Introduction/01).

Another option is using VS Code Dev Containers, which will open the project in your local VS Code instance with the help of the [Dev Containers extension](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers):

1. Start Docker Desktop (install it if it’s not already installed).
2. Open the project:

    [![Open in Dev Containers](https://img.shields.io/static/v1?style=for-the-badge&label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/microsoft/phi-3cookbook)

3. In the VS Code window that opens, wait until the project files appear (this might take a few minutes), then open a terminal window.
4. Continue with the [deployment steps](../../../../../md/01.Introduction/01).

### Local Environment

1. Ensure the following tools are installed:

    * [Ollama](https://ollama.com/)
    * [Python 3.10+](https://www.python.org/downloads/)
    * [OpenAI Python SDK](https://pypi.org/project/openai/)

## Test the model

1. Use Ollama to download and run the phi3:mini model:

    ```shell
    ollama run phi3:mini
    ```

    This step may take a few minutes to download the model.

2. Once the output shows "success," you can send a message to the model from the prompt.

    ```shell
    >>> Write a haiku about hungry hippos
    ```

3. After a few seconds, you should see the model respond.

4. To explore various techniques for working with language models, open the Python notebook [ollama.ipynb](../../../../../code/01.Introduce/ollama.ipynb) and execute each cell. If you used a model other than 'phi3:mini', update the `MODEL_NAME` in the first cell.

5. To have a conversation with the phi3:mini model from Python, open the Python file [chat.py](../../../../../code/01.Introduce/chat.py) and run it. You can change the `MODEL_NAME` at the top of the file as needed. You can also modify the system message or add few-shot examples if desired.

**Disclaimer**:  
This document has been translated using machine-based AI translation services. While we strive for accuracy, please note that automated translations may contain errors or inaccuracies. The original document in its native language should be regarded as the authoritative source. For critical information, professional human translation is recommended. We are not responsible for any misunderstandings or misinterpretations resulting from the use of this translation.