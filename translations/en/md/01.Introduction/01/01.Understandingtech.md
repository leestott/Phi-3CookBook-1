# Key technologies mentioned include

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - a low-level API for hardware-accelerated machine learning built on DirectX 12.
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - a parallel computing platform and API model developed by Nvidia, enabling general-purpose processing on GPUs.
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - an open format for representing machine learning models, allowing interoperability between different ML frameworks.
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - a format for representing and updating machine learning models, especially useful for smaller language models that work well on CPUs with 4-8bit quantization.

## DirectML

DirectML is a low-level API that facilitates hardware-accelerated machine learning. Built on DirectX 12, it leverages GPU acceleration and is vendor-agnostic, meaning it works seamlessly across various GPU vendors without requiring code changes. It’s primarily used for training and inferencing models on GPUs.

Regarding hardware support, DirectML is compatible with a wide range of GPUs, including AMD integrated and discrete GPUs, Intel integrated GPUs, and NVIDIA discrete GPUs. As part of the Windows AI Platform, it’s supported on Windows 10 and 11, enabling model training and inferencing on any Windows device.

There have been updates and developments in DirectML, such as support for up to 150 ONNX operators and its integration with ONNX runtime and WinML. It’s supported by major Integrated Hardware Vendors (IHVs), each implementing different metacommands.

## CUDA

CUDA, short for Compute Unified Device Architecture, is a parallel computing platform and API model developed by Nvidia. It enables developers to use CUDA-enabled GPUs for general-purpose processing, an approach known as GPGPU (General-Purpose computing on Graphics Processing Units). CUDA is a cornerstone of Nvidia’s GPU acceleration and is widely utilized in fields like machine learning, scientific computing, and video processing.

CUDA’s hardware support is exclusive to Nvidia GPUs, as it’s a proprietary technology. Each GPU architecture supports specific versions of the CUDA toolkit, which includes libraries and tools necessary for developing and running CUDA applications.

## ONNX

ONNX (Open Neural Network Exchange) is an open format for representing machine learning models. It defines an extensible computation graph model, built-in operators, and standard data types. ONNX enables developers to transfer models between various ML frameworks, promoting interoperability and simplifying the creation and deployment of AI applications.

Phi3 mini can operate with ONNX Runtime on both CPU and GPU across devices, including server platforms, Windows, Linux, Mac desktops, and mobile CPUs.
The optimized configurations we’ve added include:

- ONNX models for int4 DML: Quantized to int4 using AWQ
- ONNX model for fp16 CUDA
- ONNX model for int4 CUDA: Quantized to int4 using RTN
- ONNX model for int4 CPU and Mobile: Quantized to int4 using RTN

## Llama.cpp

Llama.cpp is an open-source C++ library for performing inference on various Large Language Models (LLMs), including Llama. Developed alongside the ggml library (a general-purpose tensor library), llama.cpp focuses on delivering faster inference and reduced memory usage compared to the original Python implementation. It supports hardware optimizations, quantization, and provides a straightforward API with examples. If efficient LLM inference interests you, llama.cpp is worth exploring, as Phi3 can run Llama.cpp.

## GGUF

GGUF (Generic Graph Update Format) is a format designed for representing and updating machine learning models. It’s particularly effective for smaller language models (SLMs) that run efficiently on CPUs with 4-8bit quantization. GGUF is advantageous for rapid prototyping and deploying models on edge devices or in batch processes like CI/CD pipelines.

**Disclaimer**:  
This document has been translated using machine-based AI translation services. While we strive for accuracy, please be aware that automated translations may contain errors or inaccuracies. The original document in its native language should be considered the authoritative source. For critical information, professional human translation is recommended. We are not liable for any misunderstandings or misinterpretations arising from the use of this translation.