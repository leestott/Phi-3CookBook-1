# Ključne pomenute tehnologije uključuju

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - niskonivo API za hardverski ubrzani mašinsko učenje, izgrađen na vrhu DirectX 12.
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - platforma za paralelno računanje i model programskog interfejsa (API) razvijena od strane Nvidia, koja omogućava opštu obradu na grafičkim procesorskim jedinicama (GPU).
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - otvoreni format dizajniran za predstavljanje modela mašinskog učenja, koji omogućava interoperabilnost između različitih ML okvira.
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - format koji se koristi za predstavljanje i ažuriranje modela mašinskog učenja, posebno koristan za manje jezičke modele koji mogu efikasno da rade na CPU-ovima sa kvantizacijom od 4-8 bita.

## DirectML

DirectML je niskonivo API koji omogućava hardverski ubrzano mašinsko učenje. Izgrađen je na vrhu DirectX 12 kako bi iskoristio GPU ubrzanje i nezavisan je od proizvođača, što znači da ne zahteva promene u kodu da bi radio na različitim GPU proizvođačima. Primarno se koristi za obuku modela i inferencu na GPU-ovima.

Što se tiče hardverske podrške, DirectML je dizajniran da radi sa širokim spektrom GPU-ova, uključujući AMD integrisane i diskretne GPU-ove, Intel integrisane GPU-ove i NVIDIA diskretne GPU-ove. Deo je Windows AI platforme i podržan je na Windows 10 i 11, omogućavajući obuku i inferencu modela na bilo kom Windows uređaju.

Postoje ažuriranja i mogućnosti vezane za DirectML, kao što je podrška za do 150 ONNX operatora i korišćenje od strane ONNX runtime-a i WinML-a. Podržan je od strane glavnih proizvođača hardvera (IHV-ova), pri čemu svaki implementira različite metakomande.

## CUDA

CUDA, što je skraćenica za Compute Unified Device Architecture, je platforma za paralelno računanje i model programskog interfejsa (API) koji je kreirala Nvidia. Omogućava softverskim developerima da koriste CUDA-kompatibilne grafičke procesorske jedinice (GPU) za opštu obradu – pristup poznat kao GPGPU (General-Purpose computing on Graphics Processing Units). CUDA je ključni faktor Nvidia GPU ubrzanja i široko se koristi u različitim oblastima, uključujući mašinsko učenje, naučno računanje i obradu videa.

Hardverska podrška za CUDA je specifična za Nvidia GPU-ove, jer je to vlasnička tehnologija koju je razvila Nvidia. Svaka arhitektura podržava specifične verzije CUDA alata, koji pružaju potrebne biblioteke i alate za developere da izgrade i pokrenu CUDA aplikacije.

## ONNX

ONNX (Open Neural Network Exchange) je otvoreni format dizajniran za predstavljanje modela mašinskog učenja. Pruža definiciju proširivog modela grafičkog računanja, kao i definicije ugrađenih operatora i standardnih tipova podataka. ONNX omogućava developerima da prenose modele između različitih ML okvira, čime omogućavaju interoperabilnost i olakšavaju kreiranje i implementaciju AI aplikacija.

Phi3 mini može raditi sa ONNX Runtime-om na CPU i GPU uređajima, uključujući serverske platforme, Windows, Linux i Mac desktop računare, kao i mobilne CPU-ove. Optimizovane konfiguracije koje smo dodali uključuju:

- ONNX modeli za int4 DML: Kvantizovani na int4 putem AWQ
- ONNX model za fp16 CUDA
- ONNX model za int4 CUDA: Kvantizovani na int4 putem RTN
- ONNX model za int4 CPU i mobilne uređaje: Kvantizovani na int4 putem RTN

## Llama.cpp

Llama.cpp je softverska biblioteka otvorenog koda napisana u C++. Izvodi inferencu na različitim velikim jezičkim modelima (LLM), uključujući Llama. Razvijen zajedno sa ggml bibliotekom (biblioteka za opšte namene tenzora), llama.cpp ima za cilj da pruži bržu inferencu i manju potrošnju memorije u poređenju sa originalnom Python implementacijom. Podržava hardversku optimizaciju, kvantizaciju i nudi jednostavan API sa primerima. Ako ste zainteresovani za efikasnu inferencu LLM-ova, llama.cpp vredi istražiti jer Phi3 može pokretati Llama.cpp.

## GGUF

GGUF (Generic Graph Update Format) je format koji se koristi za predstavljanje i ažuriranje modela mašinskog učenja. Posebno je koristan za manje jezičke modele (SLM) koji mogu efikasno da rade na CPU-ovima sa kvantizacijom od 4-8 bita. GGUF je koristan za brzo prototipisanje i pokretanje modela na uređajima na ivici mreže ili u serijskim zadacima poput CI/CD pipeline-ova.

**Одрицање од одговорности**:  
Овај документ је преведен коришћењем машинских АИ услуга за превођење. Иако настојимо да обезбедимо тачност, имајте на уму да аутоматизовани преводи могу садржати грешке или нетачности. Оригинални документ на изворном језику треба сматрати меродавним. За критичне информације препоручује се професионални превод од стране људи. Не сносимо одговорност за било каква погрешна тумачења или неспоразуме који могу произаћи из коришћења овог превода.