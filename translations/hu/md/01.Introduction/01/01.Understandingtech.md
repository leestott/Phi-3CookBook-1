# Főbb említett technológiák

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - egy alacsony szintű API, amely a DirectX 12-re épül, és hardvergyorsított gépi tanulást tesz lehetővé.
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - egy párhuzamos számítási platform és alkalmazásprogramozási interfész (API) modell, amelyet az Nvidia fejlesztett ki, és lehetővé teszi az általános célú feldolgozást grafikus feldolgozó egységeken (GPU-kon).
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - egy nyílt formátum, amely gépi tanulási modellek ábrázolására szolgál, és biztosítja a különböző ML keretrendszerek közötti interoperabilitást.
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - egy formátum, amely gépi tanulási modellek ábrázolására és frissítésére használható, különösen hasznos kisebb nyelvi modellek esetében, amelyek hatékonyan futtathatók CPU-kon 4-8 bites kvantálással.

## DirectML

A DirectML egy alacsony szintű API, amely hardvergyorsított gépi tanulást tesz lehetővé. A DirectX 12-re épül, hogy kihasználja a GPU-gyorsítást, és gyártófüggetlen, azaz nem igényel kódmódosítást ahhoz, hogy különböző GPU-gyártók eszközein működjön. Elsősorban modellek tanítására és következtetési feladatokra használják GPU-kon.

Ami a hardvertámogatást illeti, a DirectML széles körű GPU-kal kompatibilis, beleértve az AMD integrált és diszkrét GPU-kat, az Intel integrált GPU-kat és az NVIDIA diszkrét GPU-kat. A Windows AI Platform része, és támogatott Windows 10 és 11 rendszereken, lehetővé téve a modellek tanítását és következtetéseit bármely Windows-eszközön.

A DirectML-lel kapcsolatban frissítések és lehetőségek is elérhetők, például akár 150 ONNX operátor támogatása, illetve az ONNX runtime és a WinML általi használat. A technológiát nagy hardvergyártók (IHV-k) támogatják, amelyek különböző metaparancsokat implementáltak.

## CUDA

A CUDA, amely a Compute Unified Device Architecture rövidítése, egy párhuzamos számítási platform és alkalmazásprogramozási interfész (API) modell, amelyet az Nvidia hozott létre. Lehetővé teszi a szoftverfejlesztők számára, hogy CUDA-kompatibilis grafikus feldolgozó egységet (GPU-t) használjanak általános célú feldolgozásra – ezt a megközelítést GPGPU-nak (General-Purpose computing on Graphics Processing Units) nevezik. A CUDA kulcsfontosságú eleme az Nvidia GPU-gyorsításának, és széles körben használják különböző területeken, beleértve a gépi tanulást, a tudományos számításokat és a videófeldolgozást.

A CUDA hardvertámogatása kizárólag az Nvidia GPU-kra korlátozódik, mivel ez egy az Nvidia által fejlesztett szabadalmazott technológia. Minden architektúra specifikus CUDA eszközkészlet-verziókat támogat, amelyek biztosítják a szükséges könyvtárakat és eszközöket a fejlesztők számára a CUDA alkalmazások létrehozásához és futtatásához.

## ONNX

Az ONNX (Open Neural Network Exchange) egy nyílt formátum, amely gépi tanulási modellek ábrázolására szolgál. Meghatározza egy bővíthető számítási gráfmodellt, valamint beépített operátorok és szabványos adattípusok definícióit. Az ONNX lehetővé teszi a fejlesztők számára, hogy modelleket mozgassanak különböző ML keretrendszerek között, elősegítve az interoperabilitást és megkönnyítve a mesterséges intelligencia alkalmazások létrehozását és telepítését.

A Phi3 mini képes futni ONNX Runtime segítségével CPU-n és GPU-n különböző eszközökön, beleértve a szerverplatformokat, Windows, Linux és Mac asztali számítógépeket, valamint mobil CPU-kat. Az optimalizált konfigurációk, amelyeket hozzáadtunk:

- ONNX modellek int4 DML-hez: AWQ-n keresztül int4-re kvantálva
- ONNX modell fp16 CUDA-hoz
- ONNX modell int4 CUDA-hoz: RTN-en keresztül int4-re kvantálva
- ONNX modell int4 CPU-hoz és mobilhoz: RTN-en keresztül int4-re kvantálva

## Llama.cpp

A Llama.cpp egy nyílt forráskódú szoftverkönyvtár, amely C++ nyelven íródott. Képes következtetéseket végezni különböző nagy nyelvi modellek (LLM-ek), például a Llama esetében. A ggml könyvtár (egy általános célú tenzorkönyvtár) mellett fejlesztették ki, és célja a gyorsabb következtetés és az alacsonyabb memóriahasználat biztosítása az eredeti Python-implementációhoz képest. Támogatja a hardveroptimalizálást, a kvantálást, és egyszerű API-t, valamint példákat kínál. Ha érdekel a hatékony LLM következtetés, érdemes felfedezni a Llama.cpp-t, mivel a Phi3 képes futtatni a Llama.cpp-t.

## GGUF

A GGUF (Generic Graph Update Format) egy formátum, amely gépi tanulási modellek ábrázolására és frissítésére szolgál. Különösen hasznos kisebb nyelvi modellek (SLM-ek) esetében, amelyek hatékonyan futtathatók CPU-kon 4-8 bites kvantálással. A GGUF előnyös gyors prototípusfejlesztéshez, valamint modellek futtatásához peremhálózati eszközökön vagy olyan batch-munkákban, mint például CI/CD folyamatok.

**Felelősség kizárása**:  
Ez a dokumentum gépi AI fordítási szolgáltatások segítségével került lefordításra. Bár igyekszünk pontosságra törekedni, kérjük, vegye figyelembe, hogy az automatikus fordítások hibákat vagy pontatlanságokat tartalmazhatnak. Az eredeti dokumentum az eredeti nyelvén tekintendő hiteles forrásnak. Kritikus információk esetén javasolt professzionális, emberi fordítást igénybe venni. Nem vállalunk felelősséget az ebből a fordításból eredő félreértésekért vagy téves értelmezésekért.