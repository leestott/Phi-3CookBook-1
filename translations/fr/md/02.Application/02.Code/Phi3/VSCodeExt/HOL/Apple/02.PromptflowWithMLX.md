# **Lab 2 - Exécuter Prompt flow avec Phi-3-mini dans AIPC**

## **Qu'est-ce que Prompt flow**

Prompt flow est une suite d'outils de développement conçue pour simplifier le cycle complet de développement des applications IA basées sur les LLM, depuis l'idéation, le prototypage, les tests, l'évaluation, jusqu'au déploiement en production et la surveillance. Cela facilite grandement l'ingénierie des prompts et permet de créer des applications LLM de qualité production.

Avec Prompt flow, vous pourrez :

- Créer des flux qui relient des LLM, des prompts, du code Python et d'autres outils dans un flux de travail exécutable.

- Déboguer et itérer vos flux, en particulier les interactions avec les LLM, de manière simple.

- Évaluer vos flux, calculer des métriques de qualité et de performance avec des ensembles de données plus larges.

- Intégrer les tests et l'évaluation dans votre système CI/CD pour garantir la qualité de votre flux.

- Déployer vos flux sur la plateforme de service de votre choix ou les intégrer facilement dans le code de votre application.

- (Optionnel mais fortement recommandé) Collaborer avec votre équipe en utilisant la version cloud de Prompt flow dans Azure AI.



## **Construire des flux de génération de code sur Apple Silicon**

***Note*** : Si vous n'avez pas encore terminé l'installation de l'environnement, veuillez visiter [Lab 0 - Installations](./01.Installations.md)

1. Ouvrez l'extension Prompt flow dans Visual Studio Code et créez un projet de flux vide.

![create](../../../../../../../../../translated_images/pf_create.d6172d8277a78a7fa82cd6ff727ed44e037fa78b662f1f62d5963f36d712d229.fr.png)

2. Ajoutez des paramètres d'entrées et de sorties, puis ajoutez du code Python comme nouveau flux.

![flow](../../../../../../../../../translated_images/pf_flow.d5646a323fb7f444c0b98b4521057a592325c583e7ba18bc31500bc0415e9ef3.fr.png)


Vous pouvez vous référer à cette structure (flow.dag.yaml) pour construire votre flux.

```yaml

inputs:
  prompt:
    type: string
    default: Write python code for Fibonacci serie. Please use markdown as output
outputs:
  result:
    type: string
    reference: ${gen_code_by_phi3.output}
nodes:
- name: gen_code_by_phi3
  type: python
  source:
    type: code
    path: gen_code_by_phi3.py
  inputs:
    prompt: ${inputs.prompt}


```

3. Quantifier phi-3-mini

Nous souhaitons exécuter le SLM de manière optimale sur des appareils locaux. En général, nous quantifions le modèle (INT4, FP16, FP32).

```bash

python -m mlx_lm.convert --hf-path microsoft/Phi-3-mini-4k-instruct

```

**Note :** le dossier par défaut est mlx_model.

4. Ajoutez du code dans ***Chat_With_Phi3.py***.

```python


from promptflow import tool

from mlx_lm import load, generate


# The inputs section will change based on the arguments of the tool function, after you save the code
# Adding type to arguments and return value will help the system show the types properly
# Please update the function name/signature per need
@tool
def my_python_tool(prompt: str) -> str:

    model_id = './mlx_model_phi3_mini'

    model, tokenizer = load(model_id)

    # <|user|>\nWrite python code for Fibonacci serie. Please use markdown as output<|end|>\n<|assistant|>

    response = generate(model, tokenizer, prompt="<|user|>\n" + prompt  + "<|end|>\n<|assistant|>", max_tokens=2048, verbose=True)

    return response


```

4. Vous pouvez tester le flux en mode Debug ou Run pour vérifier si la génération de code fonctionne correctement.

![RUN](../../../../../../../../../translated_images/pf_run.d918637dc00f61e9bdeec37d4cc9646f77d270ac9203bcce13569f3157202b6e.fr.png)

5. Exécutez le flux comme une API de développement dans le terminal.

```

pf flow serve --source ./ --port 8080 --host localhost   

```

Vous pouvez le tester dans Postman / Thunder Client.


### **Remarques**

1. La première exécution prend beaucoup de temps. Il est recommandé de télécharger le modèle phi-3 depuis Hugging Face CLI.

2. Compte tenu de la puissance de calcul limitée de l'Intel NPU, il est conseillé d'utiliser Phi-3-mini-4k-instruct.

3. Nous utilisons l'accélération Intel NPU pour la conversion quantifiée en INT4, mais si vous relancez le service, vous devez supprimer les dossiers cache et nc_workshop.



## **Ressources**

1. Apprenez Promptflow [https://microsoft.github.io/promptflow/](https://microsoft.github.io/promptflow/)

2. Apprenez l'accélération Intel NPU [https://github.com/intel/intel-npu-acceleration-library](https://github.com/intel/intel-npu-acceleration-library)

3. Code exemple, téléchargez [Code exemple Agent NPU local](../../../../../../../../../code/07.Lab/01/AIPC/local-npu-agent)

**Avertissement** :  
Ce document a été traduit à l'aide de services de traduction automatisée basés sur l'IA. Bien que nous nous efforcions d'assurer l'exactitude, veuillez noter que les traductions automatisées peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue d'origine doit être considéré comme la source faisant autorité. Pour des informations critiques, il est recommandé de recourir à une traduction humaine professionnelle. Nous déclinons toute responsabilité en cas de malentendus ou d'interprétations erronées résultant de l'utilisation de cette traduction.