# Ključne spomenute tehnologije uključuju

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - niskorazinski API za hardverski ubrzano strojno učenje, izgrađen na DirectX 12.
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - platforma za paralelno računalstvo i model sučelja za programiranje aplikacija (API) koju je razvila Nvidia, omogućujući opću obradu na grafičkim procesorskim jedinicama (GPU-ovima).
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - otvoreni format dizajniran za predstavljanje modela strojnog učenja, koji omogućuje interoperabilnost između različitih ML okvira.
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - format koji se koristi za predstavljanje i ažuriranje modela strojnog učenja, posebno koristan za manje jezične modele koji učinkovito rade na CPU-ovima s kvantizacijom od 4-8 bita.

## DirectML

DirectML je niskorazinski API koji omogućuje hardverski ubrzano strojno učenje. Izgrađen je na DirectX 12 kako bi iskoristio GPU ubrzanje i ne ovisi o proizvođaču, što znači da ne zahtijeva promjene u kodu kako bi radio na različitim GPU-ovima. Primarno se koristi za treniranje modela i izvođenje zaključaka na GPU-ovima.

Što se tiče hardverske podrške, DirectML je dizajniran za rad s velikim brojem GPU-ova, uključujući AMD integrirane i diskretne GPU-ove, Intel integrirane GPU-ove te NVIDIA diskretne GPU-ove. Dio je Windows AI platforme i podržan je na Windows 10 i 11, omogućujući treniranje i izvođenje modela na bilo kojem Windows uređaju.

Postoje ažuriranja i mogućnosti vezane uz DirectML, poput podrške za do 150 ONNX operatora i korištenja u ONNX runtimeu i WinML-u. Podržan je od strane glavnih proizvođača hardvera (IHV-ova), od kojih svaki implementira različite metakomande.

## CUDA

CUDA, što znači Compute Unified Device Architecture, platforma je za paralelno računalstvo i model API-ja koji je razvila Nvidia. Omogućuje programerima korištenje CUDA-kompatibilnog GPU-a za opću obradu – pristup poznat kao GPGPU (General-Purpose computing on Graphics Processing Units). CUDA je ključni element Nvidia GPU ubrzanja i široko se koristi u različitim područjima, uključujući strojno učenje, znanstveno računalstvo i obradu videa.

Hardverska podrška za CUDA specifična je za Nvidia GPU-ove, budući da je to vlasnička tehnologija koju je razvila Nvidia. Svaka arhitektura podržava određene verzije CUDA alata, koji pružaju potrebne biblioteke i alate za razvoj i pokretanje CUDA aplikacija.

## ONNX

ONNX (Open Neural Network Exchange) je otvoreni format dizajniran za predstavljanje modela strojnog učenja. Pruža definiciju proširivog modela računalnog grafa, kao i definicije ugrađenih operatora i standardnih tipova podataka. ONNX omogućuje programerima premještanje modela između različitih ML okvira, čime se osigurava interoperabilnost i olakšava stvaranje i implementacija AI aplikacija.

Phi3 mini može se pokretati s ONNX Runtimeom na CPU-ovima i GPU-ovima na različitim uređajima, uključujući serverske platforme, Windows, Linux i Mac stolna računala te mobilne CPU-ove.
Optimizirane konfiguracije koje smo dodali uključuju:

- ONNX modeli za int4 DML: Kvantizirani na int4 putem AWQ-a
- ONNX model za fp16 CUDA
- ONNX model za int4 CUDA: Kvantizirani na int4 putem RTN-a
- ONNX model za int4 CPU i Mobile: Kvantizirani na int4 putem RTN-a

## Llama.cpp

Llama.cpp je open-source softverska biblioteka napisana u C++. Izvodi zaključke na različitim velikim jezičnim modelima (LLM-ovima), uključujući Llama. Razvijen zajedno s ggml bibliotekom (općom bibliotekom za rad s tenzorima), llama.cpp ima za cilj omogućiti brže izvođenje zaključaka i manju potrošnju memorije u usporedbi s originalnom Python implementacijom. Podržava hardversku optimizaciju, kvantizaciju i nudi jednostavan API i primjere. Ako vas zanima učinkovito izvođenje LLM-ova, llama.cpp vrijedi istražiti jer Phi3 može pokretati Llama.cpp.

## GGUF

GGUF (Generic Graph Update Format) je format koji se koristi za predstavljanje i ažuriranje modela strojnog učenja. Posebno je koristan za manje jezične modele (SLM-ove) koji učinkovito rade na CPU-ovima s kvantizacijom od 4-8 bita. GGUF je koristan za brzo prototipiranje i pokretanje modela na rubnim uređajima ili u serijskim zadacima poput CI/CD cjevovoda.

**Odricanje od odgovornosti**:  
Ovaj dokument je preveden koristeći usluge strojno baziranog AI prijevoda. Iako nastojimo osigurati točnost, imajte na umu da automatski prijevodi mogu sadržavati pogreške ili netočnosti. Izvorni dokument na izvornom jeziku treba smatrati mjerodavnim izvorom. Za ključne informacije preporučuje se profesionalni ljudski prijevod. Ne snosimo odgovornost za nesporazume ili pogrešna tumačenja koja proizlaze iz korištenja ovog prijevoda.