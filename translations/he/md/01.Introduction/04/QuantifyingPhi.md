# **כימות משפחת Phi**

כימות מודלים מתייחס לתהליך של מיפוי הפרמטרים (כמו משקלים וערכי אקטיבציה) במודל רשת עצבית מטווח ערכים גדול (בדרך כלל טווח ערכים רציף) לטווח ערכים סופי קטן יותר. טכנולוגיה זו יכולה להקטין את גודל המודל ואת המורכבות החישובית שלו ולשפר את יעילות הפעולה של המודל בסביבות עם מגבלות משאבים, כמו מכשירים ניידים או מערכות משובצות. כימות מודלים משיגה דחיסה על ידי הפחתת דיוק הפרמטרים, אך גם מכניסה אובדן מסוים של דיוק. לכן, בתהליך הכימות, יש צורך לאזן בין גודל המודל, המורכבות החישובית והדיוק. שיטות כימות נפוצות כוללות כימות בנקודה קבועה, כימות בנקודה צפה ועוד. ניתן לבחור באסטרטגיית הכימות המתאימה בהתאם לתרחיש ולצרכים הספציפיים.

אנחנו מקווים לפרוס מודלי GenAI במכשירי קצה ולאפשר ליותר מכשירים להיכנס לתרחישי GenAI, כמו מכשירים ניידים, מחשבי AI PC / Copilot+PC ומכשירי IoT מסורתיים. באמצעות מודל כימות, נוכל לפרוס אותו למכשירי קצה שונים בהתבסס על המכשירים השונים. בשילוב עם מסגרת האצת מודלים ומודל כימות שמספקים יצרני החומרה, נוכל לבנות תרחישי יישום SLM טובים יותר.

בתרחיש הכימות, יש לנו רמות דיוק שונות (INT4, INT8, FP16, FP32). להלן הסבר על רמות הכימות הנפוצות:

### **INT4**

כימות INT4 היא שיטת כימות רדיקלית שמכמתת את המשקלים וערכי האקטיבציה של המודל למספרים שלמים בני 4 ביטים. כימות INT4 בדרך כלל גורמת לאובדן דיוק גדול יותר בגלל טווח הייצוג הקטן יותר והדיוק הנמוך יותר. עם זאת, בהשוואה לכימות INT8, כימות INT4 יכולה להקטין עוד יותר את דרישות האחסון ואת המורכבות החישובית של המודל. יש לציין שכימות INT4 יחסית נדירה ביישומים מעשיים, מכיוון שדיוק נמוך מדי עלול לגרום לירידה משמעותית בביצועי המודל. בנוסף, לא כל החומרה תומכת בפעולות INT4, ולכן יש לקחת בחשבון תאימות חומרה בעת בחירת שיטת הכימות.

### **INT8**

כימות INT8 הוא תהליך המרת המשקלים והאקטיבציות של המודל ממספרים בנקודה צפה למספרים שלמים בני 8 ביטים. למרות שטווח המספרים שמיוצג על ידי INT8 קטן יותר ופחות מדויק, הוא יכול להפחית באופן משמעותי את דרישות האחסון והחישוב. בכימות INT8, המשקלים וערכי האקטיבציה של המודל עוברים תהליך כימות, הכולל קנה מידה והיסט, כדי לשמר את המידע המקורי של נקודה צפה ככל האפשר. במהלך החישוב, ערכים מכומתים אלה יומרו חזרה לנקודה צפה לצורך חישוב, ואז יכומתנו שוב ל-INT8 לשלב הבא. שיטה זו יכולה לספק דיוק מספק ברוב היישומים תוך שמירה על יעילות חישוב גבוהה.

### **FP16**

פורמט FP16, כלומר מספרים בנקודה צפה בני 16 ביטים (float16), מפחית את צריכת הזיכרון בחצי בהשוואה למספרים בנקודה צפה בני 32 ביטים (float32), מה שמספק יתרונות משמעותיים ביישומי למידה עמוקה בקנה מידה גדול. פורמט FP16 מאפשר טעינת מודלים גדולים יותר או עיבוד של יותר נתונים במסגרת מגבלות הזיכרון של ה-GPU. ככל שחומרת GPU מודרנית ממשיכה לתמוך בפעולות FP16, שימוש בפורמט FP16 עשוי גם להביא לשיפורים במהירות החישוב. עם זאת, לפורמט FP16 יש גם חסרונות מובנים, כמו דיוק נמוך יותר, שעלול להוביל לחוסר יציבות מספרית או לאובדן דיוק במקרים מסוימים.

### **FP32**

פורמט FP32 מספק דיוק גבוה יותר ויכול לייצג במדויק טווח רחב של ערכים. בתרחישים שבהם מבוצעות פעולות מתמטיות מורכבות או נדרשים תוצאות מדויקות מאוד, פורמט FP32 הוא המועדף. עם זאת, דיוק גבוה גם אומר שימוש רב יותר בזיכרון וזמן חישוב ארוך יותר. עבור מודלים גדולים של למידה עמוקה, במיוחד כאשר ישנם פרמטרים רבים במודל וכמות עצומה של נתונים, פורמט FP32 עשוי לגרום למחסור בזיכרון GPU או לירידה במהירות החישוב.

במכשירים ניידים או מכשירי IoT, ניתן להמיר מודלים של Phi-3.x ל-INT4, בעוד שמחשבי AI PC / Copilot PC יכולים להשתמש ברמות דיוק גבוהות יותר כמו INT8, FP16, FP32.

כיום, ליצרני חומרה שונים יש מסגרות שונות לתמיכה במודלים גנרטיביים, כמו OpenVINO של Intel, QNN של Qualcomm, MLX של Apple ו-CUDA של Nvidia. בשילוב עם כימות מודלים, ניתן להשלים פריסה מקומית.

מבחינה טכנולוגית, יש לנו תמיכה בפורמטים שונים לאחר כימות, כמו פורמטים של PyTorch / Tensorflow, GGUF ו-ONNX. ערכתי השוואה בין GGUF ל-ONNX ותרחישי היישום שלהם. כאן אני ממליץ על פורמט הכימות של ONNX, שמקבל תמיכה טובה ממסגרת המודל ועד החומרה. בפרק זה נתמקד ב-ONNX Runtime ל-GenAI, OpenVINO ו-Apple MLX לביצוע כימות מודלים (אם יש לכם דרך טובה יותר, אתם מוזמנים לשתף אותנו באמצעות הגשת PR).

**הפרק כולל**

1. [כימות Phi-3.5 / 4 באמצעות llama.cpp](./UsingLlamacppQuantifyingPhi.md)

2. [כימות Phi-3.5 / 4 באמצעות הרחבות Generative AI ל-onnxruntime](./UsingORTGenAIQuantifyingPhi.md)

3. [כימות Phi-3.5 / 4 באמצעות Intel OpenVINO](./UsingIntelOpenVINOQuantifyingPhi.md)

4. [כימות Phi-3.5 / 4 באמצעות מסגרת Apple MLX](./UsingAppleMLXQuantifyingPhi.md)

**כתב ויתור**:  
מסמך זה תורגם באמצעות שירותי תרגום מבוססי בינה מלאכותית. למרות שאנו שואפים לדיוק, יש להיות מודעים לכך שתרגומים אוטומטיים עשויים להכיל שגיאות או אי-דיוקים. המסמך המקורי בשפתו המקורית ייחשב כמקור הסמכותי. למידע קריטי, מומלץ להשתמש בתרגום מקצועי אנושי. אנו לא נושאים באחריות לאי-הבנות או לפרשנויות שגויות הנובעות מהשימוש בתרגום זה.