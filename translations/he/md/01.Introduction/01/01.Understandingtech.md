# טכנולוגיות מפתח שהוזכרו כוללות

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - ממשק API ברמה נמוכה ללמידת מכונה מואצת בחומרה, המבוסס על DirectX 12.
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - פלטפורמת מחשוב מקבילי ומודל ממשק תכנות יישומים (API) שפותח על ידי Nvidia, המאפשר עיבוד כללי על יחידות עיבוד גרפיות (GPUs).
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - פורמט פתוח שנועד לייצג מודלים של למידת מכונה ולספק יכולת פעולה הדדית בין מסגרות ML שונות.
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - פורמט המשמש לייצוג ועדכון מודלים של למידת מכונה, במיוחד עבור מודלים קטנים שיכולים לפעול ביעילות על מעבדים עם קוונטיזציה של 4-8 ביט.

## DirectML

DirectML הוא ממשק API ברמה נמוכה שמאפשר למידת מכונה מואצת בחומרה. הוא מבוסס על DirectX 12 כדי לנצל את האצת ה-GPU והוא בלתי תלוי בספק, כלומר לא נדרשים שינויים בקוד כדי לעבוד עם ספקי GPU שונים. הוא משמש בעיקר לאימונים והרצות של מודלים על GPUs.

מבחינת תמיכה בחומרה, DirectML נועד לעבוד עם מגוון רחב של GPUs, כולל GPUs משולבים ודיסקרטיים של AMD, GPUs משולבים של Intel, ו-GPUs דיסקרטיים של NVIDIA. הוא חלק מפלטפורמת Windows AI ותומך ב-Windows 10 ו-11, ומאפשר אימון והרצת מודלים על כל מכשיר Windows.

היו עדכונים והזדמנויות הקשורים ל-DirectML, כמו תמיכה ביותר מ-150 פעולות ONNX ושימוש על ידי ONNX Runtime ו-WinML. הוא נתמך על ידי ספקי חומרה משולבים גדולים (IHVs), שכל אחד מהם מיישם פקודות מטא שונות.

## CUDA

CUDA, קיצור של Compute Unified Device Architecture, היא פלטפורמת מחשוב מקבילי ומודל ממשק API שפותח על ידי Nvidia. היא מאפשרת למפתחים להשתמש ב-GPU שתומך ב-CUDA לעיבוד כללי – גישה שמכונה GPGPU (מחשוב כללי על יחידות עיבוד גרפיות). CUDA היא מפתח מרכזי להאצת GPU של Nvidia ומשמשת במגוון תחומים, כולל למידת מכונה, מחשוב מדעי ועיבוד וידאו.

התמיכה בחומרה עבור CUDA מוגבלת ל-GPUs של Nvidia, שכן זו טכנולוגיה קניינית שפותחה על ידי Nvidia. כל ארכיטקטורה תומכת בגרסאות ספציפיות של ערכת הכלים CUDA, המספקת את הספריות והכלים הנדרשים למפתחים לבנות ולהפעיל יישומי CUDA.

## ONNX

ONNX (Open Neural Network Exchange) הוא פורמט פתוח שנועד לייצג מודלים של למידת מכונה. הוא מספק הגדרה של מודל גרף חישובי ניתן להרחבה, כמו גם הגדרות של פעולות מובנות וסוגי נתונים סטנדרטיים. ONNX מאפשר למפתחים להעביר מודלים בין מסגרות ML שונות, מספק יכולת פעולה הדדית ומקל על יצירה ופריסה של יישומי AI.

Phi3 mini יכול לפעול עם ONNX Runtime על CPU ו-GPU במגוון מכשירים, כולל פלטפורמות שרת, שולחנות עבודה עם Windows, Linux ו-Mac, ו-CPUs ניידים. 
הקונפיגורציות המותאמות שהוספנו כוללות:

- מודלים של ONNX עבור int4 DML: קוונטיזציה ל-int4 באמצעות AWQ
- מודל ONNX עבור fp16 CUDA
- מודל ONNX עבור int4 CUDA: קוונטיזציה ל-int4 באמצעות RTN
- מודל ONNX עבור int4 CPU וניידים: קוונטיזציה ל-int4 באמצעות RTN

## Llama.cpp

Llama.cpp היא ספריית תוכנה בקוד פתוח שנכתבה ב-C++. היא מבצעת הסקה על מגוון מודלים של שפה גדולה (LLMs), כולל Llama. הספרייה פותחה יחד עם ggml (ספריית טנזורים כללית), ומטרתה לספק הסקה מהירה יותר ושימוש נמוך יותר בזיכרון בהשוואה למימוש המקורי בפייתון. היא תומכת באופטימיזציה לחומרה, קוונטיזציה, ומציעה ממשק API פשוט ודוגמאות. אם אתם מעוניינים בהסקה יעילה של LLM, כדאי לבדוק את Llama.cpp, שכן Phi3 יכול להריץ Llama.cpp.

## GGUF

GGUF (Generic Graph Update Format) הוא פורמט המשמש לייצוג ועדכון מודלים של למידת מכונה. הוא מועיל במיוחד עבור מודלים קטנים (SLMs) שיכולים לפעול ביעילות על CPUs עם קוונטיזציה של 4-8 ביט. GGUF מועיל במיוחד לפרוטוטיפ מהיר ולהפעלת מודלים על מכשירי קצה או במשימות בקבוצות כמו צינורות CI/CD.

**כתב ויתור**:  
מסמך זה תורגם באמצעות שירותי תרגום מבוססי בינה מלאכותית. בעוד שאנו שואפים לדיוק, יש להיות מודעים לכך שתרגומים אוטומטיים עשויים לכלול שגיאות או אי-דיוקים. המסמך המקורי בשפתו המקורית יש להחשיב כמקור הסמכותי. למידע קריטי, מומלץ להשתמש בתרגום מקצועי אנושי. איננו נושאים באחריות לאי-הבנות או לפרשנויות שגויות הנובעות משימוש בתרגום זה.