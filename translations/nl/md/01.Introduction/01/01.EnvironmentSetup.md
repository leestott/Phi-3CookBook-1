# Aan de slag met Phi-3 lokaal

Deze handleiding helpt je om je lokale omgeving in te stellen om het Phi-3 model te draaien met Ollama. Je kunt het model op verschillende manieren gebruiken, waaronder met GitHub Codespaces, VS Code Dev Containers, of je lokale omgeving.

## Omgevingsinstellingen

### GitHub Codespaces

Je kunt deze template virtueel draaien met GitHub Codespaces. De knop opent een webgebaseerde VS Code-instantie in je browser:

1. Open de template (dit kan enkele minuten duren):

    [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/microsoft/phi-3cookbook)

2. Open een terminalvenster

### VS Code Dev Containers

⚠️ Deze optie werkt alleen als je Docker Desktop minstens 16 GB RAM toegewezen heeft. Als je minder dan 16 GB RAM hebt, kun je de [GitHub Codespaces optie](../../../../../md/01.Introduction/01) proberen of [het lokaal instellen](../../../../../md/01.Introduction/01).

Een gerelateerde optie is VS Code Dev Containers, waarmee je het project opent in je lokale VS Code met behulp van de [Dev Containers extensie](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers):

1. Start Docker Desktop (installeer het als het nog niet geïnstalleerd is)
2. Open het project:

    [![Open in Dev Containers](https://img.shields.io/static/v1?style=for-the-badge&label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/microsoft/phi-3cookbook)

3. In het geopende VS Code-venster, zodra de projectbestanden verschijnen (dit kan enkele minuten duren), open je een terminalvenster.
4. Ga verder met de [implementatiestappen](../../../../../md/01.Introduction/01)

### Lokale omgeving

1. Zorg ervoor dat de volgende tools geïnstalleerd zijn:

    * [Ollama](https://ollama.com/)
    * [Python 3.10+](https://www.python.org/downloads/)
    * [OpenAI Python SDK](https://pypi.org/project/openai/)

## Test het model

1. Vraag Ollama om het phi3:mini model te downloaden en uit te voeren:

    ```shell
    ollama run phi3:mini
    ```

    Dit duurt enkele minuten om het model te downloaden.

2. Zodra je "success" in de uitvoer ziet, kun je een bericht naar dat model sturen vanuit de prompt.

    ```shell
    >>> Write a haiku about hungry hippos
    ```

3. Na enkele seconden zou je een antwoord van het model moeten zien binnenstromen.

4. Om meer te leren over verschillende technieken die worden gebruikt met taalmodellen, open het Python-notebook [ollama.ipynb](../../../../../code/01.Introduce/ollama.ipynb) en voer elke cel uit. Als je een ander model dan 'phi3:mini' hebt gebruikt, pas dan de `MODEL_NAME` in the first cell.

5. To have a conversation with the phi3:mini model from Python, open the Python file [chat.py](../../../../../code/01.Introduce/chat.py) and run it. You can change the `MODEL_NAME` bovenaan het bestand aan indien nodig. Je kunt ook het systeembericht wijzigen of few-shot voorbeelden toevoegen als gewenst.

**Disclaimer (Vrijwaring)**:  
Dit document is vertaald met behulp van AI-gebaseerde vertaaldiensten. Hoewel we ons best doen voor nauwkeurigheid, dient u zich ervan bewust te zijn dat geautomatiseerde vertalingen fouten of onnauwkeurigheden kunnen bevatten. Het originele document in de oorspronkelijke taal moet worden beschouwd als de gezaghebbende bron. Voor kritieke informatie wordt professionele menselijke vertaling aanbevolen. Wij zijn niet aansprakelijk voor misverstanden of verkeerde interpretaties die voortvloeien uit het gebruik van deze vertaling.