## Phi-familie in Ollama

[Ollama](https://ollama.com) maakt het mogelijk voor meer mensen om open source LLM of SLM direct te implementeren via eenvoudige scripts. Daarnaast kun je API's bouwen om lokale Copilot-toepassingen te ondersteunen.

## **1. Installatie**

Ollama werkt op Windows, macOS en Linux. Je kunt Ollama installeren via deze link ([https://ollama.com/download](https://ollama.com/download)). Na een succesvolle installatie kun je direct via een terminalvenster het Phi-3-model aanroepen met een Ollama-script. Je kunt alle [beschikbare bibliotheken in Ollama](https://ollama.com/library) bekijken. Als je deze repository opent in een Codespace, is Ollama al geïnstalleerd.

```bash

ollama run phi4

```

> [!NOTE]
> Het model wordt voor de eerste keer gedownload wanneer je het uitvoert. Uiteraard kun je ook direct een eerder gedownload Phi-4-model specificeren. We nemen WSL als voorbeeld om het commando uit te voeren. Zodra het model succesvol is gedownload, kun je direct via de terminal interactie hebben.

![run](../../../../../translated_images/ollama_run.b0be611de61f3bb3b42e22205cedf6714b0335ba9288e71d985bf9024f3c20f5.nl.png)

## **2. De phi-4 API aanroepen vanuit Ollama**

Als je de Phi-4 API wilt aanroepen die door Ollama is gegenereerd, kun je dit commando in de terminal gebruiken om de Ollama-server te starten.

```bash

ollama serve

```

> [!NOTE]
> Als je macOS of Linux gebruikt, kun je de volgende foutmelding tegenkomen: **"Error: listen tcp 127.0.0.1:11434: bind: address already in use"**. Deze foutmelding kan optreden bij het uitvoeren van het commando. Je kunt deze foutmelding negeren, aangezien dit meestal betekent dat de server al draait. Of je kunt Ollama stoppen en opnieuw starten:

**macOS**

```bash

brew services restart ollama

```

**Linux**

```bash

sudo systemctl stop ollama

```

Ollama ondersteunt twee API's: generate en chat. Je kunt de model-API gebruiken die Ollama aanbiedt, afhankelijk van jouw behoeften, door verzoeken te sturen naar de lokale service die draait op poort 11434.

**Chat**

```bash

curl http://127.0.0.1:11434/api/chat -d '{
  "model": "phi3",
  "messages": [
    {
      "role": "system",
      "content": "Your are a python developer."
    },
    {
      "role": "user",
      "content": "Help me generate a bubble algorithm"
    }
  ],
  "stream": false
  
}'

This is the result in Postman

![Screenshot of JSON results for generate request](../../../../../translated_images/ollama_gen.bd58ab69d4004826e8cd31e17a3c59840df127b0a30ac9bb38325ac58c74caa5.nl.png)

## Additional Resources

Check the list of available models in Ollama in [their library](https://ollama.com/library).

Pull your model from the Ollama server using this command

```bash
ollama pull phi4
```

Run the model using this command

```bash
ollama run phi4
```

***Note:*** Visit this link [https://github.com/ollama/ollama/blob/main/docs/api.md](https://github.com/ollama/ollama/blob/main/docs/api.md) to learn more

## Calling Ollama from Python

You can use `requests` or `urllib3` to make requests to the local server endpoints used above. However, a popular way to use Ollama in Python is via the [openai](https://pypi.org/project/openai/) SDK, since Ollama provides OpenAI-compatible server endpoints as well.

Here is an example for phi3-mini:

```python
import openai

client = openai.OpenAI(
    base_url="http://localhost:11434/v1",
    api_key="nokeyneeded",
)

response = client.chat.completions.create(
    model="phi4",
    temperature=0.7,
    n=1,
    messages=[
        {"role": "system", "content": "Je bent een behulpzame assistent."},
        {"role": "user", "content": "Schrijf een haiku over een hongerige kat"},
    ],
)

print("Response:")
print(response.choices[0].message.content)
```

## Calling Ollama from JavaScript 

```javascript
// Voorbeeld van het samenvatten van een bestand met Phi-4
script({
    model: "ollama:phi4",
    title: "Samenvatten met Phi-4",
    system: ["system"],
})

// Voorbeeld van samenvatten
const file = def("FILE", env.files)
$`Vat ${file} samen in één alinea.`
```

## Calling Ollama from C#

Create a new C# Console application and add the following NuGet package:

```bash
dotnet add package Microsoft.SemanticKernel --version 1.34.0
```

Then replace this code in the `Program.cs` file

```csharp
using Microsoft.SemanticKernel;
using Microsoft.SemanticKernel.ChatCompletion;

// Voeg een chat completion service toe via de lokale Ollama server endpoint
#pragma warning disable SKEXP0001, SKEXP0003, SKEXP0010, SKEXP0011, SKEXP0050, SKEXP0052
builder.AddOpenAIChatCompletion(
    modelId: "phi4",
    endpoint: new Uri("http://localhost:11434/"),
    apiKey: "non required");

// Voer een eenvoudige prompt uit naar de chatservice
string prompt = "Vertel een grap over kittens";
var response = await kernel.InvokePromptAsync(prompt);
Console.WriteLine(response.GetValue<string>());
```

Run the app with the command:

```bash
dotnet run

**Disclaimer**:  
Dit document is vertaald met behulp van AI-gebaseerde vertaaldiensten. Hoewel we ons best doen om nauwkeurigheid te waarborgen, dient u zich ervan bewust te zijn dat geautomatiseerde vertalingen fouten of onnauwkeurigheden kunnen bevatten. Het originele document in zijn oorspronkelijke taal moet worden beschouwd als de gezaghebbende bron. Voor kritieke informatie wordt professionele menselijke vertaling aanbevolen. Wij zijn niet aansprakelijk voor misverstanden of verkeerde interpretaties die voortvloeien uit het gebruik van deze vertaling.