# Teknologi Utama yang Disebutkan Termasuk

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - API tingkat rendah untuk pembelajaran mesin yang dipercepat oleh perangkat keras, dibangun di atas DirectX 12.
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - platform komputasi paralel dan model antarmuka pemrograman aplikasi (API) yang dikembangkan oleh Nvidia, memungkinkan pemrosesan umum pada unit pemrosesan grafis (GPU).
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - format terbuka yang dirancang untuk merepresentasikan model pembelajaran mesin, memungkinkan interoperabilitas antara berbagai kerangka kerja ML.
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - format yang digunakan untuk merepresentasikan dan memperbarui model pembelajaran mesin, sangat berguna untuk model bahasa kecil yang dapat berjalan dengan efektif pada CPU dengan kuantisasi 4-8 bit.

## DirectML

DirectML adalah API tingkat rendah yang memungkinkan pembelajaran mesin dipercepat perangkat keras. Dibangun di atas DirectX 12 untuk memanfaatkan akselerasi GPU dan bersifat vendor-agnostik, yang berarti tidak memerlukan perubahan kode untuk bekerja di berbagai vendor GPU. DirectML terutama digunakan untuk pelatihan model dan beban kerja inferensi pada GPU.

Dalam hal dukungan perangkat keras, DirectML dirancang untuk bekerja dengan berbagai GPU, termasuk GPU terintegrasi dan diskret dari AMD, GPU terintegrasi Intel, dan GPU diskret NVIDIA. DirectML adalah bagian dari Windows AI Platform dan didukung di Windows 10 & 11, memungkinkan pelatihan model dan inferensi di perangkat Windows apa pun.

Ada pembaruan dan peluang terkait DirectML, seperti mendukung hingga 150 operator ONNX dan digunakan oleh ONNX runtime serta WinML. Teknologi ini didukung oleh vendor perangkat keras terintegrasi utama (IHV), masing-masing mengimplementasikan berbagai metakomando.

## CUDA

CUDA, singkatan dari Compute Unified Device Architecture, adalah platform komputasi paralel dan model antarmuka pemrograman aplikasi (API) yang dibuat oleh Nvidia. CUDA memungkinkan pengembang perangkat lunak menggunakan unit pemrosesan grafis (GPU) yang mendukung CUDA untuk pemrosesan tujuan umum â€“ pendekatan yang disebut GPGPU (General-Purpose computing on Graphics Processing Units). CUDA adalah pendorong utama akselerasi GPU Nvidia dan banyak digunakan di berbagai bidang, termasuk pembelajaran mesin, komputasi ilmiah, dan pemrosesan video.

Dukungan perangkat keras untuk CUDA khusus untuk GPU Nvidia, karena ini adalah teknologi kepemilikan yang dikembangkan oleh Nvidia. Setiap arsitektur mendukung versi spesifik dari toolkit CUDA, yang menyediakan pustaka dan alat yang diperlukan bagi pengembang untuk membangun dan menjalankan aplikasi CUDA.

## ONNX

ONNX (Open Neural Network Exchange) adalah format terbuka yang dirancang untuk merepresentasikan model pembelajaran mesin. ONNX menyediakan definisi model grafik komputasi yang dapat diperluas, serta definisi operator bawaan dan tipe data standar. Dengan ONNX, pengembang dapat memindahkan model antara berbagai kerangka kerja ML, memungkinkan interoperabilitas dan mempermudah pembuatan serta penerapan aplikasi AI.

Phi3 mini dapat berjalan dengan ONNX Runtime di CPU dan GPU di berbagai perangkat, termasuk platform server, desktop Windows, Linux dan Mac, serta CPU mobile. Konfigurasi yang dioptimalkan yang telah kami tambahkan meliputi:

- Model ONNX untuk int4 DML: Dikualifikasi menjadi int4 melalui AWQ
- Model ONNX untuk fp16 CUDA
- Model ONNX untuk int4 CUDA: Dikualifikasi menjadi int4 melalui RTN
- Model ONNX untuk int4 CPU dan Mobile: Dikualifikasi menjadi int4 melalui RTN

## Llama.cpp

Llama.cpp adalah pustaka perangkat lunak sumber terbuka yang ditulis dalam C++. Pustaka ini melakukan inferensi pada berbagai Model Bahasa Besar (LLM), termasuk Llama. Dikembangkan bersama pustaka ggml (pustaka tensor serbaguna), llama.cpp bertujuan untuk memberikan inferensi yang lebih cepat dan penggunaan memori yang lebih rendah dibandingkan implementasi Python asli. Pustaka ini mendukung optimasi perangkat keras, kuantisasi, serta menawarkan API sederhana dan contoh. Jika Anda tertarik dengan inferensi LLM yang efisien, llama.cpp patut dieksplorasi karena Phi3 dapat menjalankan Llama.cpp.

## GGUF

GGUF (Generic Graph Update Format) adalah format yang digunakan untuk merepresentasikan dan memperbarui model pembelajaran mesin. Format ini sangat berguna untuk model bahasa kecil (SLM) yang dapat berjalan secara efektif pada CPU dengan kuantisasi 4-8 bit. GGUF bermanfaat untuk pembuatan prototipe yang cepat dan menjalankan model di perangkat edge atau dalam tugas batch seperti pipeline CI/CD.

**Penafian**:  
Dokumen ini telah diterjemahkan menggunakan perkhidmatan terjemahan AI berasaskan mesin. Walaupun kami berusaha untuk ketepatan, sila ambil perhatian bahawa terjemahan automatik mungkin mengandungi kesilapan atau ketidaktepatan. Dokumen asal dalam bahasa asalnya harus dianggap sebagai sumber yang berwibawa. Untuk maklumat kritikal, terjemahan manusia profesional adalah disyorkan. Kami tidak bertanggungjawab atas sebarang salah faham atau salah tafsir yang timbul daripada penggunaan terjemahan ini.