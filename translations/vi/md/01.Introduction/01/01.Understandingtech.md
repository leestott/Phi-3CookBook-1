# Các công nghệ chính được đề cập bao gồm

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - một API cấp thấp cho học máy tăng tốc phần cứng, được xây dựng trên nền tảng DirectX 12.
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - một nền tảng tính toán song song và mô hình giao diện lập trình ứng dụng (API) được phát triển bởi Nvidia, cho phép xử lý đa mục đích trên GPU (đơn vị xử lý đồ họa).
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - một định dạng mở được thiết kế để biểu diễn các mô hình học máy, cung cấp khả năng tương tác giữa các framework học máy khác nhau.
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - một định dạng dùng để biểu diễn và cập nhật các mô hình học máy, đặc biệt hữu ích cho các mô hình ngôn ngữ nhỏ có thể chạy hiệu quả trên CPU với lượng tử hóa 4-8bit.

## DirectML

DirectML là một API cấp thấp cho phép học máy tăng tốc phần cứng. Nó được xây dựng trên nền tảng DirectX 12 để tận dụng khả năng tăng tốc của GPU và không phụ thuộc vào nhà cung cấp phần cứng, nghĩa là không cần thay đổi mã để hoạt động trên các nhà cung cấp GPU khác nhau. DirectML chủ yếu được sử dụng cho các tác vụ huấn luyện và suy luận mô hình trên GPU.

Về hỗ trợ phần cứng, DirectML được thiết kế để hoạt động với nhiều loại GPU, bao gồm GPU tích hợp và rời của AMD, GPU tích hợp của Intel, và GPU rời của NVIDIA. Nó là một phần của Nền tảng AI Windows và được hỗ trợ trên Windows 10 & 11, cho phép huấn luyện và suy luận mô hình trên bất kỳ thiết bị Windows nào.

Đã có các cập nhật và cơ hội liên quan đến DirectML, chẳng hạn như hỗ trợ tới 150 toán tử ONNX và được sử dụng bởi cả ONNX runtime và WinML. Nó được hỗ trợ bởi các nhà cung cấp phần cứng tích hợp lớn (IHVs), mỗi nhà cung cấp triển khai các metacommand khác nhau.

## CUDA

CUDA, viết tắt của Compute Unified Device Architecture, là một nền tảng tính toán song song và mô hình giao diện lập trình ứng dụng (API) được tạo ra bởi Nvidia. Nó cho phép các nhà phát triển phần mềm sử dụng GPU hỗ trợ CUDA để xử lý đa mục đích – một cách tiếp cận được gọi là GPGPU (Tính toán Đa mục đích trên GPU). CUDA là một yếu tố quan trọng thúc đẩy khả năng tăng tốc GPU của Nvidia và được sử dụng rộng rãi trong nhiều lĩnh vực, bao gồm học máy, tính toán khoa học, và xử lý video.

Hỗ trợ phần cứng của CUDA chỉ dành riêng cho GPU của Nvidia, vì đây là một công nghệ độc quyền do Nvidia phát triển. Mỗi kiến trúc hỗ trợ các phiên bản cụ thể của bộ công cụ CUDA, cung cấp các thư viện và công cụ cần thiết để các nhà phát triển xây dựng và chạy các ứng dụng CUDA.

## ONNX

ONNX (Open Neural Network Exchange) là một định dạng mở được thiết kế để biểu diễn các mô hình học máy. Nó cung cấp một định nghĩa về mô hình đồ thị tính toán có thể mở rộng, cũng như các định nghĩa về các toán tử tích hợp và các kiểu dữ liệu tiêu chuẩn. ONNX cho phép các nhà phát triển chuyển đổi mô hình giữa các framework học máy khác nhau, tăng khả năng tương tác và giúp dễ dàng tạo và triển khai các ứng dụng AI.

Phi3 mini có thể chạy với ONNX Runtime trên CPU và GPU trên nhiều thiết bị, bao gồm các nền tảng máy chủ, desktop Windows, Linux và Mac, cũng như CPU di động. Các cấu hình tối ưu hóa mà chúng tôi đã thêm vào bao gồm:

- Các mô hình ONNX cho int4 DML: Được lượng tử hóa thành int4 qua AWQ
- Mô hình ONNX cho fp16 CUDA
- Mô hình ONNX cho int4 CUDA: Được lượng tử hóa thành int4 qua RTN
- Mô hình ONNX cho int4 CPU và di động: Được lượng tử hóa thành int4 qua RTN

## Llama.cpp

Llama.cpp là một thư viện phần mềm mã nguồn mở được viết bằng C++. Nó thực hiện suy luận trên nhiều mô hình ngôn ngữ lớn (LLM), bao gồm Llama. Được phát triển cùng với thư viện ggml (một thư viện tensor đa năng), Llama.cpp nhằm mục đích cung cấp suy luận nhanh hơn và sử dụng bộ nhớ thấp hơn so với triển khai Python gốc. Nó hỗ trợ tối ưu hóa phần cứng, lượng tử hóa và cung cấp một API đơn giản cùng các ví dụ. Nếu bạn quan tâm đến suy luận LLM hiệu quả, Llama.cpp đáng để khám phá vì Phi3 có thể chạy Llama.cpp.

## GGUF

GGUF (Generic Graph Update Format) là một định dạng được sử dụng để biểu diễn và cập nhật các mô hình học máy. Nó đặc biệt hữu ích cho các mô hình ngôn ngữ nhỏ (SLM) có thể chạy hiệu quả trên CPU với lượng tử hóa 4-8bit. GGUF mang lại lợi ích cho việc tạo mẫu nhanh và chạy các mô hình trên các thiết bị biên hoặc trong các tác vụ theo lô như pipeline CI/CD.

**Tuyên bố miễn trừ trách nhiệm**:  
Tài liệu này đã được dịch bằng các dịch vụ dịch thuật AI tự động. Mặc dù chúng tôi cố gắng đảm bảo độ chính xác, xin lưu ý rằng các bản dịch tự động có thể chứa lỗi hoặc không chính xác. Tài liệu gốc bằng ngôn ngữ bản địa nên được coi là nguồn thông tin chính thức. Đối với các thông tin quan trọng, khuyến nghị sử dụng dịch thuật chuyên nghiệp từ con người. Chúng tôi không chịu trách nhiệm về bất kỳ sự hiểu lầm hoặc diễn giải sai nào phát sinh từ việc sử dụng bản dịch này.