# **Ποσοτικοποίηση της οικογένειας Phi χρησιμοποιώντας το llama.cpp**

## **Τι είναι το llama.cpp**

Το llama.cpp είναι μια βιβλιοθήκη ανοιχτού κώδικα γραμμένη κυρίως σε C++ που εκτελεί inference σε διάφορα Μεγάλα Γλωσσικά Μοντέλα (LLMs), όπως το Llama. Ο κύριος στόχος του είναι να προσφέρει κορυφαία απόδοση inference για LLM σε μια ευρεία γκάμα υλικού με ελάχιστη προετοιμασία. Επιπλέον, υπάρχουν διαθέσιμα Python bindings για αυτήν τη βιβλιοθήκη, τα οποία παρέχουν ένα API υψηλού επιπέδου για την ολοκλήρωση κειμένου και έναν web server συμβατό με το OpenAI.

Ο κύριος στόχος του llama.cpp είναι να επιτρέψει inference LLM με ελάχιστη προετοιμασία και κορυφαία απόδοση σε μια μεγάλη ποικιλία υλικού - τοπικά και στο cloud.

- Υλοποίηση σε απλή C/C++ χωρίς εξαρτήσεις
- Το Apple silicon είναι προτεραιότητα - βελτιστοποιημένο μέσω των πλαισίων ARM NEON, Accelerate και Metal
- Υποστήριξη AVX, AVX2 και AVX512 για αρχιτεκτονικές x86
- Ποσοτικοποίηση ακεραίων 1.5-bit, 2-bit, 3-bit, 4-bit, 5-bit, 6-bit και 8-bit για ταχύτερο inference και μειωμένη χρήση μνήμης
- Προσαρμοσμένοι πυρήνες CUDA για εκτέλεση LLMs σε NVIDIA GPUs (υποστήριξη για AMD GPUs μέσω HIP)
- Υποστήριξη backend Vulkan και SYCL
- Υβριδικό inference CPU+GPU για μερική επιτάχυνση μοντέλων μεγαλύτερων από τη συνολική χωρητικότητα VRAM

## **Ποσοτικοποίηση του Phi-3.5 με το llama.cpp**

Το μοντέλο Phi-3.5-Instruct μπορεί να ποσοτικοποιηθεί χρησιμοποιώντας το llama.cpp, αλλά τα Phi-3.5-Vision και Phi-3.5-MoE δεν υποστηρίζονται ακόμα. Η μορφή που μετατρέπεται από το llama.cpp είναι το gguf, το οποίο είναι επίσης η πιο διαδεδομένη μορφή ποσοτικοποίησης.

Υπάρχει μεγάλος αριθμός ποσοτικοποιημένων μοντέλων σε μορφή GGUF στο Hugging Face. Το AI Foundry, το Ollama και το LlamaEdge βασίζονται στο llama.cpp, οπότε τα μοντέλα GGUF χρησιμοποιούνται συχνά.

### **Τι είναι το GGUF**

Το GGUF είναι μια δυαδική μορφή που είναι βελτιστοποιημένη για γρήγορη φόρτωση και αποθήκευση μοντέλων, καθιστώντας την εξαιρετικά αποδοτική για σκοπούς inference. Το GGUF έχει σχεδιαστεί για χρήση με το GGML και άλλους εκτελεστές. Το GGUF αναπτύχθηκε από τον @ggerganov, ο οποίος είναι επίσης ο δημιουργός του llama.cpp, ενός δημοφιλούς πλαισίου inference LLM σε C/C++. Μοντέλα που αναπτύχθηκαν αρχικά σε πλαίσια όπως το PyTorch μπορούν να μετατραπούν σε μορφή GGUF για χρήση με αυτές τις μηχανές.

### **ONNX vs GGUF**

Το ONNX είναι μια παραδοσιακή μορφή μηχανικής/βαθιάς μάθησης, η οποία υποστηρίζεται καλά από διαφορετικά AI πλαίσια και έχει καλές περιπτώσεις χρήσης σε συσκευές αιχμής. Όσον αφορά το GGUF, βασίζεται στο llama.cpp και μπορεί να ειπωθεί ότι είναι προϊόν της εποχής του GenAI. Τα δύο έχουν παρόμοιες χρήσεις. Αν θέλετε καλύτερη απόδοση σε ενσωματωμένο υλικό και επίπεδα εφαρμογών, το ONNX μπορεί να είναι η επιλογή σας. Αν χρησιμοποιείτε το παράγωγο πλαίσιο και την τεχνολογία του llama.cpp, τότε το GGUF μπορεί να είναι καλύτερο.

### **Ποσοτικοποίηση του Phi-3.5-Instruct χρησιμοποιώντας το llama.cpp**

**1. Διαμόρφωση Περιβάλλοντος**

```bash

git clone https://github.com/ggerganov/llama.cpp.git

cd llama.cpp

make -j8

```

**2. Ποσοτικοποίηση**

Χρησιμοποιώντας το llama.cpp για να μετατρέψετε το Phi-3.5-Instruct σε FP16 GGUF

```bash

./convert_hf_to_gguf.py <Your Phi-3.5-Instruct Location> --outfile phi-3.5-128k-mini_fp16.gguf

```

Ποσοτικοποίηση του Phi-3.5 σε INT4

```bash

./llama.cpp/llama-quantize <Your phi-3.5-128k-mini_fp16.gguf location> ./gguf/phi-3.5-128k-mini_Q4_K_M.gguf Q4_K_M

```

**3. Δοκιμή**

Εγκατάσταση του llama-cpp-python

```bash

pip install llama-cpp-python -U

```

***Σημείωση***

Αν χρησιμοποιείτε Apple Silicon, εγκαταστήστε το llama-cpp-python ως εξής

```bash

CMAKE_ARGS="-DLLAMA_METAL=on" pip install llama-cpp-python -U

```

Δοκιμή

```bash

llama.cpp/llama-cli --model <Your phi-3.5-128k-mini_Q4_K_M.gguf location> --prompt "<|user|>\nCan you introduce .NET<|end|>\n<|assistant|>\n"  --gpu-layers 10

```

## **Πηγές**

1. Μάθετε περισσότερα για το llama.cpp [https://onnxruntime.ai/docs/genai/](https://onnxruntime.ai/docs/genai/)

2. Μάθετε περισσότερα για το GGUF [https://huggingface.co/docs/hub/en/gguf](https://huggingface.co/docs/hub/en/gguf)

**Αποποίηση Ευθυνών**:  
Αυτό το έγγραφο έχει μεταφραστεί χρησιμοποιώντας υπηρεσίες μετάφρασης που βασίζονται σε τεχνητή νοημοσύνη. Ενώ καταβάλλουμε προσπάθειες για ακρίβεια, παρακαλούμε να έχετε υπόψη σας ότι οι αυτοματοποιημένες μεταφράσεις ενδέχεται να περιέχουν λάθη ή ανακρίβειες. Το πρωτότυπο έγγραφο στη μητρική του γλώσσα θα πρέπει να θεωρείται η αυθεντική πηγή. Για κρίσιμες πληροφορίες, συνιστάται επαγγελματική ανθρώπινη μετάφραση. Δεν φέρουμε ευθύνη για τυχόν παρανοήσεις ή παρερμηνείες που προκύπτουν από τη χρήση αυτής της μετάφρασης.