# **Ποσοτικοποίηση της Οικογένειας Phi**

Η ποσοτικοποίηση μοντέλων αναφέρεται στη διαδικασία χαρτογράφησης των παραμέτρων (όπως βάρη και τιμές ενεργοποίησης) ενός μοντέλου νευρωνικού δικτύου από ένα μεγάλο εύρος τιμών (συνήθως συνεχές) σε ένα μικρότερο πεπερασμένο εύρος τιμών. Αυτή η τεχνολογία μπορεί να μειώσει το μέγεθος και την υπολογιστική πολυπλοκότητα του μοντέλου και να βελτιώσει την αποδοτικότητα λειτουργίας του σε περιβάλλοντα με περιορισμένους πόρους, όπως κινητές συσκευές ή ενσωματωμένα συστήματα. Η ποσοτικοποίηση μοντέλων επιτυγχάνει συμπίεση μειώνοντας την ακρίβεια των παραμέτρων, αλλά εισάγει και μια συγκεκριμένη απώλεια ακρίβειας. Επομένως, στη διαδικασία ποσοτικοποίησης, είναι απαραίτητο να ισορροπηθεί το μέγεθος του μοντέλου, η υπολογιστική πολυπλοκότητα και η ακρίβεια. Συνήθεις μέθοδοι ποσοτικοποίησης περιλαμβάνουν την ποσοτικοποίηση σταθερής ακρίβειας, την ποσοτικοποίηση κινητής ακρίβειας κ.λπ. Μπορείτε να επιλέξετε την κατάλληλη στρατηγική ποσοτικοποίησης ανάλογα με το συγκεκριμένο σενάριο και τις ανάγκες.

Επιδιώκουμε να αναπτύξουμε το μοντέλο GenAI σε edge συσκευές και να επιτρέψουμε σε περισσότερες συσκευές να εισέλθουν σε σενάρια GenAI, όπως κινητές συσκευές, AI PC/Copilot+PC και παραδοσιακές IoT συσκευές. Μέσω του ποσοτικοποιημένου μοντέλου, μπορούμε να το αναπτύξουμε σε διαφορετικές edge συσκευές με βάση τις ιδιαιτερότητές τους. Σε συνδυασμό με το πλαίσιο επιτάχυνσης μοντέλου και το ποσοτικοποιημένο μοντέλο που παρέχεται από τους κατασκευαστές υλικού, μπορούμε να δημιουργήσουμε καλύτερα σενάρια εφαρμογής SLM.

Στο σενάριο ποσοτικοποίησης, έχουμε διαφορετικές ακρίβειες (INT4, INT8, FP16, FP32). Ακολουθεί μια επεξήγηση των πιο συχνά χρησιμοποιούμενων ακρίβειων ποσοτικοποίησης.

### **INT4**

Η ποσοτικοποίηση INT4 είναι μια ριζοσπαστική μέθοδος ποσοτικοποίησης που ποσοτικοποιεί τα βάρη και τις τιμές ενεργοποίησης του μοντέλου σε ακέραιους αριθμούς 4 bit. Η ποσοτικοποίηση INT4 συνήθως οδηγεί σε μεγαλύτερη απώλεια ακρίβειας λόγω του μικρότερου εύρους αναπαράστασης και της χαμηλότερης ακρίβειας. Ωστόσο, σε σύγκριση με την ποσοτικοποίηση INT8, η ποσοτικοποίηση INT4 μπορεί να μειώσει περαιτέρω τις απαιτήσεις αποθήκευσης και την υπολογιστική πολυπλοκότητα του μοντέλου. Πρέπει να σημειωθεί ότι η ποσοτικοποίηση INT4 είναι σχετικά σπάνια στις πρακτικές εφαρμογές, καθώς η πολύ χαμηλή ακρίβεια μπορεί να προκαλέσει σημαντική υποβάθμιση της απόδοσης του μοντέλου. Επιπλέον, δεν υποστηρίζουν όλα τα υλικά τις λειτουργίες INT4, οπότε πρέπει να ληφθεί υπόψη η συμβατότητα του υλικού κατά την επιλογή της μεθόδου ποσοτικοποίησης.

### **INT8**

Η ποσοτικοποίηση INT8 είναι η διαδικασία μετατροπής των βαρών και των ενεργοποιήσεων ενός μοντέλου από αριθμούς κινητής υποδιαστολής σε ακέραιους αριθμούς 8 bit. Παρόλο που το αριθμητικό εύρος που εκπροσωπείται από τους ακέραιους INT8 είναι μικρότερο και λιγότερο ακριβές, μπορεί να μειώσει σημαντικά τις απαιτήσεις αποθήκευσης και υπολογισμών. Στην ποσοτικοποίηση INT8, τα βάρη και οι τιμές ενεργοποίησης του μοντέλου περνούν από μια διαδικασία ποσοτικοποίησης, συμπεριλαμβανομένης της κλιμάκωσης και της μετατόπισης, για να διατηρηθεί όσο το δυνατόν περισσότερο η αρχική πληροφορία κινητής υποδιαστολής. Κατά την πρόβλεψη, αυτές οι ποσοτικοποιημένες τιμές αποποσοτικοποιούνται πίσω σε αριθμούς κινητής υποδιαστολής για υπολογισμό και στη συνέχεια ποσοτικοποιούνται ξανά σε INT8 για το επόμενο βήμα. Αυτή η μέθοδος μπορεί να παρέχει επαρκή ακρίβεια στις περισσότερες εφαρμογές διατηρώντας παράλληλα υψηλή υπολογιστική αποδοτικότητα.

### **FP16**

Η μορφή FP16, δηλαδή αριθμοί κινητής υποδιαστολής 16 bit (float16), μειώνει το αποτύπωμα μνήμης στο μισό σε σύγκριση με τους αριθμούς κινητής υποδιαστολής 32 bit (float32), κάτι που έχει σημαντικά πλεονεκτήματα σε εφαρμογές βαθιάς μάθησης μεγάλης κλίμακας. Η μορφή FP16 επιτρέπει τη φόρτωση μεγαλύτερων μοντέλων ή την επεξεργασία περισσότερων δεδομένων εντός των ίδιων περιορισμών μνήμης GPU. Καθώς το σύγχρονο υλικό GPU συνεχίζει να υποστηρίζει λειτουργίες FP16, η χρήση της μορφής FP16 μπορεί επίσης να φέρει βελτιώσεις στην ταχύτητα υπολογισμών. Ωστόσο, η μορφή FP16 έχει επίσης τα εγγενή της μειονεκτήματα, δηλαδή τη χαμηλότερη ακρίβεια, η οποία μπορεί να οδηγήσει σε αριθμητική αστάθεια ή απώλεια ακρίβειας σε ορισμένες περιπτώσεις.

### **FP32**

Η μορφή FP32 παρέχει υψηλότερη ακρίβεια και μπορεί να αναπαραστήσει με ακρίβεια ένα ευρύ φάσμα τιμών. Σε σενάρια όπου πραγματοποιούνται σύνθετες μαθηματικές πράξεις ή απαιτούνται αποτελέσματα υψηλής ακρίβειας, προτιμάται η μορφή FP32. Ωστόσο, η υψηλή ακρίβεια σημαίνει επίσης μεγαλύτερη χρήση μνήμης και μεγαλύτερο χρόνο υπολογισμού. Για μοντέλα βαθιάς μάθησης μεγάλης κλίμακας, ειδικά όταν υπάρχουν πολλές παράμετροι μοντέλου και τεράστιος όγκος δεδομένων, η μορφή FP32 μπορεί να προκαλέσει έλλειψη μνήμης GPU ή μείωση της ταχύτητας πρόβλεψης.

Σε κινητές συσκευές ή IoT συσκευές, μπορούμε να μετατρέψουμε τα μοντέλα Phi-3.x σε INT4, ενώ τα AI PC / Copilot PC μπορούν να χρησιμοποιήσουν υψηλότερη ακρίβεια, όπως INT8, FP16, FP32.

Προς το παρόν, διαφορετικοί κατασκευαστές υλικού έχουν διαφορετικά πλαίσια για την υποστήριξη γεννητικών μοντέλων, όπως το OpenVINO της Intel, το QNN της Qualcomm, το MLX της Apple και το CUDA της Nvidia, τα οποία, σε συνδυασμό με την ποσοτικοποίηση μοντέλων, ολοκληρώνουν την τοπική ανάπτυξη.

Σε τεχνικό επίπεδο, έχουμε διαφορετική υποστήριξη μορφών μετά την ποσοτικοποίηση, όπως οι μορφές PyTorch / Tensorflow, GGUF και ONNX. Έχω κάνει μια σύγκριση μορφών και σενάρια εφαρμογής μεταξύ GGUF και ONNX. Εδώ προτείνω τη μορφή ποσοτικοποίησης ONNX, η οποία έχει καλή υποστήριξη από το πλαίσιο μοντέλου έως το υλικό. Σε αυτό το κεφάλαιο, θα επικεντρωθούμε στο ONNX Runtime για GenAI, το OpenVINO και το Apple MLX για την εκτέλεση ποσοτικοποίησης μοντέλου (αν έχετε έναν καλύτερο τρόπο, μπορείτε επίσης να μας τον δώσετε υποβάλλοντας PR).

**Αυτό το κεφάλαιο περιλαμβάνει**

1. [Ποσοτικοποίηση Phi-3.5 / 4 χρησιμοποιώντας το llama.cpp](./UsingLlamacppQuantifyingPhi.md)

2. [Ποσοτικοποίηση Phi-3.5 / 4 χρησιμοποιώντας τις επεκτάσεις Generative AI για το onnxruntime](./UsingORTGenAIQuantifyingPhi.md)

3. [Ποσοτικοποίηση Phi-3.5 / 4 χρησιμοποιώντας το Intel OpenVINO](./UsingIntelOpenVINOQuantifyingPhi.md)

4. [Ποσοτικοποίηση Phi-3.5 / 4 χρησιμοποιώντας το Apple MLX Framework](./UsingAppleMLXQuantifyingPhi.md)

**Αποποίηση ευθυνών**:  
Αυτό το έγγραφο έχει μεταφραστεί χρησιμοποιώντας υπηρεσίες αυτόματης μετάφρασης με τεχνητή νοημοσύνη. Ενώ καταβάλλουμε προσπάθειες για ακρίβεια, παρακαλούμε να γνωρίζετε ότι οι αυτόματες μεταφράσεις ενδέχεται να περιέχουν λάθη ή ανακρίβειες. Το πρωτότυπο έγγραφο στη γλώσσα του θα πρέπει να θεωρείται η αυθεντική πηγή. Για κρίσιμες πληροφορίες, συνιστάται επαγγελματική ανθρώπινη μετάφραση. Δεν φέρουμε ευθύνη για τυχόν παρεξηγήσεις ή εσφαλμένες ερμηνείες που προκύπτουν από τη χρήση αυτής της μετάφρασης.