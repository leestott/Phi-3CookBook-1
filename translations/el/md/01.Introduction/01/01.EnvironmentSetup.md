# Ξεκινήστε με το Phi-3 τοπικά

Αυτός ο οδηγός θα σας βοηθήσει να ρυθμίσετε το τοπικό σας περιβάλλον για να εκτελέσετε το μοντέλο Phi-3 χρησιμοποιώντας το Ollama. Μπορείτε να εκτελέσετε το μοντέλο με διάφορους τρόπους, όπως χρησιμοποιώντας το GitHub Codespaces, τα Dev Containers του VS Code ή το τοπικό σας περιβάλλον.

## Ρύθμιση περιβάλλοντος

### GitHub Codespaces

Μπορείτε να εκτελέσετε αυτό το πρότυπο εικονικά χρησιμοποιώντας το GitHub Codespaces. Το κουμπί θα ανοίξει μια διαδικτυακή έκδοση του VS Code στο πρόγραμμα περιήγησής σας:

1. Ανοίξτε το πρότυπο (αυτό μπορεί να πάρει αρκετά λεπτά):

    [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/microsoft/phi-3cookbook)

2. Ανοίξτε ένα παράθυρο τερματικού.

### Dev Containers του VS Code

⚠️ Αυτή η επιλογή θα λειτουργήσει μόνο εάν το Docker Desktop σας έχει τουλάχιστον 16 GB RAM. Εάν έχετε λιγότερη από 16 GB RAM, μπορείτε να δοκιμάσετε την επιλογή [GitHub Codespaces](../../../../../md/01.Introduction/01) ή να [ρυθμίσετε το περιβάλλον τοπικά](../../../../../md/01.Introduction/01).

Μια σχετική επιλογή είναι τα Dev Containers του VS Code, που θα ανοίξουν το έργο στο τοπικό σας VS Code χρησιμοποιώντας την επέκταση [Dev Containers](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers):

1. Ξεκινήστε το Docker Desktop (εγκαταστήστε το αν δεν το έχετε ήδη εγκαταστήσει).
2. Ανοίξτε το έργο:

    [![Open in Dev Containers](https://img.shields.io/static/v1?style=for-the-badge&label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/microsoft/phi-3cookbook)

3. Στο παράθυρο του VS Code που ανοίγει, μόλις εμφανιστούν τα αρχεία του έργου (αυτό μπορεί να πάρει αρκετά λεπτά), ανοίξτε ένα παράθυρο τερματικού.
4. Συνεχίστε με τα [βήματα ανάπτυξης](../../../../../md/01.Introduction/01).

### Τοπικό περιβάλλον

1. Βεβαιωθείτε ότι έχετε εγκαταστήσει τα παρακάτω εργαλεία:

    * [Ollama](https://ollama.com/)
    * [Python 3.10+](https://www.python.org/downloads/)
    * [OpenAI Python SDK](https://pypi.org/project/openai/)

## Δοκιμή του μοντέλου

1. Ζητήστε από το Ollama να κατεβάσει και να εκτελέσει το μοντέλο phi3:mini:

    ```shell
    ollama run phi3:mini
    ```

    Αυτό θα πάρει μερικά λεπτά για να κατεβάσει το μοντέλο.

2. Μόλις δείτε "success" στην έξοδο, μπορείτε να στείλετε ένα μήνυμα σε αυτό το μοντέλο από το prompt.

    ```shell
    >>> Write a haiku about hungry hippos
    ```

3. Μετά από μερικά δευτερόλεπτα, θα πρέπει να δείτε μια ροή απόκρισης από το μοντέλο.

4. Για να μάθετε σχετικά με διαφορετικές τεχνικές που χρησιμοποιούνται με γλωσσικά μοντέλα, ανοίξτε το Python notebook [ollama.ipynb](../../../../../code/01.Introduce/ollama.ipynb) και εκτελέστε κάθε κελί. Εάν χρησιμοποιήσατε ένα μοντέλο διαφορετικό από το 'phi3:mini', αλλάξτε το `MODEL_NAME` in the first cell.

5. To have a conversation with the phi3:mini model from Python, open the Python file [chat.py](../../../../../code/01.Introduce/chat.py) and run it. You can change the `MODEL_NAME` στην κορυφή του αρχείου όπως απαιτείται, και μπορείτε επίσης να τροποποιήσετε το μήνυμα συστήματος ή να προσθέσετε παραδείγματα few-shot αν το επιθυμείτε.

**Αποποίηση ευθυνών**:  
Αυτό το έγγραφο έχει μεταφραστεί χρησιμοποιώντας υπηρεσίες μηχανικής μετάφρασης με τεχνητή νοημοσύνη. Ενώ καταβάλλουμε προσπάθειες για ακρίβεια, παρακαλούμε να έχετε υπόψη ότι οι αυτοματοποιημένες μεταφράσεις ενδέχεται να περιέχουν λάθη ή ανακρίβειες. Το πρωτότυπο έγγραφο στη μητρική του γλώσσα θα πρέπει να θεωρείται η έγκυρη πηγή. Για κρίσιμες πληροφορίες, συνιστάται επαγγελματική ανθρώπινη μετάφραση. Δεν φέρουμε ευθύνη για τυχόν παρεξηγήσεις ή παρερμηνείες που προκύπτουν από τη χρήση αυτής της μετάφρασης.