# Οι βασικές τεχνολογίες που αναφέρονται περιλαμβάνουν

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - ένα χαμηλού επιπέδου API για επιτάχυνση μηχανικής μάθησης μέσω υλικού, βασισμένο στο DirectX 12.
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - μια πλατφόρμα παράλληλου υπολογισμού και μοντέλο διεπαφής προγραμματισμού εφαρμογών (API) που αναπτύχθηκε από τη Nvidia, επιτρέποντας γενικούς υπολογισμούς σε μονάδες γραφικών (GPUs).
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - μια ανοιχτή μορφή σχεδιασμένη να αναπαριστά μοντέλα μηχανικής μάθησης, προσφέροντας διαλειτουργικότητα μεταξύ διαφορετικών πλαισίων ML.
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - μια μορφή που χρησιμοποιείται για την αναπαράσταση και την ενημέρωση μοντέλων μηχανικής μάθησης, ιδιαίτερα χρήσιμη για μικρότερα μοντέλα γλώσσας που μπορούν να λειτουργήσουν αποδοτικά σε CPUs με ποσοτικοποίηση 4-8bit.

## DirectML

Το DirectML είναι ένα χαμηλού επιπέδου API που επιτρέπει τη μηχανική μάθηση με επιτάχυνση μέσω υλικού. Είναι βασισμένο στο DirectX 12 για να αξιοποιεί την επιτάχυνση από GPUs και είναι ανεξάρτητο από τον προμηθευτή, πράγμα που σημαίνει ότι δεν απαιτούνται αλλαγές στον κώδικα για να λειτουργήσει με διαφορετικούς προμηθευτές GPU. Χρησιμοποιείται κυρίως για εκπαίδευση μοντέλων και inferencing σε GPUs.

Όσον αφορά την υποστήριξη υλικού, το DirectML έχει σχεδιαστεί να λειτουργεί με μια ευρεία γκάμα GPUs, συμπεριλαμβανομένων των ενσωματωμένων και διακριτών GPUs της AMD, των ενσωματωμένων GPUs της Intel και των διακριτών GPUs της NVIDIA. Είναι μέρος της πλατφόρμας Windows AI και υποστηρίζεται στα Windows 10 & 11, επιτρέποντας την εκπαίδευση και το inferencing μοντέλων σε οποιαδήποτε συσκευή Windows.

Υπήρξαν ενημερώσεις και ευκαιρίες που σχετίζονται με το DirectML, όπως η υποστήριξη έως και 150 λειτουργιών ONNX και η χρήση του τόσο από το ONNX runtime όσο και από το WinML. Υποστηρίζεται από μεγάλους προμηθευτές ενσωματωμένου υλικού (IHVs), οι οποίοι υλοποιούν διάφορες μετα-εντολές.

## CUDA

Η CUDA, που σημαίνει Compute Unified Device Architecture, είναι μια πλατφόρμα παράλληλου υπολογισμού και μοντέλο διεπαφής προγραμματισμού εφαρμογών (API) που δημιουργήθηκε από τη Nvidia. Επιτρέπει στους προγραμματιστές λογισμικού να χρησιμοποιούν μια GPU με δυνατότητα CUDA για γενικούς υπολογισμούς – μια προσέγγιση που ονομάζεται GPGPU (General-Purpose computing on Graphics Processing Units). Η CUDA αποτελεί βασικό στοιχείο της επιτάχυνσης GPU της Nvidia και χρησιμοποιείται ευρέως σε διάφορους τομείς, όπως η μηχανική μάθηση, οι επιστημονικοί υπολογισμοί και η επεξεργασία βίντεο.

Η υποστήριξη υλικού για την CUDA είναι συγκεκριμένη για τις GPUs της Nvidia, καθώς είναι μια ιδιόκτητη τεχνολογία που αναπτύχθηκε από τη Nvidia. Κάθε αρχιτεκτονική υποστηρίζει συγκεκριμένες εκδόσεις του εργαλείου CUDA toolkit, το οποίο παρέχει τις απαραίτητες βιβλιοθήκες και εργαλεία για τους προγραμματιστές να δημιουργούν και να εκτελούν εφαρμογές CUDA.

## ONNX

Το ONNX (Open Neural Network Exchange) είναι μια ανοιχτή μορφή σχεδιασμένη να αναπαριστά μοντέλα μηχανικής μάθησης. Παρέχει έναν ορισμό ενός επεκτάσιμου μοντέλου γραφήματος υπολογισμών, καθώς και ορισμούς ενσωματωμένων λειτουργιών και τυπικών τύπων δεδομένων. Το ONNX επιτρέπει στους προγραμματιστές να μεταφέρουν μοντέλα μεταξύ διαφορετικών πλαισίων ML, διευκολύνοντας τη διαλειτουργικότητα και καθιστώντας ευκολότερη τη δημιουργία και ανάπτυξη εφαρμογών AI.

Το Phi3 mini μπορεί να λειτουργήσει με το ONNX Runtime σε CPU και GPU σε διάφορες συσκευές, όπως πλατφόρμες server, desktops με Windows, Linux και Mac, καθώς και κινητές CPUs. Οι βελτιστοποιημένες διαμορφώσεις που έχουμε προσθέσει περιλαμβάνουν:

- Μοντέλα ONNX για int4 DML: Ποσοτικοποιημένα σε int4 μέσω AWQ
- Μοντέλο ONNX για fp16 CUDA
- Μοντέλο ONNX για int4 CUDA: Ποσοτικοποιημένο σε int4 μέσω RTN
- Μοντέλο ONNX για int4 CPU και Mobile: Ποσοτικοποιημένο σε int4 μέσω RTN

## Llama.cpp

Το Llama.cpp είναι μια βιβλιοθήκη λογισμικού ανοιχτού κώδικα γραμμένη σε C++. Εκτελεί inferencing σε διάφορα Large Language Models (LLMs), συμπεριλαμβανομένου του Llama. Αναπτύχθηκε παράλληλα με τη βιβλιοθήκη ggml (μια γενικής χρήσης βιβλιοθήκη τανυστών), και το llama.cpp στοχεύει να προσφέρει ταχύτερο inferencing και χαμηλότερη χρήση μνήμης σε σύγκριση με την αρχική υλοποίηση σε Python. Υποστηρίζει βελτιστοποίηση υλικού, ποσοτικοποίηση, και παρέχει ένα απλό API και παραδείγματα. Αν σας ενδιαφέρει το αποδοτικό inferencing LLM, αξίζει να εξερευνήσετε το llama.cpp, καθώς το Phi3 μπορεί να λειτουργήσει με αυτό.

## GGUF

Το GGUF (Generic Graph Update Format) είναι μια μορφή που χρησιμοποιείται για την αναπαράσταση και την ενημέρωση μοντέλων μηχανικής μάθησης. Είναι ιδιαίτερα χρήσιμη για μικρότερα μοντέλα γλώσσας (SLMs) που μπορούν να λειτουργήσουν αποδοτικά σε CPUs με ποσοτικοποίηση 4-8bit. Το GGUF είναι ωφέλιμο για γρήγορη ανάπτυξη πρωτοτύπων και την εκτέλεση μοντέλων σε edge συσκευές ή σε παρτίδες εργασιών όπως οι αγωγοί CI/CD.

**Αποποίηση Ευθύνης**:  
Αυτό το έγγραφο έχει μεταφραστεί χρησιμοποιώντας υπηρεσίες αυτόματης μετάφρασης με τεχνητή νοημοσύνη. Παρόλο που καταβάλλουμε προσπάθειες για ακρίβεια, παρακαλούμε να έχετε υπόψη σας ότι οι αυτοματοποιημένες μεταφράσεις ενδέχεται να περιέχουν λάθη ή ανακρίβειες. Το αρχικό έγγραφο στη μητρική του γλώσσα θα πρέπει να θεωρείται η έγκυρη πηγή. Για κρίσιμες πληροφορίες, συνιστάται επαγγελματική ανθρώπινη μετάφραση. Δεν φέρουμε καμία ευθύνη για τυχόν παρεξηγήσεις ή εσφαλμένες ερμηνείες που προκύπτουν από τη χρήση αυτής της μετάφρασης.