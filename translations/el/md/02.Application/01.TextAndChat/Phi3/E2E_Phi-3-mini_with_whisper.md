# Διαδραστικός Chatbot Phi 3 Mini 4K Instruct με Whisper

## Επισκόπηση

Ο Διαδραστικός Chatbot Phi 3 Mini 4K Instruct είναι ένα εργαλείο που επιτρέπει στους χρήστες να αλληλεπιδρούν με την επίδειξη Microsoft Phi 3 Mini 4K Instruct μέσω κειμένου ή φωνητικής εισόδου. Το chatbot μπορεί να χρησιμοποιηθεί για διάφορες εργασίες, όπως μετάφραση, ενημερώσεις καιρού και γενική συλλογή πληροφοριών.

### Ξεκινώντας

Για να χρησιμοποιήσετε αυτό το chatbot, ακολουθήστε τις παρακάτω οδηγίες:

1. Ανοίξτε το [E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb](https://github.com/microsoft/Phi-3CookBook/blob/main/code/06.E2E/E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb)
2. Στο κύριο παράθυρο του notebook, θα δείτε μια διεπαφή συνομιλίας με ένα πεδίο εισαγωγής κειμένου και ένα κουμπί "Send".
3. Για να χρησιμοποιήσετε το chatbot μέσω κειμένου, απλά πληκτρολογήστε το μήνυμά σας στο πεδίο εισαγωγής κειμένου και κάντε κλικ στο κουμπί "Send". Το chatbot θα απαντήσει με ένα αρχείο ήχου που μπορείτε να ακούσετε απευθείας μέσα από το notebook.

**Σημείωση**: Αυτό το εργαλείο απαιτεί GPU και πρόσβαση στα μοντέλα Microsoft Phi-3 και OpenAI Whisper, τα οποία χρησιμοποιούνται για αναγνώριση ομιλίας και μετάφραση.

### Απαιτήσεις GPU

Για να εκτελέσετε αυτή την επίδειξη, χρειάζεστε 12GB μνήμης GPU.

Οι απαιτήσεις μνήμης για την εκτέλεση της επίδειξης **Microsoft-Phi-3-Mini-4K instruct** σε GPU εξαρτώνται από διάφορους παράγοντες, όπως το μέγεθος των δεδομένων εισόδου (ήχος ή κείμενο), η γλώσσα που χρησιμοποιείται για μετάφραση, η ταχύτητα του μοντέλου και η διαθέσιμη μνήμη στη GPU.

Γενικά, το μοντέλο Whisper έχει σχεδιαστεί για να εκτελείται σε GPU. Η προτεινόμενη ελάχιστη μνήμη GPU για την εκτέλεση του μοντέλου Whisper είναι 8 GB, αλλά μπορεί να χρησιμοποιήσει μεγαλύτερη μνήμη αν χρειαστεί.

Είναι σημαντικό να σημειωθεί ότι η εκτέλεση μεγάλου όγκου δεδομένων ή αιτημάτων στο μοντέλο μπορεί να απαιτεί περισσότερη μνήμη GPU και/ή να προκαλέσει προβλήματα απόδοσης. Συνιστάται να δοκιμάσετε τη χρήση σας με διαφορετικές ρυθμίσεις και να παρακολουθείτε τη χρήση μνήμης για να καθορίσετε τις βέλτιστες ρυθμίσεις για τις ανάγκες σας.

## Παράδειγμα E2E για τον Διαδραστικό Chatbot Phi 3 Mini 4K Instruct με Whisper

Το jupyter notebook με τίτλο [Interactive Phi 3 Mini 4K Instruct Chatbot with Whisper](https://github.com/microsoft/Phi-3CookBook/blob/main/code/06.E2E/E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb) δείχνει πώς να χρησιμοποιήσετε την επίδειξη Microsoft Phi 3 Mini 4K Instruct για να δημιουργήσετε κείμενο από φωνητική ή γραπτή είσοδο. Το notebook ορίζει αρκετές λειτουργίες:

1. `tts_file_name(text)`: Αυτή η λειτουργία δημιουργεί ένα όνομα αρχείου βασισμένο στο κείμενο εισόδου για την αποθήκευση του παραγόμενου αρχείου ήχου.
1. `edge_free_tts(chunks_list,speed,voice_name,save_path)`: Αυτή η λειτουργία χρησιμοποιεί το Edge TTS API για να δημιουργήσει ένα αρχείο ήχου από μια λίστα κομματιών κειμένου εισόδου. Οι παράμετροι εισόδου είναι η λίστα κομματιών, η ταχύτητα ομιλίας, το όνομα φωνής και η διαδρομή εξόδου για την αποθήκευση του παραγόμενου αρχείου ήχου.
1. `talk(input_text)`: Αυτή η λειτουργία δημιουργεί ένα αρχείο ήχου χρησιμοποιώντας το Edge TTS API και το αποθηκεύει με ένα τυχαίο όνομα αρχείου στον κατάλογο /content/audio. Η παράμετρος εισόδου είναι το κείμενο που θα μετατραπεί σε ομιλία.
1. `run_text_prompt(message, chat_history)`: Αυτή η λειτουργία χρησιμοποιεί την επίδειξη Microsoft Phi 3 Mini 4K Instruct για να δημιουργήσει ένα αρχείο ήχου από ένα μήνυμα εισόδου και το προσθέτει στο ιστορικό συνομιλιών.
1. `run_audio_prompt(audio, chat_history)`: Αυτή η λειτουργία μετατρέπει ένα αρχείο ήχου σε κείμενο χρησιμοποιώντας το Whisper model API και το περνά στη λειτουργία `run_text_prompt()`.
1. Ο κώδικας εκκινεί μια εφαρμογή Gradio που επιτρέπει στους χρήστες να αλληλεπιδρούν με την επίδειξη Phi 3 Mini 4K Instruct είτε πληκτρολογώντας μηνύματα είτε ανεβάζοντας αρχεία ήχου. Η έξοδος εμφανίζεται ως μήνυμα κειμένου μέσα στην εφαρμογή.

## Επίλυση Προβλημάτων

Εγκατάσταση οδηγών GPU Cuda

1. Βεβαιωθείτε ότι οι εφαρμογές Linux σας είναι ενημερωμένες

    ```bash
    sudo apt update
    ```

1. Εγκαταστήστε τους οδηγούς Cuda

    ```bash
    sudo apt install nvidia-cuda-toolkit
    ```

1. Καταχωρίστε τη θέση του οδηγού cuda

    ```bash
    echo /usr/lib64-nvidia/ >/etc/ld.so.conf.d/libcuda.conf; ldconfig
    ```

1. Έλεγχος μεγέθους μνήμης Nvidia GPU (Απαιτούνται 12GB μνήμης GPU)

    ```bash
    nvidia-smi
    ```

1. Άδειασμα Cache: Αν χρησιμοποιείτε PyTorch, μπορείτε να καλέσετε την torch.cuda.empty_cache() για να απελευθερώσετε όλη την αχρησιμοποίητη μνήμη cache ώστε να μπορεί να χρησιμοποιηθεί από άλλες εφαρμογές GPU.

    ```python
    torch.cuda.empty_cache() 
    ```

1. Έλεγχος Nvidia Cuda

    ```bash
    nvcc --version
    ```

1. Εκτελέστε τα παρακάτω βήματα για να δημιουργήσετε ένα Hugging Face token.

    - Μεταβείτε στη [σελίδα Ρυθμίσεων Token Hugging Face](https://huggingface.co/settings/tokens?WT.mc_id=aiml-137032-kinfeylo).
    - Επιλέξτε **New token**.
    - Εισάγετε το **Όνομα** του έργου που θέλετε να χρησιμοποιήσετε.
    - Επιλέξτε **Type** σε **Write**.

> **Σημείωση**
>
> Αν αντιμετωπίσετε το παρακάτω σφάλμα:
>
> ```bash
> /sbin/ldconfig.real: Can't create temporary cache file /etc/ld.so.cache~: Permission denied 
> ```
>
> Για να το επιλύσετε, πληκτρολογήστε την παρακάτω εντολή στο τερματικό σας.
>
> ```bash
> sudo ldconfig
> ```

**Αποποίηση ευθύνης**:  
Αυτό το έγγραφο έχει μεταφραστεί χρησιμοποιώντας υπηρεσίες μηχανικής μετάφρασης με τεχνητή νοημοσύνη. Παρόλο που καταβάλλουμε προσπάθειες για ακρίβεια, παρακαλούμε να έχετε υπόψη ότι οι αυτοματοποιημένες μεταφράσεις ενδέχεται να περιέχουν λάθη ή ανακρίβειες. Το πρωτότυπο έγγραφο στη μητρική του γλώσσα θα πρέπει να θεωρείται η αυθεντική πηγή. Για κρίσιμες πληροφορίες, συνιστάται επαγγελματική ανθρώπινη μετάφραση. Δεν φέρουμε ευθύνη για τυχόν παρεξηγήσεις ή εσφαλμένες ερμηνείες που προκύπτουν από τη χρήση αυτής της μετάφρασης.