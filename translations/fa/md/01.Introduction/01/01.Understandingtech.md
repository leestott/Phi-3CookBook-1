# فناوری‌های کلیدی ذکر شده شامل

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - یک API سطح پایین برای یادگیری ماشین با شتاب سخت‌افزاری که بر اساس DirectX 12 ساخته شده است.
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - یک پلتفرم محاسبات موازی و مدل رابط برنامه‌نویسی (API) که توسط Nvidia توسعه یافته و امکان پردازش عمومی روی واحدهای پردازش گرافیکی (GPUs) را فراهم می‌کند.
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - یک فرمت باز طراحی شده برای نمایش مدل‌های یادگیری ماشین که قابلیت همکاری بین چارچوب‌های مختلف یادگیری ماشین را فراهم می‌کند.
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - فرمتی برای نمایش و به‌روزرسانی مدل‌های یادگیری ماشین که به‌ویژه برای مدل‌های زبانی کوچک که می‌توانند به طور مؤثر روی CPUها با کوانتیزاسیون ۴-۸ بیت اجرا شوند، مفید است.

## DirectML

DirectML یک API سطح پایین است که یادگیری ماشین با شتاب سخت‌افزاری را ممکن می‌سازد. این ابزار بر اساس DirectX 12 ساخته شده تا از شتاب GPU استفاده کند و مستقل از فروشنده است، به این معنی که نیازی به تغییر کد برای کار با GPUهای مختلف ندارد. این ابزار عمدتاً برای بارهای کاری آموزش مدل و استنتاج روی GPUها استفاده می‌شود.

از نظر پشتیبانی سخت‌افزاری، DirectML به گونه‌ای طراحی شده که با طیف گسترده‌ای از GPUها از جمله GPUهای یکپارچه و مجزای AMD، GPUهای یکپارچه Intel و GPUهای مجزای NVIDIA کار کند. این ابزار بخشی از پلتفرم Windows AI است و در ویندوز ۱۰ و ۱۱ پشتیبانی می‌شود، که امکان آموزش و استنتاج مدل‌ها روی هر دستگاه ویندوزی را فراهم می‌کند.

به‌روزرسانی‌ها و فرصت‌هایی مرتبط با DirectML وجود داشته است، مانند پشتیبانی از حداکثر ۱۵۰ اپراتور ONNX و استفاده توسط هر دو زمان اجرای ONNX و WinML. این ابزار توسط فروشندگان بزرگ سخت‌افزار یکپارچه (IHVs) پشتیبانی می‌شود که هر یک از متاکامندهای مختلفی پیروی می‌کنند.

## CUDA

CUDA که مخفف Compute Unified Device Architecture است، یک پلتفرم محاسبات موازی و مدل رابط برنامه‌نویسی (API) است که توسط Nvidia ایجاد شده است. این ابزار به توسعه‌دهندگان نرم‌افزار اجازه می‌دهد از یک واحد پردازش گرافیکی (GPU) سازگار با CUDA برای پردازش عمومی استفاده کنند – رویکردی که به آن GPGPU (محاسبات عمومی روی واحدهای پردازش گرافیکی) گفته می‌شود. CUDA یکی از عوامل کلیدی شتاب‌دهنده GPUهای Nvidia است و به طور گسترده در زمینه‌های مختلف از جمله یادگیری ماشین، محاسبات علمی و پردازش ویدئو استفاده می‌شود.

پشتیبانی سخت‌افزاری CUDA مختص GPUهای Nvidia است، زیرا این فناوری توسط Nvidia توسعه یافته و اختصاصی است. هر معماری از نسخه‌های خاصی از جعبه ابزار CUDA پشتیبانی می‌کند که کتابخانه‌ها و ابزارهای لازم را برای توسعه‌دهندگان فراهم می‌کند تا برنامه‌های CUDA را بسازند و اجرا کنند.

## ONNX

ONNX (Open Neural Network Exchange) یک فرمت باز طراحی شده برای نمایش مدل‌های یادگیری ماشین است. این ابزار تعریفی از یک مدل گراف محاسباتی قابل گسترش، همچنین تعاریفی از اپراتورهای داخلی و انواع داده‌های استاندارد ارائه می‌دهد. ONNX به توسعه‌دهندگان اجازه می‌دهد مدل‌ها را بین چارچوب‌های مختلف یادگیری ماشین جابجا کنند، که این امر قابلیت همکاری را ممکن می‌سازد و ایجاد و استقرار برنامه‌های هوش مصنوعی را آسان‌تر می‌کند.

Phi3 mini می‌تواند با ONNX Runtime روی CPU و GPU در دستگاه‌های مختلف، از جمله پلتفرم‌های سرور، دسکتاپ‌های ویندوز، لینوکس و مک، و CPUهای موبایل اجرا شود. پیکربندی‌های بهینه‌ای که اضافه کرده‌ایم شامل موارد زیر است:

- مدل‌های ONNX برای int4 DML: کوانتیزه شده به int4 از طریق AWQ
- مدل ONNX برای fp16 CUDA
- مدل ONNX برای int4 CUDA: کوانتیزه شده به int4 از طریق RTN
- مدل ONNX برای int4 CPU و موبایل: کوانتیزه شده به int4 از طریق RTN

## Llama.cpp

Llama.cpp یک کتابخانه نرم‌افزاری متن‌باز نوشته شده به زبان C++ است. این ابزار استنتاج را روی مدل‌های زبانی بزرگ (LLMs) مختلف، از جمله Llama انجام می‌دهد. این کتابخانه همراه با کتابخانه ggml (یک کتابخانه عمومی تنسور) توسعه داده شده است و هدف آن ارائه استنتاج سریع‌تر و استفاده کمتر از حافظه نسبت به پیاده‌سازی اصلی پایتون است. این ابزار از بهینه‌سازی سخت‌افزاری، کوانتیزاسیون، و یک API ساده با مثال‌ها پشتیبانی می‌کند. اگر به استنتاج کارآمد مدل‌های زبانی بزرگ علاقه‌مند هستید، Llama.cpp ارزش بررسی دارد زیرا Phi3 می‌تواند Llama.cpp را اجرا کند.

## GGUF

GGUF (Generic Graph Update Format) فرمتی است که برای نمایش و به‌روزرسانی مدل‌های یادگیری ماشین استفاده می‌شود. این ابزار به‌ویژه برای مدل‌های زبانی کوچک (SLMs) که می‌توانند به طور مؤثر روی CPUها با کوانتیزاسیون ۴-۸ بیت اجرا شوند، مفید است. GGUF برای نمونه‌سازی سریع و اجرای مدل‌ها روی دستگاه‌های لبه یا در وظایف دسته‌ای مانند خطوط CI/CD مفید است.

**سلب مسئولیت**:  
این سند با استفاده از خدمات ترجمه ماشینی مبتنی بر هوش مصنوعی ترجمه شده است. در حالی که ما تلاش می‌کنیم دقت را رعایت کنیم، لطفاً توجه داشته باشید که ترجمه‌های خودکار ممکن است شامل خطاها یا نادقتی‌ها باشند. سند اصلی به زبان بومی خود باید به عنوان منبع معتبر در نظر گرفته شود. برای اطلاعات حیاتی، توصیه می‌شود از ترجمه حرفه‌ای انسانی استفاده شود. ما هیچ‌گونه مسئولیتی در قبال سوءتفاهم‌ها یا تفسیرهای نادرست ناشی از استفاده از این ترجمه نداریم.