# شروع کار با Phi-3 به صورت محلی

این راهنما به شما کمک می‌کند محیط محلی خود را برای اجرای مدل Phi-3 با استفاده از Ollama تنظیم کنید. شما می‌توانید این مدل را به روش‌های مختلفی اجرا کنید، از جمله استفاده از GitHub Codespaces، VS Code Dev Containers یا محیط محلی خود.

## تنظیم محیط

### GitHub Codespaces

شما می‌توانید این قالب را به صورت مجازی با استفاده از GitHub Codespaces اجرا کنید. این دکمه یک نمونه مبتنی بر وب از VS Code را در مرورگر شما باز می‌کند:

1. قالب را باز کنید (این ممکن است چند دقیقه طول بکشد):

    [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/microsoft/phi-3cookbook)

2. یک پنجره ترمینال باز کنید.

### VS Code Dev Containers

⚠️ این گزینه فقط در صورتی کار می‌کند که حداقل 16 گیگابایت RAM به Docker Desktop اختصاص داده شده باشد. اگر کمتر از 16 گیگابایت RAM دارید، می‌توانید گزینه [GitHub Codespaces](../../../../../md/01.Introduction/01) را امتحان کنید یا [آن را به صورت محلی تنظیم کنید](../../../../../md/01.Introduction/01).

یک گزینه مرتبط، VS Code Dev Containers است که پروژه را در VS Code محلی شما با استفاده از [افزونه Dev Containers](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers) باز می‌کند:

1. Docker Desktop را اجرا کنید (اگر نصب نشده است، آن را نصب کنید).
2. پروژه را باز کنید:

    [![Open in Dev Containers](https://img.shields.io/static/v1?style=for-the-badge&label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/microsoft/phi-3cookbook)

3. در پنجره VS Code که باز می‌شود، وقتی فایل‌های پروژه ظاهر شدند (این ممکن است چند دقیقه طول بکشد)، یک پنجره ترمینال باز کنید.
4. با [مراحل استقرار](../../../../../md/01.Introduction/01) ادامه دهید.

### محیط محلی

1. اطمینان حاصل کنید که ابزارهای زیر نصب شده‌اند:

    * [Ollama](https://ollama.com/)
    * [Python 3.10+](https://www.python.org/downloads/)
    * [OpenAI Python SDK](https://pypi.org/project/openai/)

## آزمایش مدل

1. از Ollama بخواهید که مدل phi3:mini را دانلود و اجرا کند:

    ```shell
    ollama run phi3:mini
    ```

    این فرآیند چند دقیقه طول می‌کشد تا مدل دانلود شود.

2. وقتی در خروجی عبارت "success" را مشاهده کردید، می‌توانید از طریق پرامپت یک پیام به آن مدل ارسال کنید.

    ```shell
    >>> Write a haiku about hungry hippos
    ```

3. پس از چند ثانیه، باید یک پاسخ از مدل دریافت کنید.

4. برای یادگیری تکنیک‌های مختلف استفاده‌شده با مدل‌های زبانی، دفترچه Python با نام [ollama.ipynb](../../../../../code/01.Introduce/ollama.ipynb) را باز کنید و هر سلول را اجرا کنید. اگر از مدلی غیر از 'phi3:mini' استفاده کردید، `MODEL_NAME` in the first cell.

5. To have a conversation with the phi3:mini model from Python, open the Python file [chat.py](../../../../../code/01.Introduce/chat.py) and run it. You can change the `MODEL_NAME` را در ابتدای فایل تغییر دهید. همچنین می‌توانید پیام سیستم را تغییر دهید یا مثال‌های چندشات اضافه کنید.

**سلب مسئولیت**:  
این سند با استفاده از خدمات ترجمه ماشینی مبتنی بر هوش مصنوعی ترجمه شده است. در حالی که ما برای دقت تلاش می‌کنیم، لطفاً توجه داشته باشید که ترجمه‌های خودکار ممکن است حاوی خطاها یا نادرستی‌هایی باشند. سند اصلی به زبان مادری باید به عنوان منبع معتبر در نظر گرفته شود. برای اطلاعات حیاتی، توصیه می‌شود از ترجمه انسانی حرفه‌ای استفاده کنید. ما هیچ گونه مسئولیتی در قبال سوءتفاهم‌ها یا تفسیرهای نادرست ناشی از استفاده از این ترجمه نمی‌پذیریم.