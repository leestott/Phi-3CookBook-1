# **کوانتیزه کردن خانواده فی**

کوانتیزه کردن مدل به فرآیندی اشاره دارد که در آن پارامترهای مدل شبکه عصبی (مانند وزن‌ها و مقادیر فعال‌سازی) از یک بازه بزرگ (معمولاً یک بازه پیوسته) به یک بازه کوچک‌تر و محدود نگاشت می‌شوند. این فناوری می‌تواند اندازه و پیچیدگی محاسباتی مدل را کاهش داده و کارایی اجرای مدل را در محیط‌هایی با منابع محدود، مانند دستگاه‌های موبایل یا سیستم‌های تعبیه‌شده، بهبود بخشد. کوانتیزه کردن مدل با کاهش دقت پارامترها فشرده‌سازی را ممکن می‌سازد، اما در عین حال مقداری از دقت را نیز از دست می‌دهد. بنابراین، در فرآیند کوانتیزه کردن، لازم است بین اندازه مدل، پیچیدگی محاسباتی و دقت تعادل برقرار شود. روش‌های رایج کوانتیزه کردن شامل کوانتیزه کردن به اعداد ثابت و کوانتیزه کردن به اعداد شناور است. شما می‌توانید بسته به سناریو و نیاز خاص، استراتژی کوانتیزه کردن مناسب را انتخاب کنید.

ما امیدواریم مدل GenAI را روی دستگاه‌های لبه‌ای مستقر کنیم و به دستگاه‌های بیشتری اجازه دهیم وارد سناریوهای GenAI شوند، مانند دستگاه‌های موبایل، کامپیوترهای AI PC/Copilot+PC و دستگاه‌های IoT سنتی. از طریق مدل کوانتیزه شده، می‌توانیم آن را بر اساس دستگاه‌های مختلف روی دستگاه‌های لبه‌ای متفاوت مستقر کنیم. با ترکیب چارچوب‌های شتاب‌دهی مدل و مدل‌های کوانتیزه شده ارائه‌شده توسط تولیدکنندگان سخت‌افزار، می‌توانیم سناریوهای بهتری برای کاربردهای SLM بسازیم.

در سناریوی کوانتیزه کردن، ما دقت‌های مختلفی (INT4، INT8، FP16، FP32) داریم. در ادامه توضیحی درباره دقت‌های رایج کوانتیزه کردن ارائه شده است:

### **INT4**

کوانتیزه کردن به INT4 یک روش کوانتیزه کردن رادیکال است که وزن‌ها و مقادیر فعال‌سازی مدل را به اعداد صحیح ۴ بیتی تبدیل می‌کند. کوانتیزه کردن به INT4 معمولاً به دلیل بازه نمایشی کوچک‌تر و دقت پایین‌تر، باعث از دست رفتن بیشتری از دقت می‌شود. با این حال، در مقایسه با کوانتیزه کردن به INT8، این روش می‌تواند بیشتر نیازهای ذخیره‌سازی و پیچیدگی محاسباتی مدل را کاهش دهد. لازم به ذکر است که کوانتیزه کردن به INT4 در کاربردهای عملی نسبتاً نادر است، زیرا دقت بسیار پایین ممکن است باعث افت قابل توجهی در عملکرد مدل شود. علاوه بر این، همه سخت‌افزارها از عملیات INT4 پشتیبانی نمی‌کنند، بنابراین هنگام انتخاب روش کوانتیزه کردن باید سازگاری سخت‌افزاری را در نظر گرفت.

### **INT8**

کوانتیزه کردن به INT8 فرآیندی است که وزن‌ها و فعال‌سازی‌های مدل را از اعداد شناور به اعداد صحیح ۸ بیتی تبدیل می‌کند. اگرچه بازه عددی نمایشی توسط اعداد INT8 کوچک‌تر و کمتر دقیق است، اما می‌تواند به طور قابل توجهی نیازهای ذخیره‌سازی و محاسباتی را کاهش دهد. در کوانتیزه کردن به INT8، وزن‌ها و مقادیر فعال‌سازی مدل از یک فرآیند کوانتیزه کردن شامل مقیاس‌بندی و آفست عبور می‌کنند تا اطلاعات اصلی اعداد شناور تا حد امکان حفظ شود. در طول استنتاج، این مقادیر کوانتیزه شده دوباره به اعداد شناور برای محاسبه تبدیل می‌شوند و سپس برای مرحله بعدی دوباره به INT8 کوانتیزه می‌شوند. این روش می‌تواند در اکثر کاربردها دقت کافی را فراهم کند و در عین حال کارایی محاسباتی بالایی را حفظ کند.

### **FP16**

فرمت FP16، یعنی اعداد شناور ۱۶ بیتی (float16)، میزان استفاده از حافظه را در مقایسه با اعداد شناور ۳۲ بیتی (float32) به نصف کاهش می‌دهد، که در کاربردهای یادگیری عمیق بزرگ‌مقیاس مزایای قابل توجهی دارد. فرمت FP16 امکان بارگذاری مدل‌های بزرگ‌تر یا پردازش داده‌های بیشتر را در محدودیت‌های حافظه GPU فراهم می‌کند. با توجه به اینکه سخت‌افزار مدرن GPU به طور فزاینده‌ای از عملیات FP16 پشتیبانی می‌کند، استفاده از فرمت FP16 ممکن است بهبودهایی در سرعت محاسبات نیز به همراه داشته باشد. با این حال، فرمت FP16 معایب ذاتی خود را نیز دارد، یعنی دقت پایین‌تر، که ممکن است منجر به بی‌ثباتی عددی یا از دست دادن دقت در برخی موارد شود.

### **FP32**

فرمت FP32 دقت بالاتری ارائه می‌دهد و می‌تواند بازه گسترده‌ای از مقادیر را به دقت نمایش دهد. در سناریوهایی که عملیات ریاضی پیچیده انجام می‌شود یا نتایج با دقت بالا مورد نیاز است، فرمت FP32 ترجیح داده می‌شود. با این حال، دقت بالا همچنین به معنای استفاده بیشتر از حافظه و زمان محاسبات طولانی‌تر است. برای مدل‌های یادگیری عمیق بزرگ‌مقیاس، به ویژه زمانی که تعداد پارامترهای مدل زیاد است و حجم داده‌ها بسیار بزرگ است، فرمت FP32 ممکن است باعث کمبود حافظه GPU یا کاهش سرعت استنتاج شود.

روی دستگاه‌های موبایل یا دستگاه‌های IoT، می‌توانیم مدل‌های Phi-3.x را به INT4 تبدیل کنیم، در حالی که کامپیوترهای AI PC / Copilot PC می‌توانند از دقت‌های بالاتر مانند INT8، FP16 و FP32 استفاده کنند.

در حال حاضر، تولیدکنندگان سخت‌افزار مختلف چارچوب‌های متفاوتی برای پشتیبانی از مدل‌های تولیدی ارائه می‌دهند، مانند OpenVINO اینتل، QNN کوالکام، MLX اپل و CUDA انویدیا. این چارچوب‌ها همراه با کوانتیزه کردن مدل می‌توانند استقرار محلی را تکمیل کنند.

از نظر فناوری، ما پس از کوانتیزه کردن فرمت‌های مختلفی برای پشتیبانی داریم، مانند فرمت‌های PyTorch / Tensorflow، GGUF و ONNX. من مقایسه‌ای بین فرمت GGUF و ONNX و سناریوهای کاربردی آن‌ها انجام داده‌ام. در اینجا فرمت کوانتیزه ONNX را پیشنهاد می‌کنم که از پشتیبانی خوبی از چارچوب مدل تا سخت‌افزار برخوردار است. در این فصل، ما بر روی ONNX Runtime برای GenAI، OpenVINO و Apple MLX تمرکز خواهیم کرد تا کوانتیزه کردن مدل را انجام دهیم (اگر روش بهتری دارید، می‌توانید آن را از طریق ارسال PR به ما ارائه دهید).

**این فصل شامل می‌شود:**

1. [کوانتیزه کردن Phi-3.5 / 4 با استفاده از llama.cpp](./UsingLlamacppQuantifyingPhi.md)

2. [کوانتیزه کردن Phi-3.5 / 4 با استفاده از افزونه‌های Generative AI برای onnxruntime](./UsingORTGenAIQuantifyingPhi.md)

3. [کوانتیزه کردن Phi-3.5 / 4 با استفاده از Intel OpenVINO](./UsingIntelOpenVINOQuantifyingPhi.md)

4. [کوانتیزه کردن Phi-3.5 / 4 با استفاده از چارچوب Apple MLX](./UsingAppleMLXQuantifyingPhi.md)

**سلب مسئولیت**:  
این سند با استفاده از خدمات ترجمه ماشینی مبتنی بر هوش مصنوعی ترجمه شده است. در حالی که ما برای دقت تلاش می‌کنیم، لطفاً توجه داشته باشید که ترجمه‌های خودکار ممکن است شامل خطاها یا نادقتی‌هایی باشد. سند اصلی به زبان بومی آن باید به عنوان منبع معتبر در نظر گرفته شود. برای اطلاعات حیاتی، ترجمه حرفه‌ای انسانی توصیه می‌شود. ما هیچ مسئولیتی در قبال سوءتفاهم‌ها یا تفسیرهای نادرست ناشی از استفاده از این ترجمه نمی‌پذیریم.