# چت‌بات تعاملی Phi 3 Mini 4K Instruct با Whisper

## مروری کلی

چت‌بات تعاملی Phi 3 Mini 4K Instruct ابزاری است که به کاربران اجازه می‌دهد با دمو Microsoft Phi 3 Mini 4K Instruct از طریق ورودی متنی یا صوتی تعامل داشته باشند. این چت‌بات می‌تواند برای وظایف مختلفی مانند ترجمه، به‌روزرسانی وضعیت آب‌وهوا و جمع‌آوری اطلاعات عمومی استفاده شود.

### شروع به کار

برای استفاده از این چت‌بات، مراحل زیر را دنبال کنید:

1. یک فایل [E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb](https://github.com/microsoft/Phi-3CookBook/blob/main/code/06.E2E/E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb) باز کنید.
2. در پنجره اصلی نوت‌بوک، یک رابط کاربری چت‌باکس با یک کادر ورودی متن و یک دکمه "Send" مشاهده خواهید کرد.
3. برای استفاده از چت‌بات متنی، پیام خود را در کادر ورودی متن تایپ کرده و روی دکمه "Send" کلیک کنید. چت‌بات یک فایل صوتی تولید می‌کند که می‌توانید مستقیماً از داخل نوت‌بوک آن را پخش کنید.

**توجه**: این ابزار به یک GPU و دسترسی به مدل‌های Microsoft Phi-3 و OpenAI Whisper نیاز دارد که برای تشخیص گفتار و ترجمه استفاده می‌شوند.

### نیازمندی‌های GPU

برای اجرای این دمو به ۱۲ گیگابایت حافظه GPU نیاز دارید.

نیازمندی‌های حافظه برای اجرای دمو **Microsoft-Phi-3-Mini-4K instruct** روی GPU به عوامل مختلفی مانند اندازه داده ورودی (صوتی یا متنی)، زبان مورد استفاده برای ترجمه، سرعت مدل و حافظه موجود در GPU بستگی دارد.

به طور کلی، مدل Whisper برای اجرا روی GPU طراحی شده است. حداقل میزان حافظه GPU توصیه‌شده برای اجرای این مدل ۸ گیگابایت است، اما در صورت نیاز می‌تواند حافظه بیشتری را مدیریت کند.

توجه داشته باشید که اجرای حجم زیادی از داده یا درخواست‌های متعدد ممکن است به حافظه GPU بیشتری نیاز داشته باشد و/یا باعث مشکلات عملکردی شود. توصیه می‌شود که مورد استفاده خود را با پیکربندی‌های مختلف آزمایش کرده و استفاده از حافظه را نظارت کنید تا تنظیمات بهینه برای نیازهای خاص خود را تعیین کنید.

## نمونه E2E برای چت‌بات تعاملی Phi 3 Mini 4K Instruct با Whisper

نوت‌بوک جویتر با عنوان [Interactive Phi 3 Mini 4K Instruct Chatbot with Whisper](https://github.com/microsoft/Phi-3CookBook/blob/main/code/06.E2E/E2E_Phi-3-mini-4k-instruct-Whispser_Demo.ipynb) نحوه استفاده از دمو Microsoft Phi 3 Mini 4K instruct برای تولید متن از ورودی صوتی یا متنی را نشان می‌دهد. این نوت‌بوک چندین تابع تعریف می‌کند:

1. `tts_file_name(text)`: این تابع بر اساس متن ورودی، نام فایل را برای ذخیره فایل صوتی تولیدشده ایجاد می‌کند.
2. `edge_free_tts(chunks_list,speed,voice_name,save_path)`: این تابع از API Edge TTS برای تولید یک فایل صوتی از یک لیست بخش‌های متنی ورودی استفاده می‌کند. پارامترهای ورودی شامل لیست بخش‌ها، نرخ گفتار، نام صدا و مسیر خروجی برای ذخیره فایل صوتی تولیدشده هستند.
3. `talk(input_text)`: این تابع یک فایل صوتی تولید کرده و آن را با استفاده از API Edge TTS به یک نام فایل تصادفی در دایرکتوری /content/audio ذخیره می‌کند. پارامتر ورودی متن ورودی برای تبدیل به گفتار است.
4. `run_text_prompt(message, chat_history)`: این تابع از دمو Microsoft Phi 3 Mini 4K instruct برای تولید یک فایل صوتی از یک پیام ورودی استفاده کرده و آن را به تاریخچه چت اضافه می‌کند.
5. `run_audio_prompt(audio, chat_history)`: این تابع یک فایل صوتی را با استفاده از API مدل Whisper به متن تبدیل کرده و آن را به تابع `run_text_prompt()` ارسال می‌کند.
6. کد یک اپلیکیشن Gradio را راه‌اندازی می‌کند که به کاربران اجازه می‌دهد با دمو Phi 3 Mini 4K instruct از طریق تایپ پیام‌ها یا آپلود فایل‌های صوتی تعامل داشته باشند. خروجی به صورت یک پیام متنی در داخل اپلیکیشن نمایش داده می‌شود.

## رفع مشکلات

نصب درایورهای Cuda GPU

1. اطمینان حاصل کنید که برنامه‌های لینوکس شما به‌روز هستند.

    ```bash
    sudo apt update
    ```

2. درایورهای Cuda را نصب کنید.

    ```bash
    sudo apt install nvidia-cuda-toolkit
    ```

3. مکان درایورهای Cuda را ثبت کنید.

    ```bash
    echo /usr/lib64-nvidia/ >/etc/ld.so.conf.d/libcuda.conf; ldconfig
    ```

4. بررسی اندازه حافظه GPU Nvidia (نیاز به ۱۲ گیگابایت حافظه GPU)

    ```bash
    nvidia-smi
    ```

5. خالی کردن حافظه کش: اگر از PyTorch استفاده می‌کنید، می‌توانید با فراخوانی torch.cuda.empty_cache() تمام حافظه کش استفاده‌نشده را آزاد کنید تا توسط برنامه‌های دیگر GPU استفاده شود.

    ```python
    torch.cuda.empty_cache() 
    ```

6. بررسی Cuda Nvidia

    ```bash
    nvcc --version
    ```

7. وظایف زیر را برای ایجاد یک توکن Hugging Face انجام دهید:

    - به [صفحه تنظیمات توکن Hugging Face](https://huggingface.co/settings/tokens?WT.mc_id=aiml-137032-kinfeylo) بروید.
    - گزینه **توکن جدید** را انتخاب کنید.
    - **نام پروژه** موردنظر خود را وارد کنید.
    - **نوع** را روی **نوشتن** تنظیم کنید.

> **توجه**
>
> اگر با خطای زیر مواجه شدید:
>
> ```bash
> /sbin/ldconfig.real: Can't create temporary cache file /etc/ld.so.cache~: Permission denied 
> ```
>
> برای رفع این مشکل، دستور زیر را در ترمینال خود وارد کنید:
>
> ```bash
> sudo ldconfig
> ```

**سلب مسئولیت**:  
این سند با استفاده از خدمات ترجمه مبتنی بر هوش مصنوعی ترجمه شده است. در حالی که ما برای دقت تلاش می‌کنیم، لطفاً توجه داشته باشید که ترجمه‌های خودکار ممکن است شامل خطاها یا نادقتی‌ها باشند. سند اصلی به زبان اصلی آن باید به عنوان منبع معتبر در نظر گرفته شود. برای اطلاعات حساس، توصیه می‌شود از ترجمه انسانی حرفه‌ای استفاده کنید. ما هیچ مسئولیتی در قبال سوءتفاهم‌ها یا تفسیرهای نادرست ناشی از استفاده از این ترجمه نداریم.