# 언급된 주요 기술들

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - DirectX 12 위에 구축된 하드웨어 가속 머신 러닝을 위한 저수준 API.
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - Nvidia가 개발한 병렬 컴퓨팅 플랫폼 및 애플리케이션 프로그래밍 인터페이스(API) 모델로, 그래픽 처리 장치(GPU)에서 범용 처리(GPGPU)를 가능하게 함.
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - 머신 러닝 모델의 상호 운용성을 제공하기 위해 설계된 개방형 포맷.
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - 머신 러닝 모델을 표현하고 업데이트하는 데 사용되는 포맷으로, 특히 4-8비트 양자화로 CPU에서 효과적으로 실행 가능한 소형 언어 모델에 유용.

## DirectML

DirectML은 하드웨어 가속 머신 러닝을 가능하게 하는 저수준 API입니다. GPU 가속을 활용하기 위해 DirectX 12 위에 구축되었으며, 공급업체에 구애받지 않아 다양한 GPU 공급업체에서 코드 변경 없이 작동할 수 있습니다. 주로 GPU에서 모델 학습 및 추론 작업에 사용됩니다.

하드웨어 지원 측면에서 DirectML은 AMD 통합 및 독립형 GPU, Intel 통합 GPU, NVIDIA 독립형 GPU를 포함한 광범위한 GPU와 호환되도록 설계되었습니다. Windows AI 플랫폼의 일부로, Windows 10 및 11에서 지원되며, 모든 Windows 장치에서 모델 학습 및 추론이 가능합니다.

DirectML과 관련하여 ONNX 연산자 최대 150개를 지원하고, ONNX 런타임 및 WinML에서 사용되는 등 여러 업데이트와 기회가 있었습니다. 주요 통합 하드웨어 공급업체(IHV)의 지원을 받으며, 각 공급업체는 다양한 메타 명령을 구현하고 있습니다.

## CUDA

CUDA는 Compute Unified Device Architecture의 약자로, Nvidia가 만든 병렬 컴퓨팅 플랫폼 및 API 모델입니다. CUDA는 CUDA를 지원하는 그래픽 처리 장치(GPU)를 범용 처리(GPGPU)에 사용할 수 있도록 하며, 이는 머신 러닝, 과학 계산, 비디오 처리 등 다양한 분야에서 널리 사용됩니다.

CUDA의 하드웨어 지원은 Nvidia의 GPU에 한정되며, 이는 Nvidia가 개발한 독점 기술입니다. 각 아키텍처는 특정 버전의 CUDA 툴킷을 지원하며, 이 툴킷은 개발자가 CUDA 애플리케이션을 빌드하고 실행하는 데 필요한 라이브러리와 도구를 제공합니다.

## ONNX

ONNX(Open Neural Network Exchange)는 머신 러닝 모델을 표현하기 위해 설계된 개방형 포맷입니다. 확장 가능한 연산 그래프 모델 정의뿐만 아니라 내장 연산자 및 표준 데이터 타입 정의를 제공합니다. ONNX는 개발자가 다양한 머신 러닝 프레임워크 간에 모델을 이동할 수 있도록 하여 상호 운용성을 제공하며, AI 애플리케이션을 보다 쉽게 생성하고 배포할 수 있도록 합니다.

Phi3 mini는 ONNX 런타임을 사용하여 서버 플랫폼, Windows, Linux 및 Mac 데스크톱, 모바일 CPU를 포함한 다양한 장치에서 CPU와 GPU로 실행 가능합니다. 우리가 추가한 최적화된 구성은 다음과 같습니다:

- int4 DML용 ONNX 모델: AWQ를 통해 int4로 양자화
- fp16 CUDA용 ONNX 모델
- int4 CUDA용 ONNX 모델: RTN을 통해 int4로 양자화
- int4 CPU 및 모바일용 ONNX 모델: RTN을 통해 int4로 양자화

## Llama.cpp

Llama.cpp는 C++로 작성된 오픈 소스 소프트웨어 라이브러리로, Llama를 포함한 다양한 대형 언어 모델(LLM)의 추론을 수행합니다. ggml 라이브러리(범용 텐서 라이브러리)와 함께 개발된 Llama.cpp는 원래의 Python 구현에 비해 더 빠른 추론 속도와 낮은 메모리 사용량을 목표로 합니다. 하드웨어 최적화, 양자화 등을 지원하며, 간단한 API와 예제를 제공합니다. 효율적인 LLM 추론에 관심이 있다면, Phi3에서 실행 가능한 Llama.cpp를 탐색해볼 가치가 있습니다.

## GGUF

GGUF(Generic Graph Update Format)는 머신 러닝 모델을 표현하고 업데이트하는 데 사용되는 포맷입니다. 특히 4-8비트 양자화로 CPU에서 효과적으로 실행 가능한 소형 언어 모델(SLM)에 유용합니다. GGUF는 신속한 프로토타이핑 및 엣지 디바이스 또는 CI/CD 파이프라인과 같은 배치 작업에서 모델 실행에 유리합니다.

**면책 조항**:  
이 문서는 기계 기반 AI 번역 서비스를 사용하여 번역되었습니다. 정확성을 위해 노력하고 있지만, 자동 번역에는 오류나 부정확성이 포함될 수 있습니다. 원본 문서(원어로 작성된 문서)가 신뢰할 수 있는 권위 있는 자료로 간주되어야 합니다. 중요한 정보의 경우, 전문적인 인간 번역을 권장합니다. 이 번역 사용으로 인해 발생하는 오해나 잘못된 해석에 대해 당사는 책임을 지지 않습니다.