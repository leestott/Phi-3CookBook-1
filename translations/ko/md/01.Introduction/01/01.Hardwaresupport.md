# Phi 하드웨어 지원

Microsoft Phi는 ONNX Runtime에 최적화되어 있으며 Windows DirectML을 지원합니다. 이는 GPU, CPU, 심지어 모바일 장치 등 다양한 하드웨어 유형에서 원활하게 작동합니다.

## 장치 하드웨어
지원되는 하드웨어는 다음과 같습니다:

- GPU SKU: RTX 4090 (DirectML)
- GPU SKU: 1 A100 80GB (CUDA)
- CPU SKU: Standard F64s v2 (64 vCPUs, 128 GiB 메모리)

## 모바일 SKU

- Android - Samsung Galaxy S21
- Apple iPhone 14 이상 A16/A17 프로세서

## Phi 하드웨어 사양

- 최소 요구 구성
- Windows: DirectX 12를 지원하는 GPU 및 최소 4GB의 통합 RAM

CUDA: Compute Capability >= 7.02를 갖춘 NVIDIA GPU

![HardwareSupport](../../../../../translated_images/01.phihardware.925db5699da7752cf486314e6db087580583cfbcd548970f8a257e31a8aa862c.ko.png)

## 여러 GPU에서 onnxruntime 실행하기

현재 제공되는 Phi ONNX 모델은 1개의 GPU만 지원합니다. Phi 모델에 대해 멀티 GPU를 지원하는 것은 가능하지만, 2개의 GPU를 사용하는 ORT가 ORT의 두 인스턴스를 사용하는 것보다 더 높은 처리량을 보장하지는 않습니다. 최신 업데이트는 [ONNX Runtime](https://onnxruntime.ai/)을 참고하세요.

[Build 2024 GenAI ONNX Team](https://youtu.be/WLW4SE8M9i8?si=EtG04UwDvcjunyfC)에서는 Phi 모델에 대해 멀티 GPU 대신 멀티 인스턴스를 활성화했다고 발표했습니다.

현재는 CUDA_VISIBLE_DEVICES 환경 변수를 사용하여 아래와 같이 하나의 onnxruntime 또는 onnxruntime-genai 인스턴스를 실행할 수 있습니다.

```Python
CUDA_VISIBLE_DEVICES=0 python infer.py
CUDA_VISIBLE_DEVICES=1 python infer.py
```

[Azure AI Foundry](https://ai.azure.com)에서 Phi에 대해 더 자세히 탐구해 보세요.

**면책 조항**:  
이 문서는 기계 기반 AI 번역 서비스를 사용하여 번역되었습니다. 정확성을 위해 노력하고 있지만, 자동 번역에는 오류나 부정확성이 포함될 수 있음을 유의하시기 바랍니다. 원어로 작성된 원본 문서를 신뢰할 수 있는 권위 있는 자료로 간주해야 합니다. 중요한 정보의 경우, 전문적인 인간 번역을 권장합니다. 이 번역 사용으로 인해 발생하는 오해나 잘못된 해석에 대해 당사는 책임을 지지 않습니다.