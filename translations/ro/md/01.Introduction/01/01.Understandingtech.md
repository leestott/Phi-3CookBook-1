# Tehnologiile cheie menționate includ

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - o API de nivel jos pentru învățarea automată accelerată hardware, construită pe DirectX 12.
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - o platformă de calcul paralel și un model de interfață de programare a aplicațiilor (API) dezvoltat de Nvidia, care permite procesarea generală pe unități de procesare grafică (GPU).
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - un format deschis conceput pentru a reprezenta modelele de învățare automată, oferind interoperabilitate între diferite cadre ML.
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - un format utilizat pentru reprezentarea și actualizarea modelelor de învățare automată, util în special pentru modelele lingvistice mai mici care pot rula eficient pe CPU-uri cu cuantizare de 4-8 biți.

## DirectML

DirectML este o API de nivel jos care permite învățarea automată accelerată hardware. Este construită pe DirectX 12 pentru a utiliza accelerarea GPU și este independentă de furnizor, ceea ce înseamnă că nu necesită modificări de cod pentru a funcționa pe diferiți furnizori de GPU. Este utilizată în principal pentru sarcini de antrenare și inferență a modelelor pe GPU-uri.

În ceea ce privește suportul hardware, DirectML este conceput pentru a funcționa cu o gamă largă de GPU-uri, inclusiv GPU-uri integrate și discrete AMD, GPU-uri integrate Intel și GPU-uri discrete NVIDIA. Face parte din platforma Windows AI și este compatibil cu Windows 10 & 11, permițând antrenarea și inferența modelelor pe orice dispozitiv Windows.

Au existat actualizări și oportunități legate de DirectML, cum ar fi suportul pentru până la 150 de operatori ONNX și utilizarea sa de către ONNX runtime și WinML. Este susținut de marii furnizori de hardware integrat (IHV), fiecare implementând diverse metacomenzi.

## CUDA

CUDA, care înseamnă Compute Unified Device Architecture, este o platformă de calcul paralel și un model de interfață de programare a aplicațiilor (API) creat de Nvidia. Permite dezvoltatorilor de software să utilizeze o unitate de procesare grafică (GPU) compatibilă CUDA pentru procesare generală – o abordare numită GPGPU (General-Purpose computing on Graphics Processing Units). CUDA este un factor cheie pentru accelerarea GPU a Nvidia și este utilizată pe scară largă în diverse domenii, inclusiv învățare automată, calcul științific și procesare video.

Suportul hardware pentru CUDA este specific GPU-urilor Nvidia, fiind o tehnologie proprietară dezvoltată de Nvidia. Fiecare arhitectură suportă versiuni specifice ale kitului de instrumente CUDA, care oferă bibliotecile și instrumentele necesare pentru dezvoltatori pentru a construi și rula aplicații CUDA.

## ONNX

ONNX (Open Neural Network Exchange) este un format deschis conceput pentru a reprezenta modelele de învățare automată. Oferă o definiție a unui model de grafic de calcul extensibil, precum și definiții ale operatorilor încorporați și tipurilor de date standard. ONNX permite dezvoltatorilor să mute modelele între diferite cadre ML, oferind interoperabilitate și facilitând crearea și implementarea aplicațiilor AI.

Phi3 mini poate rula cu ONNX Runtime pe CPU și GPU pe diferite dispozitive, inclusiv platforme server, desktopuri Windows, Linux și Mac, și procesoare mobile.
Configurările optimizate pe care le-am adăugat sunt:

- Modele ONNX pentru int4 DML: Cuantizate la int4 prin AWQ
- Model ONNX pentru fp16 CUDA
- Model ONNX pentru int4 CUDA: Cuantizat la int4 prin RTN
- Model ONNX pentru int4 CPU și Mobile: Cuantizat la int4 prin RTN

## Llama.cpp

Llama.cpp este o bibliotecă software open-source scrisă în C++. Realizează inferența pe diverse modele lingvistice mari (LLM), inclusiv Llama. Dezvoltată împreună cu biblioteca ggml (o bibliotecă generală de tensori), llama.cpp își propune să ofere o inferență mai rapidă și o utilizare mai redusă a memoriei comparativ cu implementarea originală în Python. Oferă suport pentru optimizarea hardware, cuantizare și include o API simplă și exemple. Dacă ești interesat de inferența eficientă a LLM-urilor, llama.cpp merită explorată, deoarece Phi3 poate rula Llama.cpp.

## GGUF

GGUF (Generic Graph Update Format) este un format utilizat pentru reprezentarea și actualizarea modelelor de învățare automată. Este deosebit de util pentru modelele lingvistice mai mici (SLM) care pot rula eficient pe CPU-uri cu cuantizare de 4-8 biți. GGUF este benefic pentru prototipare rapidă și rularea modelelor pe dispozitive de tip edge sau în sarcini de tip batch, precum pipeline-urile CI/CD.

**Declinări de responsabilitate**:  
Acest document a fost tradus folosind servicii de traducere bazate pe inteligență artificială. Deși ne străduim să asigurăm acuratețea, vă rugăm să rețineți că traducerile automate pot conține erori sau inexactități. Documentul original, în limba sa natală, ar trebui considerat sursa autoritară. Pentru informații critice, se recomandă traducerea profesională realizată de un specialist. Nu ne asumăm responsabilitatea pentru neînțelegeri sau interpretări greșite care ar putea rezulta din utilizarea acestei traduceri.