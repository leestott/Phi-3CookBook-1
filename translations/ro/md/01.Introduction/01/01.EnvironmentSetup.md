# Începeți cu Phi-3 local

Acest ghid vă va ajuta să configurați mediul local pentru a rula modelul Phi-3 folosind Ollama. Puteți rula modelul în mai multe moduri, inclusiv utilizând GitHub Codespaces, containere Dev pentru VS Code sau mediul local.

## Configurarea mediului

### GitHub Codespaces

Puteți rula acest șablon virtual utilizând GitHub Codespaces. Butonul va deschide o instanță VS Code bazată pe web direct în browserul dvs.:

1. Deschideți șablonul (aceasta poate dura câteva minute):

    [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/microsoft/phi-3cookbook)

2. Deschideți o fereastră de terminal.

### Containere Dev pentru VS Code

⚠️ Această opțiune va funcționa doar dacă Docker Desktop are alocate cel puțin 16 GB de RAM. Dacă aveți mai puțin de 16 GB de RAM, puteți încerca opțiunea [GitHub Codespaces](../../../../../md/01.Introduction/01) sau [să configurați mediul local](../../../../../md/01.Introduction/01).

O altă opțiune este utilizarea containerelor Dev pentru VS Code, care vor deschide proiectul în VS Code local utilizând extensia [Dev Containers](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers):

1. Porniți Docker Desktop (instalați-l dacă nu este deja instalat).
2. Deschideți proiectul:

    [![Open in Dev Containers](https://img.shields.io/static/v1?style=for-the-badge&label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/microsoft/phi-3cookbook)

3. În fereastra VS Code care se deschide, odată ce fișierele proiectului apar (aceasta poate dura câteva minute), deschideți o fereastră de terminal.
4. Continuați cu [pașii de implementare](../../../../../md/01.Introduction/01).

### Mediu local

1. Asigurați-vă că aveți instalate următoarele instrumente:

    * [Ollama](https://ollama.com/)
    * [Python 3.10+](https://www.python.org/downloads/)
    * [OpenAI Python SDK](https://pypi.org/project/openai/)

## Testarea modelului

1. Cereți lui Ollama să descarce și să ruleze modelul phi3:mini:

    ```shell
    ollama run phi3:mini
    ```

    Aceasta va dura câteva minute pentru a descărca modelul.

2. După ce vedeți "success" în output, puteți trimite un mesaj către acel model din prompt.

    ```shell
    >>> Write a haiku about hungry hippos
    ```

3. După câteva secunde, ar trebui să vedeți un răspuns transmis de model.

4. Pentru a învăța despre diferite tehnici utilizate cu modelele de limbaj, deschideți notebook-ul Python [ollama.ipynb](../../../../../code/01.Introduce/ollama.ipynb) și rulați fiecare celulă. Dacă ați folosit un model diferit de 'phi3:mini', modificați `MODEL_NAME` in the first cell.

5. To have a conversation with the phi3:mini model from Python, open the Python file [chat.py](../../../../../code/01.Introduce/chat.py) and run it. You can change the `MODEL_NAME` în partea de sus a fișierului, după cum este necesar, și puteți, de asemenea, să modificați mesajul sistemului sau să adăugați exemple few-shot, dacă doriți.

**Declinări de responsabilitate**:  
Acest document a fost tradus folosind servicii de traducere bazate pe inteligență artificială. Deși ne străduim să asigurăm acuratețea, vă rugăm să rețineți că traducerile automate pot conține erori sau inexactități. Documentul original în limba sa nativă ar trebui considerat sursa de autoritate. Pentru informații critice, se recomandă traducerea profesională realizată de un specialist uman. Nu ne asumăm răspunderea pentru eventualele neînțelegeri sau interpretări greșite care pot apărea din utilizarea acestei traduceri.