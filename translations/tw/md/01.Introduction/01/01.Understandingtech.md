# 提到的關鍵技術包括

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - 一個基於 DirectX 12 的底層 API，用於硬體加速的機器學習。
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - 由 Nvidia 開發的平行運算平台和應用程式介面 (API) 模型，支持在圖形處理單元 (GPU) 上進行通用運算。
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - 一種開放格式，用於表示機器學習模型，實現不同 ML 框架之間的互通性。
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - 一種用於表示和更新機器學習模型的格式，特別適合能在 CPU 上以 4-8bit 量化高效運行的小型語言模型。

## DirectML

DirectML 是一個底層 API，支持硬體加速的機器學習。它建立在 DirectX 12 之上，利用 GPU 加速，並且不依賴於特定的硬體廠商，這意味著無需修改程式碼即可跨不同的 GPU 廠商運行。主要用於 GPU 上的模型訓練和推理工作。

在硬體支持方面，DirectML 設計適配多種 GPU，包括 AMD 的集成和獨立 GPU、Intel 的集成 GPU 和 NVIDIA 的獨立 GPU。它是 Windows AI 平台的一部分，支持 Windows 10 和 11，允許在任何 Windows 設備上進行模型訓練和推理。

DirectML 的相關更新和機會包括支持多達 150 個 ONNX 運算元，並被 ONNX Runtime 和 WinML 使用。它由主要的硬體廠商 (IHVs) 支持，每個廠商都實現了各自的元命令。

## CUDA

CUDA，全名 Compute Unified Device Architecture，是由 Nvidia 創建的平行運算平台和應用程式介面 (API) 模型。它允許軟體開發人員使用支持 CUDA 的圖形處理單元 (GPU) 進行通用運算，這種方法稱為 GPGPU (General-Purpose computing on Graphics Processing Units)。CUDA 是 Nvidia GPU 加速的一個重要技術，被廣泛應用於機器學習、科學計算和影片處理等領域。

CUDA 的硬體支持僅限於 Nvidia 的 GPU，因為這是一項由 Nvidia 開發的專有技術。每個架構支持特定版本的 CUDA 工具包，該工具包提供了開發人員用於構建和運行 CUDA 應用程式所需的庫和工具。

## ONNX

ONNX (Open Neural Network Exchange) 是一種開放格式，用於表示機器學習模型。它提供了可擴展的計算圖模型定義，以及內建運算元和標準數據類型的定義。ONNX 允許開發人員在不同的 ML 框架之間轉移模型，實現互通性，並讓 AI 應用的開發和部署更加輕鬆。

Phi3 mini 可以在 CPU 和 GPU 上使用 ONNX Runtime 運行，支持多種設備，包括伺服器平台、Windows、Linux 和 Mac 桌面，以及移動 CPU。我們新增的優化配置包括：

- 用於 int4 DML 的 ONNX 模型：通過 AWQ 量化為 int4
- 用於 fp16 CUDA 的 ONNX 模型
- 用於 int4 CUDA 的 ONNX 模型：通過 RTN 量化為 int4
- 用於 int4 CPU 和移動端的 ONNX 模型：通過 RTN 量化為 int4

## Llama.cpp

Llama.cpp 是一個用 C++ 編寫的開源軟體庫，用於執行多種大型語言模型 (LLMs) 的推理，包括 Llama。它與 ggml 庫（一個通用張量庫）一起開發，旨在比原始的 Python 實現提供更快的推理速度和更低的記憶體使用量。Llama.cpp 支持硬體優化、量化，並提供簡單的 API 和範例。如果您對高效的大型語言模型推理感興趣，Llama.cpp 值得探索，因為 Phi3 可以運行 Llama.cpp。

## GGUF

GGUF (Generic Graph Update Format) 是一種用於表示和更新機器學習模型的格式。它特別適用於能在 CPU 上以 4-8bit 量化高效運行的小型語言模型 (SLMs)。GGUF 在快速原型設計以及在邊緣設備或像 CI/CD 管道這樣的批處理任務中運行模型方面非常有用。

**免責聲明**：  
本文檔使用基於機器的人工智能翻譯服務進行翻譯。我們雖然努力確保準確性，但請注意，自動翻譯可能包含錯誤或不準確之處。應以原始語言的文件作為權威來源。對於關鍵信息，建議尋求專業人工翻譯。我們對因使用此翻譯而產生的任何誤解或誤讀概不負責。