# Comienza con Phi-3 localmente

Esta guía te ayudará a configurar tu entorno local para ejecutar el modelo Phi-3 usando Ollama. Puedes ejecutar el modelo de varias maneras, incluyendo el uso de GitHub Codespaces, contenedores de desarrollo de VS Code o tu entorno local.

## Configuración del entorno

### GitHub Codespaces

Puedes ejecutar esta plantilla virtualmente usando GitHub Codespaces. El botón abrirá una instancia de VS Code basada en la web en tu navegador:

1. Abre la plantilla (esto puede tardar varios minutos):

    [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/microsoft/phi-3cookbook)

2. Abre una ventana de terminal.

### Contenedores de desarrollo de VS Code

⚠️ Esta opción solo funcionará si tu Docker Desktop tiene asignados al menos 16 GB de RAM. Si tienes menos de 16 GB de RAM, puedes probar la [opción de GitHub Codespaces](../../../../../md/01.Introduction/01) o [configurarlo localmente](../../../../../md/01.Introduction/01).

Una opción relacionada son los contenedores de desarrollo de VS Code, que abrirán el proyecto en tu VS Code local usando la [extensión de Dev Containers](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers):

1. Inicia Docker Desktop (instálalo si aún no lo tienes instalado).
2. Abre el proyecto:

    [![Open in Dev Containers](https://img.shields.io/static/v1?style=for-the-badge&label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/microsoft/phi-3cookbook)

3. En la ventana de VS Code que se abre, una vez que aparezcan los archivos del proyecto (esto puede tardar varios minutos), abre una ventana de terminal.
4. Continúa con los [pasos de implementación](../../../../../md/01.Introduction/01).

### Entorno local

1. Asegúrate de tener instaladas las siguientes herramientas:

    * [Ollama](https://ollama.com/)
    * [Python 3.10+](https://www.python.org/downloads/)
    * [OpenAI Python SDK](https://pypi.org/project/openai/)

## Probar el modelo

1. Pídele a Ollama que descargue y ejecute el modelo phi3:mini:

    ```shell
    ollama run phi3:mini
    ```

    Esto tomará unos minutos para descargar el modelo.

2. Una vez que veas "success" en la salida, puedes enviar un mensaje a ese modelo desde el prompt.

    ```shell
    >>> Write a haiku about hungry hippos
    ```

3. Después de varios segundos, deberías ver una respuesta transmitida desde el modelo.

4. Para aprender sobre diferentes técnicas utilizadas con modelos de lenguaje, abre el cuaderno de Python [ollama.ipynb](../../../../../code/01.Introduce/ollama.ipynb) y ejecuta cada celda. Si usaste un modelo diferente a 'phi3:mini', cambia el `MODEL_NAME` in the first cell.

5. To have a conversation with the phi3:mini model from Python, open the Python file [chat.py](../../../../../code/01.Introduce/chat.py) and run it. You can change the `MODEL_NAME` en la parte superior del archivo según sea necesario. También puedes modificar el mensaje del sistema o agregar ejemplos few-shot si lo deseas.

**Descargo de responsabilidad**:  
Este documento ha sido traducido utilizando servicios de traducción automática basados en inteligencia artificial. Si bien nos esforzamos por lograr precisión, tenga en cuenta que las traducciones automáticas pueden contener errores o imprecisiones. El documento original en su idioma nativo debe considerarse como la fuente autorizada. Para información crítica, se recomienda una traducción profesional realizada por humanos. No nos hacemos responsables de malentendidos o interpretaciones erróneas que puedan surgir del uso de esta traducción.